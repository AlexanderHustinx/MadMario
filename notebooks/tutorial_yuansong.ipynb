{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_yuansong.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuansongFeng/MadMario/blob/master/tutorial_yuansong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdoL_As2yKf9",
        "colab_type": "text"
      },
      "source": [
        "Pre-MVP tutorial for walking users through building a learning Mario. Guidelines for creating this notebook (feel free to add/edit):\n",
        "1. Extensive explanation (link to AI cheatsheet where necessary) \n",
        "2. Only ask for core logics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SseXTspseRAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d174d585-27a1-419a-e095-d8061ee945a2"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null \n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null \n",
        "!pip install gym-super-mario-bros==7.3.0 > /dev/null "
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: nes-py 8.1.4 has requirement pyglet>=1.5.5, but you'll have pyglet 1.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: gym 0.17.2 has requirement pyglet<=1.5.0,>=1.4.0, but you'll have pyglet 1.5.7 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pp2iiMw4JkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    "
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3tZhKiAxFej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    '''mini cnn structure\n",
        "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "    '''\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ConvNet, self).__init__()\n",
        "        c, h, w = input_dim\n",
        "        self.conv_1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv_3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(3136, 512)\n",
        "        self.output = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input: B x C x H x W\n",
        "        x = input\n",
        "        x = self.conv_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv_3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4hh1rNclV9",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to Mad Mario! \n",
        "\n",
        "We put together this project to walk you through fundamentals of reinforcement learning. Along the project, you will implement a smart Mario that learns to complete levels on itself. To begin with, you don't need to know anything about Reinforcement Learning (RL). In case you wanna peek ahead, here is a [cheatsheet on RL basics](https://drive.google.com/file/d/1qwyemCX7o37OLbSky7Kkufg_YViWwL-g/view?usp=sharing) that we will refer to throughout the project. At the end of the tutorial, you will gain a solid understanding of RL fundamentals and implement a classic RL algorithm, Q-learning, on yourself. \n",
        "\n",
        "\n",
        "It's recommended that you have familiarity with Python and high school or equivalent level of math/statistics background -- that said, don't worry if memory is blurry. Just leave comments anywhere you feel confused, and we will explain the section in more details. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6IAcrfyegjL",
        "colab_type": "text"
      },
      "source": [
        "## Let's get started! \n",
        "\n",
        "First thing first, let's look at what we will build: Just like when we first try the game, Mario enters the game not knowing anything about the game. It makes random action just to understand the game better. Each failure experience adds to Mario's memory, and as failure accumulates, Mario starts to recognize the better action to take in a particular scenario. Eventually Mario learns a good strategy and completes the level. \n",
        "\n",
        "Let's put the story into pseudo code.\n",
        "\n",
        "```\n",
        "for a total of N episodes:\n",
        "  for a total of M steps in each episode:\n",
        "    Mario makes an action\n",
        "    Game gives a feedback \n",
        "    Mario remembers the action and feedback\n",
        "    after building up some experiences:\n",
        "      Mario learns from experiences   \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzxAwU4f1-H",
        "colab_type": "text"
      },
      "source": [
        "In RL terminology: agent (Mario) interacts with environment (Game) by choosing actions, and environment responds with reward and next state. Based on the collected (state, action, reward) information, agent learns to maximize its future return by optimizing its action policy. \n",
        "\n",
        "While these terms may sound scary, in a short while they will all make sense. It'd be helpful to review the [cheatsheet](https://drive.google.com/file/d/1qwyemCX7o37OLbSky7Kkufg_YViWwL-g/view?usp=sharing), before we start coding. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgK3jHa65bW9",
        "colab_type": "text"
      },
      "source": [
        "# Environment\n",
        "Environment is a key concept in reinforcement learning. It's the world that Mario interacts with and learns from. Environment is characterized by state (cheatsheet). In Mario, this is the game console consisting of tubes, mushrooms and other components. When Mario makes an action, environment responds with a reward (cheatsheet) and the next state (cheatsheet).  \n",
        "\n",
        "\n",
        "To run Mario envirioment, do\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt3eAj6PC0nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gym_super_mario_bros\n",
        "# from gym.wrappers import Monitor\n",
        "# from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "\n",
        "# environment = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "# ### for simplicity, Mario can only have 2 actions initially: walk right \n",
        "# ### or jump right\n",
        "# ### for a full list of possible actions, see\n",
        "# ### https://github.com/Kautenja/gym-super-mario-bros/blob/master/gym_super_mario_bros/actions.py\n",
        "# environment = JoypadSpace(\n",
        "#     env,\n",
        "#     [['right'],\n",
        "#     ['right', 'A']]\n",
        "# )\n",
        "# environment = Monitor(environment, './video', force=True, mode = 'evaluation')\n",
        "# environment.reset()\n",
        "# for _ in range(1000):\n",
        "#   # render game output\n",
        "#   environment.render()\n",
        "\n",
        "#   # choose random action\n",
        "#   action = environment.action_space.sample()\n",
        "\n",
        "#   # perform action on environment, environment provides feedback \n",
        "#   # with env.step() function\n",
        "#   environment.step(action=action)\n",
        "\n",
        "# environment.close()\n",
        "\n",
        "# show_video()"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bN0TFDnCpil",
        "colab_type": "text"
      },
      "source": [
        "## Wrappers\n",
        "\n",
        "In the previous section we talked about environment in general. However, in many cases we want to do some pre-processing to the environment and its data before we feed them to the agent. This introduces the idea of a **wrapper**.\n",
        "\n",
        "Like its name suggests, a wrapper wraps around the environment and modifies its methods/data in some generic way. For example, the Mario environment gives you some observations, but you want to accumulate them in some buffer and provide to the agent the N last observations, which is a common scenario for dynamic computer games like Mario, when one single frame is just not enough to get full information about the game state. For a concrete example, we cannot determine if Mario is in a jumping or landing motion from just one frame of environment output. We need some previous frames to have that insight.\n",
        "\n",
        "Another common wrapper is to transform RGB picture frames into Grayscale. It makes sense because as far as the agent is concerned, the outcome of the game or how much reward it collects doesn't change whether he lives in a RGB world or Grayscale world!\n",
        "\n",
        "**before wrapper**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1c9-tUWFyk4u_vNNrkZo1Rg0e2FUcbF3N)\n",
        "<!-- ![picture](https://drive.google.com/uc?id=1s7UewXkmF4g_gZfD7vloH7n1Cr-D3YYX)\n",
        "![picture](https://drive.google.com/uc?id=1mXDt8rFLKT9a-YvhGOgGZT4bq0T2y7iw) -->\n",
        "\n",
        "**after wrapper**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1ED9brgnbPmUZL43Bl_x2FDmXd-hsHBQt)\n",
        "<!-- ![picture](https://drive.google.com/uc?id=1PB1hHSPk6jIhSxVok2u2ntHjvE3zrk7W)\n",
        "![picture](https://drive.google.com/uc?id=1CYm5q71f_OlY_mqvZADuMMjPmcMgbjVW) -->\n",
        "\n",
        "We apply a wrapper to environment in this fashion: \n",
        "```\n",
        "env = wrapper(env)\n",
        "```\n",
        "\n",
        "\n",
        "### Instructions\n",
        "\n",
        "We want to apply 3 built-in wrappers to the given `env`, `GrayScaleObservation`, `ResizeObservation`, and `FrameStack`.  \n",
        "\n",
        "https://github.com/openai/gym/tree/master/gym/wrappers\n",
        "\n",
        "\n",
        "`FrameStack` is a wrapper that will allow us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can differentiate between when Mario was landing or jumping based on his direction of movement in the previous several frames. \n",
        "\n",
        "We can start with the following arguments:\n",
        "`GrayScaleObservation`: keep_dim=False \n",
        "`ResizeObservation`: shape=84 \n",
        "`FrameStack`: num_stack=4 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ib7vjUD5cGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import FrameStack, GrayScaleObservation\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym.spaces import Box\n",
        "import cv2\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    \"\"\"Downsample the image observation to a square image. \"\"\"\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
        "        return observation\n",
        "\n",
        "# the original environment object \n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        ")\n",
        "\n",
        "# TODO wrap the given env with GrayScaleObservation\n",
        "env = GrayScaleObservation(env, keep_dim=False)\n",
        "# TODO wrap the given env with ResizeObservation\n",
        "env = ResizeObservation(env, shape=84)\n",
        "# TODO wrap the given env with FrameStack \n",
        "env = FrameStack(env, num_stack=4)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5DA19ABgiY",
        "colab_type": "text"
      },
      "source": [
        "## Custom Wrapper\n",
        "\n",
        "We also would like you to get a taste of implementing an environment wrapper on your own, instead of calling off-the-shelf packages. \n",
        "\n",
        "Here is an idea:\n",
        "As an effort of downsizing our model to make training faster, we can choose to skip every n-th frame. In other words, our wrapped environment will only output every n-th frame. Below is a skeleton of the class `SkipFrame`, inherited from `gym.Wrapper`.  Notice in the `__init__` function, the `_skip` field is overriden by the input parameter, default set at 4.\n",
        "\n",
        "However, it is important to accumulate the reward during these skipped steps, because the reward is the most important factor in determining the success of the learning model, so while we can skip frame for dimension reduction purpose, it is crucial we keep adding those rewards to our total reward. \n",
        "\n",
        "\n",
        "### Instruction\n",
        "\n",
        "Implement the reward accumulation function, using your favorite `for` loop.  \n",
        "\n",
        "Here, the custom `SkipFrame` inherits `gym.Wrapper`, the superclass of all wrappers we are going to use. We are going to override its `step()` method, which is basically the endpoint that environment interacts with agent:\n",
        "\n",
        "Given an `action`, env.step() returns a tuple of `next_state(obs), reward, done, info`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZKyjJVHQBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # TODO accumulate reward and repeat the same action \n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "       \n",
        "env = SkipFrame(env)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VUq3afemcj-",
        "colab_type": "text"
      },
      "source": [
        "# Agent\n",
        "\n",
        "Agent is the other core concept in Reinforcement Learning. It interacts with environemnt by making actions (link to cheatsheet). The agent acts according to its action policy (link to cheatsheet). \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t7f9pit9bu2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```\n",
        "Initialize environment\n",
        "While True:\n",
        "  ### agent chooses an action \n",
        "  ### based on current state\n",
        "  agent.act(state)\n",
        "  ### environment provides feedback/reward \n",
        "  ### and state transition\n",
        "  next_state, reward, done, info = env.step(action=action)\n",
        "  state = next_state\n",
        "  if done: \n",
        "    break\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmr1axsfHf4Y",
        "colab_type": "text"
      },
      "source": [
        "The agent class, `Mario`, captures Mario's behavior in the game environment. The agent should be able to \n",
        "\n",
        "- Make its decision about next action to take. Mario takes the environment state and acts following its optimal action (link to cheatsheet) \n",
        "\n",
        "- Remember past experiences. Later mario uses these experiences to update its action policy (link to cheatsheet)\n",
        "\n",
        "- Improve action policy over time. Mario updates its action policy following the Q-learning algorithm (link to cheatsheet). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FMfCSqgmv23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTdoKmKCovs",
        "colab_type": "text"
      },
      "source": [
        "## Initialize\n",
        "Before implementing the core functions, let's define some key parameters. For example, we use *epsilon* in action policy (refer to cheatsheet) to encourage exploration, use *gamma* to calculate discounted future return (link to cheatsheet). \n",
        "\n",
        "eps, float\n",
        "> Random Exploration Prabability. Under some probability, agent does not follow the optimal action policy (cheatsheet), but instead chooses a random actino to explore the environment. A high exploration rate is important at the early stage of learning to ensure proper exploration and not falling to local optima. The exploration rate should decrease as agent improves its policy. \n",
        "\n",
        "> Initialize to 1.0. \n",
        "\n",
        "\n",
        "\n",
        "eps_decay, float\n",
        "\n",
        "> Decay rate of eps. Agent rigorously explores space at the early stage, but gradually reduces its exploration rate to maintain action quality. In the later stage, agent already learns a fairly good policy, so we want it to follow its policy more frequently. Decrease eps by the factor of eps_decay each time the agent acts.\n",
        "\n",
        "> Initialize to 0.99999975. \n",
        "\n",
        "\n",
        "gamma, float\n",
        "> Future reward discount rate. *gamma* serves to make agent give higher weight on the short-term rewards over future reward.\n",
        "\n",
        "> Initialize to 0.9\n",
        "\n",
        "batch_size, int\n",
        "> Number of experiences used to update each time.\n",
        "\n",
        "> Please initialize it to 32\n",
        "\n",
        "state_dim, tuple of int\n",
        "\n",
        "> State space dimension. In Mario, this is 4 consecutive snapshots of the enviroment stacked together, where each snapshot is a 84*84 gray-scale image, so state_dim = (4, 84, 84).\n",
        "\n",
        "action_dim, int\n",
        ">  Action space dimension. In Mario, this is the number of total possible actions. \n",
        "\n",
        "max_memory, int\n",
        "> Size of Mario's memory. The memory is filled with Mario's past experiences. Each experience consists of (state, next_state, action, reward, done). Note we use a *queue* (FIFO) for storing memory. As Mario collects more experiences, old experiences are popped to make room for most recent ones. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxRwyg8D6gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "class Mario(object):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "       # state space dimension\n",
        "      self.state_dim = state_dim\n",
        "      # action space dimension\n",
        "      self.action_dim = action_dim\n",
        "      # replay buffer\n",
        "      self.memory = deque(maxlen=100000)\n",
        "      # current step, updated everytime the agent acts\n",
        "      self.step = 0\n",
        "\n",
        "      # TODO: Please initialize other variables as described above\n",
        "      self.eps = 1.0\n",
        "      self.eps_decay = 0.99999975\n",
        "      self.eps_min = 0.1\n",
        "      self.gamma = 0.9\n",
        "      self.batch_size = 32\n",
        "    "
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TPviLoSpAg5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Predict Q value\n",
        "\n",
        "Optimal value action function, $Q^*(s, a)$, is the most important function in this project. It's used to choose optimal action (cheatsheet) and to improve action policy (cheatsheet to q-learning). Let's implement `agent.predict()` to predict $Q^*(s, a)$.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwoZWE97LJXf",
        "colab_type": "text"
      },
      "source": [
        "### Define Q function\n",
        "\n",
        "As a reminder, `env.step(action)` is the endpoint for which the environment provides feedback to the agent's action. Its output is of type `LazyFrame`, and we need to normalize it into an numpy array before doing any more processing. \n",
        "\n",
        "Because input to *Q(s, a)* is a stack of images(`LazyFrame`), let's use a *convolution neural network(ConvNet)* as our Q function. Instead of passing action together with state(image) into the Q function, we pass only state to the ConvNet, and it would return a list of real values representing Q values mapping to each possible action. We then choose the specific Q value based on the given action. \n",
        "\n",
        "Let's now look at Q-learning more closely.\n",
        "\n",
        "$$\n",
        "Q^*(s, a) \\leftarrow Q^*(s, a)+\\alpha (r + \\gamma \\max_{a'} Q^*(s', a') -Q^*(s, a))\n",
        "$$\n",
        "\n",
        "$r + \\gamma \\max_{a'} Q^*(s', a')$ is the *TD target* (cheatsheet) and $Q^*(s, a)$ is the *TD estimate* (cheatsheet). $s$ and $a$ are the current state and action, and $s'$ and $a'$ are next state and next action. \n",
        "\n",
        "We use $Q_{online}$ to represent $Q^*(s, a)$ and $Q_{target}$ to represent $Q^*(s', a')$. \n",
        "\n",
        "\n",
        "### Instructions\n",
        "\n",
        "We have implemented `ConvNet`, a simple convolution neural network, for you. Define `self.online_q` and `self.target_q` as two separate `ConvNet`s. `ConvNet` takes two parameters: input dimension `input_dim` and output dimension `output_dim`. Initialize input dimension with state dimension and output dimension with action dimension. \n",
        "\n",
        "Example:\n",
        "```\n",
        "self.neural_net = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BuvQ1-RpPF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "      super().__init__(state_dim, action_dim)\n",
        "      # TODO: define online action value function\n",
        "      self.online_q = ConvNet(input_dim=self.state_dim, output_dim=self.action_dim)\n",
        "      # TODO: define target action value function\n",
        "      self.target_q = ConvNet(input_dim=self.state_dim, output_dim=self.action_dim)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY-WQ4_MCvjB",
        "colab_type": "text"
      },
      "source": [
        "### Call Q function\n",
        "\n",
        "### Instruction \n",
        "\n",
        "Q function takes a single input `state`. Depending on the requested model ('online'/'target'), use the corresponding Q functons to calculate Q values of the given `state`. Note the result is Q values for all possible actions. \n",
        "\n",
        "Example:\n",
        "```\n",
        "q_values = q_function(state)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc1gXVO6Cv4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):\n",
        "      def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all possible actions using specified model (either online or target)\n",
        "        Input:\n",
        "          state\n",
        "           dimension of (batch_size * state_dim)\n",
        "          model\n",
        "           either 'online' or 'target'\n",
        "        Output\n",
        "          pred_q_values (torch.tensor)\n",
        "            dimension of (batch_size * action_dim), predicted Q values for all possible actions given the state\n",
        "        \"\"\"\n",
        "        # LazyFrame -> np array -> torch tensor\n",
        "        state_float = torch.FloatTensor(np.array(state))\n",
        "        # normalize\n",
        "        state_float = state_float / 255.\n",
        "        \n",
        "        if model == 'online':\n",
        "          # TODO return the predicted Q values using self.online_q\n",
        "          pred_q_values = self.online_q(state_float)\n",
        "        elif model == 'target':\n",
        "          # TODO return the predicted Q values using self.target_q\n",
        "          pred_q_values = self.target_q(state_float)\n",
        "\n",
        "        return pred_q_values"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU6KQb-AohNS",
        "colab_type": "text"
      },
      "source": [
        "## Act\n",
        "\n",
        "Let's now look at how Mario should `act()` in the environment. \n",
        "\n",
        "Given a state, Mario mostly chooses the action with the highest Q value (cheatsheet to optimal action). There is an *epislon* chance that Mario acts randomly instead, which encourages environment exploration. \n",
        "\n",
        "### Instruction\n",
        "\n",
        "We will use `torch.tensor` and `numpy.array` in this section. Familiarize yourself with [basic syntax with some examples](https://colab.research.google.com/drive/1D8k6i-TIKfqEHVkzKwYMjJvZRAKe9IuH?usp=sharing).  \n",
        "\n",
        "We will now implement `Mario.act()`. Recall that we have defined $Q_{online}$ above, which we will use here to calculate Q values for all actions given *state*. We then need to select the action that results in largest Q value. We have set up the logic for epsilon-greedy policy, and leave it to you to determine the optimal and random action. \n",
        "\n",
        "Before implementing `Mario.act()`, let's first get used to basic operations on *torch.tensor*, which is the data type returned in `Mario.predict()`\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYs9t-6nCho",
        "colab_type": "text"
      },
      "source": [
        "###     Act Function\n",
        "Now it's time to implement mario.act()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW3W8i2Dpoxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action and update value of step\n",
        "        Input\n",
        "          state(np.array) \n",
        "            A single observation of the current state, dimension is (state_dim)\n",
        "        Output\n",
        "          action\n",
        "            An integer representing which action agent will perform\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.eps:\n",
        "          # TODO: choose a random action from all possible actions (self.action_dim)\n",
        "          action = np.random.randint(self.action_dim)\n",
        "        else:\n",
        "          state = np.expand_dims(state, 0)\n",
        "          # TODO: choose the best action based on self.online_q\n",
        "          action_values = self.predict(state, model='online')\n",
        "          action = torch.argmax(action_values, axis=1).item()\n",
        "          \n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyw4YkzkqgpZ",
        "colab_type": "text"
      },
      "source": [
        "## Remember\n",
        "\n",
        "In order to improve policy, Mario need to collect and save past experiences. Each time agent performs an action, it collects an experience which includes the current state, action it performs, the next state after performing the action, the reward it collected, and whether the game is finished or not. We use a Queue structure to save historic experience, consisting of (state, next_state, action, reward, done). We will refer to this Queue as our memory. \n",
        "\n",
        "### Instruction\n",
        "\n",
        "Implement `Mario.remember()` to save the experience to Mario's memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H488eKESqg59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the experience to self.memory\n",
        "        Input\n",
        "          experience =  (state, next_state, action, reward, done) tuple\n",
        "        Output\n",
        "            None\n",
        "        \"\"\"\n",
        "        # TODO Add the experience to memory\n",
        "        self.memory.append(experience)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3j9G7Zq5OC",
        "colab_type": "text"
      },
      "source": [
        "## Learn\n",
        "\n",
        "\n",
        "The entire learning process is based on Q-learning algorithm (cheatsheet). By learning, we mean updating our Q function to better predict the Q value of given state-action pair. Recall that Q_online is the function that we are updating and using to choose actions. Q_target is used to determine the target Q value.  \n",
        "\n",
        "\n",
        "Some key steps to perform:\n",
        "- Experience Sampling: \n",
        "We will sample experiences from memory as the “training set” of the current learning step. \n",
        "\n",
        "\n",
        "- Predicting Online Q Values: \n",
        "We predict Q values for all sampled state-action pairs using Q_online.\n",
        "\n",
        "\n",
        "- Predicting Target Q Values:\n",
        "We predict Q values for all sampled state-action pairs using Q_target and reward. Unlike predicting online Q values, we don't directly calculate the Q value for the state-action pair. Instead we approximate it with the sum of reward and discounted Q value for the next state-action pair (cheatsheet). \n",
        "\n",
        "\n",
        "- Loss between Online and Target Q:\n",
        "In this step, we calculate the loss between predicted online Q values and target Q values. \n",
        "\n",
        "\n",
        "- Update Q_online: \n",
        "Update Q_online to minimize the loss using Adam optimizer. This improves the predicting accuracy of Q_online. \n",
        "\n",
        "\n",
        "Summarizing the above in pseudo code for `Mario.learn()`:\n",
        "\n",
        "```\n",
        "if enough experiences are collected:\n",
        "  sample a batch of experiences\n",
        "  calculate the predicted Q values using Q_online\n",
        "  calculate the target Q values using Q_target and reward\n",
        "  calculate loss between prediction and target Q values\n",
        "  update Q_online based on loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb3ip_Wgp0l-",
        "colab_type": "text"
      },
      "source": [
        "### Experience Sampling \n",
        "Mario learns by drawing past experiences from its memory. The memory is a queue data structure that stores each individual experience in the format of \n",
        "\n",
        "> state, next_state, action, reward, done\n",
        "\n",
        "Examples of some experiences in Mario's memory:\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1D34QpsmJSwHrdzszROt405ZwNY9LkTej)  next_state: ![pic](https://drive.google.com/uc?id=13j2TzRd1SGmFru9KJImZsY9DMCdqcr_J) action: jump reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v) next_state: ![pic](https://drive.google.com/uc?id=1hmGGVO1cS7N7kdcM99-K3Y2sxrAFd0Oh) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=10MHERSI6lap79VcZfHtIzCS9qT45ksk-) next_state: ![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1T6CAIMzNxeZlBTUdz3sB8t_GhDFbNdUO) next_state: ![pic](https://drive.google.com/uc?id=1aZlA0EnspQdcSQcVxuVmaqPW_7jT3lfW) action: jump_right reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1bPRnGRx2c1HJ_0y_EEOFL5GOG8sUBdIo) next_state: ![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv) action: right reward: 10.0 done: False\n",
        "\n",
        "State/next_state:\n",
        "Observation at timestep *t*/*t+1*. They are both of type `LazyFrame`. \n",
        "\n",
        "Action:\n",
        "Mario's action during state transition. \n",
        "\n",
        "Reward:\n",
        "Environment's reward during state transition. \n",
        "\n",
        "Done:\n",
        "Boolean indicating if next_state is a terminal state (end of game). Terminal state has a known Q value of 0. \n",
        "\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Sample a batch of experiences from `self.memory` of size `self.batch_size`. \n",
        "\n",
        "Return a tuple of numpy arrays, in the order of (state, next_state, action, reward, done). Each numpy array should have its first dimension equal to `self.batch_size`. \n",
        "\n",
        "To convert a `LazyFrame` to numpy array, do\n",
        "\n",
        "```\n",
        "state_np_array = np.array(state_lazy_frame)\n",
        "```\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR3QdqXmhr7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "class Mario(Mario):\n",
        "  def sample_batch(self):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      self.memory (FIFO queue)\n",
        "        a queue where each entry has five elements as below\n",
        "        state: LazyFrame of dimension (state_dim)\n",
        "        next_state: LazyFrame of dimension (state_dim)\n",
        "        action: integer, representing the action taken\n",
        "        reward: float, the reward from state to next_state with action\n",
        "        done: boolean, whether state is a terminal state\n",
        "      self.batch_size (int)\n",
        "        size of the batch to return \n",
        "\n",
        "    Output\n",
        "      state, next_state, action, reward, done (tuple)\n",
        "        a tuple of numpy arrays: state, next_state, action, reward, done\n",
        "        state: numpy array of dimension (batch_size x state_dim)\n",
        "        next_state: numpy array of dimension (batch_size x state_dim)\n",
        "        action: numpy array of dimension (batch_size)\n",
        "        reward: numpy array of dimension (batch_size)\n",
        "        done: numpy array of dimension (batch_size)\n",
        "    \"\"\"\n",
        "    # TODO convert everything into numpy array \n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    state, next_state, action, reward, done = map(np.array, zip(*batch))\n",
        "    return state, next_state, action, reward, done"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOALqrSC5VIf",
        "colab_type": "text"
      },
      "source": [
        "### Estimated Q Value\n",
        "\n",
        "This is referred as the *TD estimate* (cheatsheet) in Q-learning. It is the estimated value of current state-action pair $Q^*(s, a)$. We will use $Q_{online}$ to calculate this. \n",
        "\n",
        "Recall our defined `Mario.predict()` above:\n",
        "```\n",
        "q_values = self.predict(state, model='online')\n",
        "```\n",
        "\n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of states (s) and actions (a), calculate the estimated value for each state-action pair Q(s, a). Return the results in `torch.tensor` format. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJx-LMQmKMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):  \n",
        "  def calculate_prediction_q(self, state, action):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the current state \n",
        "      action (np.array)\n",
        "        dimension is (batch_size), each item is an integer representing the \n",
        "        action taken for current state \n",
        "\n",
        "    Output\n",
        "      pred_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a predicted Q value of the \n",
        "        current state-action pair \n",
        "    \"\"\"\n",
        "    curr_state_q = self.predict(state, model='online')\n",
        "    # TODO select specific Q values based on input actions \n",
        "    curr_state_q = curr_state_q[np.arange(0, self.batch_size), action]\n",
        "\n",
        "    return curr_state_q"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6efQFg5WCD",
        "colab_type": "text"
      },
      "source": [
        "### Target Q Value\n",
        "\n",
        "This is the *TD target* (cheatsheet) in Q-learning. It is the estimated value of current state-action pair using next state and reward $r+\\gamma Q^*(s', a')$. We use $Q_{target}$ to calculate the target Q value. \n",
        "\n",
        "\n",
        "A small caveat is the terminal state, as recorded with the variable `done`, which is 1 when Mario is dead or the game finishes. \n",
        "\n",
        "Hence, we need to make sure we don't keep adding future rewards when \"there is no future\", i.e. when the game reaches terminal state. Since `done` is a boolean, we can multiply `1. - done` with future reward. This way, future reward after the terminal state is not taken into account in TD target.\n",
        "\n",
        "The complete *TD target* are in the form of \n",
        "\n",
        "$$\n",
        "r + (1 - done) \\gamma \\max_{a'} Q^*(s', a')\n",
        "$$\n",
        "\n",
        "$r$ is the current reward, $s'$ is the next state, $a'$ is the next action. We use $Q_{target}$ to calculate $Q^*(s', a')$. Because we don't know what next action $a'$ will be, we estimate it using next state $s'$ and $Q_{online}$. Specifically,\n",
        "\n",
        "$$\n",
        "a' = argmax_a Q_{online}(s', a)\n",
        "$$\n",
        "\n",
        "Let's calculate *TD Target* now. \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of next_states $s'$ and rewards $r$, calculate the *TD target*. Note that $a'$ is not explicitly given, so we will need to first obtain that using $Q_{online}$ and next state $s'$.\n",
        "\n",
        "Return the results in `torch.tensor` format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_yrSjmhsCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):  \n",
        "  def calculate_target_q(self, next_state, reward):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      next_state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the next state \n",
        "      reward (np.array)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      target_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a target Q value of the current\n",
        "        state-action pair, calculated based on reward collected and \n",
        "        estimated Q value for next state\n",
        "    \"\"\"\n",
        "    next_state_q = self.predict(next_state, 'target')\n",
        "\n",
        "    online_q = self.predict(next_state, 'online')\n",
        "    # TODO select the best action at next state based on online Q function\n",
        "    action_idx = torch.argmax(online_q, axis=1)\n",
        "\n",
        "    # TODO calculate target Q values based on action_idx and reward\n",
        "    target_q = torch.tensor(reward) + (1. - done) * next_state_q[np.arange(0, self.batch_size), action_idx] * self.gamma\n",
        "    \n",
        "    return target_q"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkq2h_jg5Wn_",
        "colab_type": "text"
      },
      "source": [
        "### Loss\n",
        "\n",
        "Let's now calculate the loss between estimated and target Q values. Loss is what drives the optimization and updates $Q_{online}$ to better predict Q values in the future. We will calculate the mean squared loss as:\n",
        "\n",
        "$MSE = \\frac{1}{n}\\sum_{i=0}^n( y_i - \\hat{y}_i)^2$\n",
        "\n",
        "In Pytorch, this translates to:\n",
        "```\n",
        "loss = nn.functional.mse_loss(pred_q, target_q)\n",
        "```\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Given predicted and target Q values for the batch of experiences, return the Mean Squared Error. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnzaTGehsHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Mario(Mario):\n",
        "  def calculate_loss(self, pred_q, target_q):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      pred_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is an observation \n",
        "        for the next state \n",
        "      target_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the MSE loss of pred_q and target_q\n",
        "    \"\"\"\n",
        "    # TODO calculate mean squared error loss\n",
        "    loss = nn.functional.mse_loss(pred_q, target_q)\n",
        "    return loss"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-vyeniHBsdQ",
        "colab_type": "text"
      },
      "source": [
        "### Update $Q_{online}$\n",
        "\n",
        "As the final step to complete `Mario.learn()`, we use Adam optimizer to optimize upon the above calculated `loss`. This updates the parameters inside $Q_{online}$ function so that estimated Q values are closer to target Q values.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Behu1bxODdrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super().__init__(state_dim, action_dim)\n",
        "    # optimizer updates parameters in online_q using backpropagation\n",
        "    self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "  def update_online_q(self, loss):\n",
        "    '''\n",
        "    Input\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "      optimizer\n",
        "        optimizer updates parameter in our online_q neural network to reduce\n",
        "        the loss\n",
        "    '''\n",
        "    # update online_q\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    \n"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lex52LDkfmJp",
        "colab_type": "text"
      },
      "source": [
        "### Put them Together\n",
        "With all the helper methods implemented, let's revisit our `Mario.learn()` function. \n",
        "\n",
        "### Instructions\n",
        "\n",
        "We've added some logic on checking learning criterion. For the rest, use the helper methods defined above to complete `Mario.learn()` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acxIouqZflqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__(state_dim, action_dim)\n",
        "        # number of experiences to collect before training\n",
        "        self.burnin = 1e5\n",
        "        # number of experiences between updating online q\n",
        "        self.learn_every = 3\n",
        "        # number of experiences between updating target q with online q\n",
        "        self.sync_every = 1e4\n",
        "        # number of experiences between saving the current agent\n",
        "        self.save_every = 1e5\n",
        "        self.save_dir = os.path.join(\n",
        "            \"checkpoints\",\n",
        "            f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n",
        "        )\n",
        "        if not os.path.exists(self.save_dir):\n",
        "            os.makedirs(self.save_dir)\n",
        "\n",
        "    def sync_target_q(self):\n",
        "        \"\"\"Update target action value (Q) function with online action value (Q) function\n",
        "        \"\"\"\n",
        "        self.target_q.load_state_dict(self.online_q.state_dict())\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the current agent\n",
        "        \"\"\"\n",
        "        save_path = os.path.join(self.save_dir, f\"online_q_{self.step}.chkpt\")\n",
        "        torch.save(self.online_q.state_dict(), save_path)\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.sync_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "\n",
        "        # TODO: sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = self.sample_batch()\n",
        "\n",
        "        # TODO: calculate prediction Q values for the batch\n",
        "        pred_q = self.calculate_prediction_q(state, action)\n",
        "\n",
        "        # TODO: calculate target Q values for the batch\n",
        "        target_q = self.calculate_target_q(next_state, reward)\n",
        "\n",
        "        # TODO: calculate huber loss of target and prediction values\n",
        "        loss = self.calculate_loss(pred_q, target_q)\n",
        "        \n",
        "        # TODO: update target network\n",
        "        self.update_online_q(loss)\n",
        "        print('udpating')\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49CcL-pvKdA",
        "colab_type": "text"
      },
      "source": [
        "## Completed Agent\n",
        "\n",
        "We now have all the key functionlities implemented! Glory is all yours. Here is the completed agent file, revisited. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StRaEpPovKli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from collections import deque\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import numpy as np\n",
        "# import random\n",
        "# from neural import ConvNet\n",
        "# import os\n",
        "# import datetime\n",
        "\n",
        "# import pdb\n",
        "\n",
        "# class Mario:\n",
        "#     def __init__(self, state_dim, action_dim, max_memory, double_q):\n",
        "#         # state space dimension\n",
        "#         self.state_dim = state_dim\n",
        "#         # action space dimension\n",
        "#         self.action_dim = action_dim\n",
        "#         # replay buffer\n",
        "#         self.memory = deque(maxlen=max_memory)\n",
        "#         # if double_q, use best action from online_q for next state q value\n",
        "#         self.double_q = double_q\n",
        "#         # future reward discount rate\n",
        "#         self.gamma = 0.9\n",
        "#         # initial epsilon(random exploration rate)\n",
        "#         self.eps = 1\n",
        "#         # final epsilon\n",
        "#         self.eps_min = 0.1\n",
        "#         # epsilon decay rate\n",
        "#         self.eps_decay = 0.99999975\n",
        "#         # current step, updated everytime the agent acts\n",
        "#         self.step = 0\n",
        "#         # number of experiences between updating online q\n",
        "#         self.learn_every = 3\n",
        "#         # number of experiences to collect before training\n",
        "#         self.burnin = 1e5\n",
        "#         # self.burnin = 1e2\n",
        "#         # number of experiences between updating target q with online q\n",
        "#         self.sync_every = 1e4\n",
        "#         # number of experiences between saving the current agent\n",
        "#         self.save_every = 1e5\n",
        "#         # number of consecutive marios to save\n",
        "#         self.save_total = 5\n",
        "#         # a new directory to save marios to\n",
        "#         self.save_dir = os.path.join(\n",
        "#             \"checkpoints\",\n",
        "#             f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n",
        "#         )\n",
        "#         if not os.path.exists(self.save_dir):\n",
        "#             os.makedirs(self.save_dir)\n",
        "\n",
        "#         # batch size used to update online q\n",
        "#         self.batch_size = 32\n",
        "#         # train on cpu/gpu\n",
        "#         self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#         # online action value function, Q(s, a)\n",
        "#         self.online_q = ConvNet(input_dim=state_dim, output_dim=action_dim).to(self.device)\n",
        "#         # target action value function, Q'(s, a)\n",
        "#         self.target_q = ConvNet(input_dim=state_dim, output_dim=action_dim).to(self.device)\n",
        "#         # optimizer\n",
        "#         self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "#     def predict(self, state, model):\n",
        "#         \"\"\"Given a state, predict Q values of all actions\n",
        "#         model is either 'online' or 'target'\n",
        "#         \"\"\"\n",
        "#         # LazyFrame -> np array -> torch tensor\n",
        "#         state_float = torch.FloatTensor(np.array(state)).to(self.device)\n",
        "#         # normalize\n",
        "#         state_float = state_float / 255.\n",
        "#         if model == 'online':\n",
        "#             return self.online_q(state_float)\n",
        "#         if model == 'target':\n",
        "#             return self.target_q(state_float)\n",
        "\n",
        "#     def act(self, state):\n",
        "#         \"\"\"Given a state, choose an epsilon-greedy action\n",
        "#         \"\"\"\n",
        "#         if np.random.rand() < self.eps:\n",
        "#             # random action\n",
        "#             action = np.random.randint(low=0, high=self.action_dim)\n",
        "#         else:\n",
        "#             # policy action\n",
        "#             q = self.predict(np.expand_dims(state, 0), model='online')\n",
        "#             action = torch.max(q, axis=1)[1].item()\n",
        "#         # decrease eps\n",
        "#         self.eps *= self.eps_decay\n",
        "#         self.eps = max(self.eps_min, self.eps)\n",
        "#         # increment step\n",
        "#         self.step += 1\n",
        "#         return action\n",
        "\n",
        "#     def remember(self, experience):\n",
        "#         \"\"\"Add the observation to memory\n",
        "#         \"\"\"\n",
        "#         self.memory.append(experience)\n",
        "\n",
        "#     def learn(self):\n",
        "#         \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "#         \"\"\"\n",
        "#         # sync target network\n",
        "#         if self.step % self.sync_every == 0:\n",
        "#             self.sync_target_q()\n",
        "#         # checkpoint model\n",
        "#         if self.step % self.save_every < self.save_total:\n",
        "#             self.save_model()\n",
        "#         # break if burn-in\n",
        "#         if self.step < self.burnin:\n",
        "#             return None, None\n",
        "#         # break if no training\n",
        "#         if self.step % self.learn_every != 0:\n",
        "#             return None, None\n",
        "#         # sample batch\n",
        "#         batch = random.sample(self.memory, self.batch_size)\n",
        "#         state, next_state, action, reward, done = map(np.array, zip(*batch))\n",
        "#         # get next q values from target_q\n",
        "#         next_q = self.predict(next_state, 'target')\n",
        "#         # calculate discounted future reward\n",
        "#         if self.double_q:\n",
        "#             q = self.predict(next_state, 'online')\n",
        "#             q_idx = torch.max(q, axis=1)[1]\n",
        "#             target_q = torch.FloatTensor(reward).to(self.device) + torch.FloatTensor(1. - done).to(self.device) \\\n",
        "#                 * self.gamma * next_q[np.arange(0, self.batch_size), q_idx]\n",
        "#         else:\n",
        "#             target_q = torch.FloatTensor(reward).to(self.device) + torch.FloatTensor(1. - done).to(self.device) \\\n",
        "#                 * self.gamma * torch.max(next_q, axis=1)[0]\n",
        "#         # get predicted q values from online_q and actions taken\n",
        "#         curr_q = self.predict(state, 'online')\n",
        "#         pred_q = curr_q[np.arange(0, self.batch_size), action]\n",
        "#         # huber loss\n",
        "#         loss = nn.functional.smooth_l1_loss(pred_q, target_q)\n",
        "#         # update online_q\n",
        "#         self.optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         self.optimizer.step()\n",
        "\n",
        "#         return pred_q.mean().item(), loss.item()\n",
        "\n",
        "\n",
        "#     def save_model(self):\n",
        "#         \"\"\"Save the current agent\n",
        "#         \"\"\"\n",
        "#         save_path = os.path.join(self.save_dir, f\"online_q_{self.step % self.save_total}.chkpt\")\n",
        "#         torch.save(self.online_q.state_dict(), save_path)\n",
        "\n",
        "\n",
        "#     def load_model(self, chkpt_dir, eps):\n",
        "#         # update epsilon\n",
        "#         self.eps = eps\n",
        "#         # a new directory to save marios to\n",
        "#         self.save_dir = os.path.join(\n",
        "#             \"checkpoints\",\n",
        "#             f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n",
        "#         )\n",
        "#         if not os.path.exists(self.save_dir):\n",
        "#             os.makedirs(self.save_dir)\n",
        "\n",
        "#         # load online q\n",
        "#         if not os.path.exists(chkpt_dir):\n",
        "#             return\n",
        "#         state_dict = torch.load(chkpt_dir, map_location=self.device)\n",
        "#         self.online_q.load_state_dict(state_dict)\n",
        "#         # sync target q\n",
        "#         self.sync_target_q()\n",
        "#         self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "\n",
        "#     def sync_target_q(self):\n",
        "#         \"\"\"Update target action value (Q) function with online action value (Q) function\n",
        "#         \"\"\"\n",
        "#         self.target_q.load_state_dict(self.online_q.state_dict())\n",
        "\n",
        "\n",
        "#     def replay(self, env, load_dir=None, load_idx=0):\n",
        "#         if not load_dir:\n",
        "#             dirs = os.listdir(\"checkpoints\")\n",
        "#             load_dir = sorted(dirs)[-1]\n",
        "#         load_path = os.path.join(\"checkpoints\", load_dir, f\"online_q_{load_idx}.chkpt\")\n",
        "\n",
        "#         if not os.path.exists(load_path):\n",
        "#             return\n",
        "#         state_dict = torch.load(load_path, map_location=self.device)\n",
        "#         self.online_q.load_state_dict(state_dict)\n",
        "#         self.eps = self.eps_min\n",
        "\n",
        "#         state = env.reset()\n",
        "#         total_reward = 0\n",
        "#         while True:\n",
        "#             env.render()\n",
        "#             action = self.act(state=state)\n",
        "#             next_state, reward, done, info = env.step(action=action)\n",
        "#             total_reward += reward\n",
        "#             state = next_state\n",
        "#             if done or info['flag_get']:\n",
        "#                 break\n",
        "#         return total_reward\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwxsUgYe9_8B",
        "colab_type": "text"
      },
      "source": [
        "# Start Learning! \n",
        "\n",
        "With the agent and environment wrappers implemented, we are ready to put Mario in the game and start learning! We will wrap the learning process in a big `for` loop that repeats the process of acting, remembering and learning by Mario. \n",
        "\n",
        "The meat of the algorithm is in the loop, let's take a closer look: \n",
        "\n",
        "### Instruction\n",
        "\n",
        "1.At the beginning of a new episode, we need to reinitialize the `state` by calling `env.reset()`\n",
        "\n",
        "2.Then we need several variables to hold the logging information we collected in this episode:\n",
        "\n",
        "`ep_reward`: reward collected in this episode\n",
        "\n",
        "`ep_num_steps`: total number of actions perforemed in this episode\n",
        "    \n",
        "`ep_total_loss`: total loss collected in this episode\n",
        "\n",
        "`ep_total_q`: total loss collected in this episode\n",
        "\n",
        "`ep_learn_length`: used for mean loss/q_value\n",
        "\n",
        "\n",
        "3.Now we are inside the while loop that plays the game, and we can call `env.render()` to display the visual\n",
        "\n",
        "4.We want to apply the action policy on the current state by calling `agent.act`, remember action policy  $pi$ characterizes how the agent reacts to environment.\n",
        "\n",
        "5.Then after the agent performs the action, the environment will ouput its feedback, including information such as next state, reward, if Mario is dead(`done`), and some other info. Calling `env.step` with the agent's action should do the trick.\n",
        "\n",
        "6.Agent needs to remember the experience in this action he takes in this state, call `agent.remember` using all the environment output from the step above.\n",
        "\n",
        "7.Call `agent.learn`\n",
        "\n",
        "8.Update logging info\n",
        "\n",
        "9.Update state to get the evironment ready for the next interation of agent-environment/action-reward interactions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4boS5vr4-AJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "5ead3cb1-9cfc-48bb-a16c-b3fcb9508bc5"
      },
      "source": [
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n)\n",
        "episodes = 10000\n",
        "\n",
        "### for Loop that train the model num_episodes times by playing the game\n",
        "\n",
        "for e in range(episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = env.reset()\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # 6. Remember\n",
        "        mario.remember(experience=(state, next_state, action, reward, done))\n",
        "\n",
        "        # 7.Learn \n",
        "        mario.learn()\n",
        "\n",
        "        # 8.Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            print(f\"episode length: {ep_length}, reward: {ep_reward}\")\n",
        "            break"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode length: 40, reward: 231.0\n",
            "udpating\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-a22e477fa89d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# 7.Learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# 8.Logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-5ae6402df2c1>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# TODO: calculate target Q values for the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_target_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# TODO: calculate huber loss of target and prediction values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-121-c838ee3d9d8d>\u001b[0m in \u001b[0;36mcalculate_target_q\u001b[0;34m(self, next_state, reward)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mestimated\u001b[0m \u001b[0mQ\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mnext_state_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0monline_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'online'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-116-6a2e1b5063c0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0;31m# TODO return the predicted Q values using self.target_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           \u001b[0mpred_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred_q_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-6775a5487bfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# input: B x C x H x W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB4N7gPXpp0x",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Below is the fully functional `main` class, we added logging info that will help keep track of the status of the training. \n",
        "\n",
        "We have helped you initialize and applied the wrappers for the environment for you, and we also initialized the agent.\n",
        "\n",
        "In addition, we also added model saving functionality for you so that you can replay the model you trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFXMe9N4pxyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "import numpy as np\n",
        "import pdb\n",
        "import time\n",
        "# original environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "\n",
        "# define action space on the environment:\n",
        "# NOOP: no action\n",
        "# right: walk right\n",
        "# right, A: jump right\n",
        "# right, B: run right\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        "    )\n",
        "\n",
        "## apply environment wrappers\n",
        "env = apply_wrappers_to_env(env)\n",
        "\n",
        "# After applying environment wrappers, observation space (a.k.a state_dim) shrinks from\n",
        "# 240 (height) x 256 (width) x 3 (RGB color channels) \n",
        "# to \n",
        "# 4 (#frames) x 84 (height) x 84 (width)\n",
        "\n",
        "# dimensional parameters after reshaping\n",
        "state_dim = (4,84,84)\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# Intialize agent\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, max_memory=100000, double_q=True)\n",
        "\n",
        "# Logs\n",
        "log = {\n",
        "    \"rewards\": [],\n",
        "    \"lengths\": [],\n",
        "    \"losses\": [],\n",
        "    \"q_values\": []\n",
        "}\n",
        "log_file = os.path.join(agent.save_dir, \"log.txt\")\n",
        "\n",
        "# Timing\n",
        "start = time.time()\n",
        "step = 0\n",
        "\n",
        "# number of episodes\n",
        "num_episodes = 10000\n",
        "\n",
        "# Main training loop\n",
        "for e in range(num_episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = None\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "    ep_total_loss = 0.0\n",
        "    ep_total_q = 0.0\n",
        "    ep_learn_length = 1 # used for mean loss/q_value\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = None\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = None\n",
        "\n",
        "        # 6. Remember\n",
        "        agent.remember(experience=(None,None,None,None))\n",
        "\n",
        "        # 7.Learn (conditional) (80% time cost)\n",
        "        q_value, loss = None\n",
        "\n",
        "        # 8.Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "        if q_value and loss:\n",
        "            ep_total_loss += loss\n",
        "            ep_total_q += q_value\n",
        "            ep_learn_length += 1\n",
        "\n",
        "        # Update state\n",
        "        state = None\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            break\n",
        "\n",
        "    # Log info in this episode\n",
        "    log[\"rewards\"].append(ep_reward)\n",
        "    log[\"lengths\"].append(ep_length)\n",
        "    log[\"losses\"].append(np.round(ep_total_loss/ep_learn_length, 5))\n",
        "    log[\"q_values\"].append(np.round(ep_total_q/ep_learn_length, 5))\n",
        "\n",
        "    # Print & Log every 50th episode\n",
        "    if e % 50 == 0:\n",
        "        mean_reward = np.round(np.mean(log['rewards'][-100:]), 3)\n",
        "        mean_length = np.round(np.mean(log['lengths'][-100:]), 3)\n",
        "        mean_loss = np.round(np.mean(log['losses'][-100:]), 3)\n",
        "        mean_q_value = np.round(np.mean(log['q_values'][-100:]), 3)\n",
        "        eps = np.round(agent.eps, 3)\n",
        "        step_time = np.round((time.time() - start_time)/(agent.step - start_step), 3)\n",
        "        start_time = time.time()\n",
        "        start_step = agent.step\n",
        "        print(\n",
        "            f\"Episode {e} - \"\n",
        "            f\"Step {agent.step} - \"\n",
        "            f\"Step Time {step_time} - \"\n",
        "            f\"Epsilon {eps} - \"\n",
        "            f\"Mean Reward {mean_reward} - \"\n",
        "            f\"Mean Length {mean_length} - \"\n",
        "            f\"Mean Loss {mean_loss} - \"\n",
        "            f\"Mean Q Value {mean_q_value} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{e:8d}{agent.step:10d}{eps:10.3f}\"\n",
        "                f\"{mean_reward:15.3f}{mean_length:15.3f}{mean_loss:15.3f}{mean_q_value:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # Running on Colab, download checkpoints to local\n",
        "        if 'google.colab' in sys.modules:\n",
        "            from google.colab import files\n",
        "            files.download(os.path.join(agent.save_dir, \"online_q_1.chkpt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o7N4mNSQDK8",
        "colab_type": "text"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "One question one might ask why do we want to sample data points from all past experiences rather than the most recent ones(for example, from the previous episode), which are newly trained with higher accuracy. \n",
        "\n",
        "The intuition is behind the tradeoff between these two approaches:\n",
        "\n",
        "Do we want to train on data that are generated from a small-size dataset with relatively high quality or a huge-size dataset with relatively lower quality? \n",
        "\n",
        "The answer is the latter, because the more data we have, the more of a wholistic, comprehensive point of view we have on the overall behavior of the system we have, in our case, the Mario game. Limited size dataset has the danger of overfitting and overlooking bigger pictures of the entire action/state space. \n",
        "\n",
        "\n",
        "Remember, Reinforcement Learning is all about exploring different scenarios(state) and keeping improving based on trial and errors, generated from the interactions between the **agent**(action) and the **environmental feedback**(reward). \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Why use two Q functions?\n",
        "\n",
        "While mathematically Q(s', a') and Q(s, a) are all the same Q function, in practice we use a separate function to approximate each. This is to prevent the divergence problem during optimization. "
      ]
    }
  ]
}