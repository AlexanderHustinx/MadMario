{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuansongFeng/MadMario/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdoL_As2yKf9",
        "colab_type": "text"
      },
      "source": [
        "Pre-MVP tutorial for walking users through building a learning Mario. Guidelines for creating this notebook (feel free to add/edit):\n",
        "1. Extensive explanation (link to AI cheatsheet where necessary) \n",
        "2. Only ask for core logics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4hh1rNclV9",
        "colab_type": "text"
      },
      "source": [
        "Welcome to Mad Mario! \n",
        "\n",
        "We put together this project to walk you through fundamentals of reinforcement learning. Along the project, you will implement a smart Mario that learns to complete levels on itself. For now, we don't expect you to know anything about reinforcement learning. In case you wanna peek ahead, here is a [cheatsheet on RL basics](https://drive.google.com/file/d/1qwyemCX7o37OLbSky7Kkufg_YViWwL-g/view?usp=sharing) that we will refer to throughout the project. At the end of the tutorial, you will gain a solid understanding of RL fundamentals and implement a classic RL algorithm, Q-learning, on yourself. \n",
        "\n",
        "\n",
        "It's recommended that you have familiarity with Python and high school level of math/stats background -- that said, don't worry if those memory are blurry. Just leave comments anywhere you feel confused, and we will explain the section in more details. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6IAcrfyegjL",
        "colab_type": "text"
      },
      "source": [
        "Let's get started! \n",
        "\n",
        "First thing first, let's look at how Mario will learn: Just like when we first try the game, Mario enters the game not knowing anything about the game. It makes random action just to understand the game better. Each failure experience adds to Mario's memory, and as failure accumulates, Mario starts to see what actions are better in a particular scenario. Eventually Mario learns a good strategy and completes the level. \n",
        "\n",
        "Let's put the story into pesudo code.\n",
        "\n",
        "```\n",
        "for a total of N episodes:\n",
        "  for a total of M steps in each episode:\n",
        "    Mario makes an action\n",
        "    Game gives a feedback \n",
        "    Mario remembers the action and feedback\n",
        "    after building up some experiences:\n",
        "      Mario learns from experiences   \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzxAwU4f1-H",
        "colab_type": "text"
      },
      "source": [
        "In RL terminology: agent (Mario) interacts with environment (Game) by making actions, and environment responds with new state. Based on the collected (state, action, reward) information, agent learns to maximize its future return by optimizing its action policy. \n",
        "\n",
        "While these terms may sounds scary, in a short while they will all make sense. It'd be helpful to review the [cheatsheet]((https://drive.google.com/file/d/1qwyemCX7o37OLbSky7Kkufg_YViWwL-g/view?usp=sharing)), before we start coding. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgK3jHa65bW9",
        "colab_type": "text"
      },
      "source": [
        "# Environment\n",
        "Environemtn is a key concept in reinforcement learning. It's the world that Mario interacts with and learns from. Environment is characterized by state (refer to state in cheatsheet). In Mario, this is the game console consisting of tubes, mushrooms and all other goodies. When Mario makes an action, environment responds with a reward (refer to cheatsheet) and the next state.  \n",
        "\n",
        "\n",
        "To run Mario envirioment, do\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt3eAj6PC0nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym_super_mario_bros\n",
        "\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "  # render game output\n",
        "  env.render()\n",
        "\n",
        "  # choose random action\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  # perform action\n",
        "  env.step(action=action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNBusrylG2uN",
        "colab_type": "text"
      },
      "source": [
        "Examples of state and reward:\n",
        "\n",
        "state:\n",
        "![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90)\n",
        "reward: -10\n",
        "\n",
        "state:\n",
        "![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv)\n",
        "reward: 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bN0TFDnCpil",
        "colab_type": "text"
      },
      "source": [
        "## Wrappers\n",
        "\n",
        "Like in supervised machine learning, data preparation is crucial in reinforcement learning. For example, a standard pre-processing step is turning RGB image/videos to grayscale. This compresses input size without losing much information. \n",
        "\n",
        "**before pre-processing**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1c9-tUWFyk4u_vNNrkZo1Rg0e2FUcbF3N)\n",
        "<!-- ![picture](https://drive.google.com/uc?id=1s7UewXkmF4g_gZfD7vloH7n1Cr-D3YYX)\n",
        "![picture](https://drive.google.com/uc?id=1mXDt8rFLKT9a-YvhGOgGZT4bq0T2y7iw) -->\n",
        "\n",
        "**after pre-processing**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1ED9brgnbPmUZL43Bl_x2FDmXd-hsHBQt)\n",
        "<!-- ![picture](https://drive.google.com/uc?id=1PB1hHSPk6jIhSxVok2u2ntHjvE3zrk7W)\n",
        "![picture](https://drive.google.com/uc?id=1CYm5q71f_OlY_mqvZADuMMjPmcMgbjVW) -->\n",
        "\n",
        "We perform pre-processing on state through environment wrappers. By wrapping `env` like so \n",
        "```\n",
        "env = wrapper(env)\n",
        "```\n",
        "states from environment are transformed into our desired format.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXl7ISdnn5UF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*Instructions*\n",
        "\n",
        "We want to apply 3 built-in wrappers to the given `env`, `GrayScaleObservation`, `ResizeObservation`, and `FrameStack`.  \n",
        "\n",
        "https://github.com/openai/gym/tree/master/gym/wrappers\n",
        "\n",
        "\n",
        "`FrameStack` is a wrapper that will allow us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can differentiate between when Mario was landing or jumping based on his direction of movement in the previous several frames. \n",
        "\n",
        "We can start with the following arguments:\n",
        "`GrayScaleObservation`: keep_dim=False \n",
        "`ResizeObservation`: shape=84 \n",
        "`FrameStack`: num_stack=4 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ib7vjUD5cGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
        "import gym_super_mario_bros\n",
        "\n",
        "# the original environment object \n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "\n",
        "# TODO wrap the given env with GrayScaleObservation\n",
        "env = None\n",
        "# TODO wrap the given env with ResizeObservation\n",
        "env = None\n",
        "# TODO wrap the given env with FrameStack \n",
        "env = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5DA19ABgiY",
        "colab_type": "text"
      },
      "source": [
        "## Custom Wrapper\n",
        "\n",
        "We also would like you to get a taste of implementing an environment wrapper on your own, instead of calling off-the-shelf packages. \n",
        "\n",
        "Here is an idea:\n",
        "As an effort of downsizing our model to make training faster, we can choose to skip every n-th frame. In other words, our wrapped environment will only output every n-th frame. Below is a skeleton of the class `SkipFrame`, inherited from `gym.Wrapper`.  Notice in the `__init__` function, the `_skip` field is overriden by the input parameter, default set at 4.\n",
        "\n",
        "\n",
        "However, it is important to accumulate the reward during these skipped steps, because the reward is the most important factor in determining the success of the learning model, so while we can skip frame for dimension reduction purpose, it is crucial we keep adding those rewards to our total reward. \n",
        "\n",
        "\n",
        "*Instruction*\n",
        "\n",
        "Implement the reward accumulation function, using your favorite for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZKyjJVHQBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = None\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzNYGLqcJ0Gt",
        "colab_type": "text"
      },
      "source": [
        "*Instruction*\n",
        "\n",
        "Our environment is already wrapped with three built-in wrappers. Now further wrap your `env` with `SkipFrame`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlemM08BrR-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: wrap the env from previous section with SkipFrame \n",
        "env = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VUq3afemcj-",
        "colab_type": "text"
      },
      "source": [
        "# Agent\n",
        "\n",
        "Agent is the other core concept in reinforcement learning. It interacts with environemnt by making actions (link to cheatsheet). We call the rules by which Mario acts upon action policy (link to cheatsheet). For example in the below code, the agent is making random actions in every step. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLWekHcuHfe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "\n",
        "while True:\n",
        "\n",
        "  # choose random action\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  # perform action\n",
        "  next_state, reward, done, info = env.step(action=action)\n",
        "\n",
        "  # update state\n",
        "  state = next_state\n",
        "\n",
        "  # game ends\n",
        "  if done:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmr1axsfHf4Y",
        "colab_type": "text"
      },
      "source": [
        "The agent class, `Mario`, captures Mario's behavior in the game environment. The agent should be able to \n",
        "\n",
        "- Make its decision about next action to take. Mario takes the environment state and acts following its optimal action (link to cheatsheet) \n",
        "\n",
        "- Remember past experiences. Later mario uses these experiences to update its action policy (link to cheatsheet)\n",
        "\n",
        "- Improve action policy over time. Mario updates its action policy following the Q-learning algorithm (link to cheatsheet). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FMfCSqgmv23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, max_memory):\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTdoKmKCovs",
        "colab_type": "text"
      },
      "source": [
        "## Initialize\n",
        "Before implementing the core functions, let's define some key parameters. For example, we use *epsilon* in action policy (refer to cheatsheet) to encourage exploration, use *gamma* to calculate discounted future return (link to cheatsheet). \n",
        "\n",
        "eps, float\n",
        "> Random Exploration Prabability. Under some probability, agent does not follow the optimal action policy (cheatsheet), but instead chooses a random actino to explore the environment. A high exploration rate is important at the early stage of learning to ensure proper exploration and not falling to local optima. The exploration rate should decrease as agent improves its policy. \n",
        "\n",
        "> Initialize to 1.0. \n",
        "\n",
        "\n",
        "\n",
        "eps_decay, float\n",
        "\n",
        "> Decay rate of eps. Agent rigorously explores space at the early stage, but gradually reduces its exploration rate to maintain action quality. In the later stage, agent already learns a fairly good policy, so we want it to follow its policy more frequently. Decrease eps by the factor of eps_decay each time the agent acts.\n",
        "\n",
        "> Initialize to 0.99999975. \n",
        "\n",
        "\n",
        "gamma, float\n",
        "> Future reward discount rate. *gamma* serves to make agent give higher weight on the short-term rewards over future reward.\n",
        "\n",
        "> Initialize to 0.9\n",
        "\n",
        "batch_size, int\n",
        "> Number of experiences used to update each time.\n",
        "\n",
        "> Please initialize it to 32\n",
        "\n",
        "state_dim, tuple of int\n",
        "\n",
        "> State space dimension. In Mario, this is 4 consecutive snapshots of the enviroment stacked together, where each snapshot is a 84*84 gray-scale image, so state_dim = (4, 84, 84).\n",
        "\n",
        "action_dim, int\n",
        ">  Action space dimension. In Mario, this is the number of total possible actions. \n",
        "\n",
        "max_memory, int\n",
        "> Size of Mario's memory. The memory is filled with Mario's past experiences. Each experience consists of (state, next_state, action, reward, done). Note we use a *queue* (FIFO) for storing memory. As Mario collects more experiences, old experiences are popped to make room for most recent ones. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxRwyg8D6gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, max_memory):\n",
        "       # state space dimension\n",
        "      self.state_dim = state_dim\n",
        "      # action space dimension\n",
        "      self.action_dim = action_dim\n",
        "      # replay buffer\n",
        "      self.memory = deque(maxlen=max_memory)\n",
        "      # current step, updated everytime the agent acts\n",
        "      self.step = 0\n",
        "\n",
        "      # TODO: Please initialize other variables as described above\n",
        "      pass\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TPviLoSpAg5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Predict Q value\n",
        "\n",
        "*Q(s, a)*, optimal value function of a state-action pair, is the most important function in this project. It's used to both choose optimal action (cheatsheet) and to improve action policy (cheatsheet to q-learning). Let's implement the method `agent.predict()`. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwoZWE97LJXf",
        "colab_type": "text"
      },
      "source": [
        "### Define Q function\n",
        "\n",
        "Because input to *Q(s, a)* is a stack of images, let's use a *convolution neural network* as our Q function. Instead of passing action together with state into the Q function, we pass only state, and obtain a list of Q values for all possible actions. We then choose the specific Q value based on the given action. \n",
        "\n",
        "Let's now look at Q-learning more closely. We use *Q\\*(s, a)* to define both the *TD target* (r + \\gamma max_a Q(s', a')) and current state-action value (Q(s, a)). While mathematically Q(s', a') and Q(s, a) are all the same Q function, in practice we use a separate function to approximate each. This is to prevent the divergence problem during optimization. We use Q_online to represent the Q(s, a) function, and Q_target to represent the Q(s', a') function. Q_online is used to make actual action decision by agent in `agent.act()`. Q_target is used to determine the optimization target for Q_online. \n",
        "\n",
        "### Instructions\n",
        "\n",
        "We have implemented `ConvNet`, a simple convolution neural network, for you. Define `self.online_q` and `self.target_q` as two separate `ConvNet`s. `ConvNet` takes two parameters: input dimension `input_dim` and output dimension `output_dim`. Initialize input dimension with state dimension and output dimension with action dimension. \n",
        "\n",
        "Example:\n",
        "```\n",
        "self.neural_net = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BuvQ1-RpPF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, ...):\n",
        "      # TODO: define online action value function\n",
        "      self.online_q = None\n",
        "      # TODO: define target action value function\n",
        "      self.target_q = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY-WQ4_MCvjB",
        "colab_type": "text"
      },
      "source": [
        "### Call Q function\n",
        "\n",
        "### Instruction \n",
        "\n",
        "Q function takes a single input `state`. Depending on the requested model ('online'/'target'), use the corresponding Q functons to calculate Q values of the given `state`. Note the result is Q values for all possible actions. \n",
        "\n",
        "Example:\n",
        "```\n",
        "q_values = q_function(state)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc1gXVO6Cv4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "      def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all possible actions using specified model (either online or target)\n",
        "        Input:\n",
        "          state\n",
        "           dimension of (batch_size * state_dim)\n",
        "          model\n",
        "           either 'online' or 'target'\n",
        "        Output\n",
        "          pred_q_values (torch.tensor)\n",
        "            dimension of (batch_size * action_dim), predicted Q values for all possible actions given the state\n",
        "        \"\"\"\n",
        "        # LazyFrame -> np array -> torch tensor\n",
        "        state_float = torch.FloatTensor(np.array(state))\n",
        "        # normalize\n",
        "        state_float = state_float / 255.\n",
        "        \n",
        "        # TODO return the predicted Q values for online/target function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU6KQb-AohNS",
        "colab_type": "text"
      },
      "source": [
        "## Act\n",
        "\n",
        "The *act* function defines how agent reacts to current state (cheatsheet). Given a state, the agent chooses the optimal action to perform based on the policy(Q function), or sometimes it would act randomly regardless of the policy to explore the state space. Please refer to the optimal action section in the cheatsheet. To choose an action, we need to predict the Q values for all possible actions in action dimension, and choose the one that gives the highest Q-value.\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhhV_Sxk2FSQ",
        "colab_type": "text"
      },
      "source": [
        "###    Small examples on NumPy indexing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boIQ-Hi22Jf6",
        "colab_type": "code",
        "outputId": "f15f7775-7a8f-43a5-ab56-3d0a0f83f3d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [7, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoaaXCcL2jLz",
        "colab_type": "text"
      },
      "source": [
        "To get element at 1st row, 2nd col:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za-Gi-Bd2tPc",
        "colab_type": "code",
        "outputId": "9fe54cb0-8f85-4ca6-a1f9-8200aa7c29b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = a[0,1]\n",
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WFiMf2U2zS-",
        "colab_type": "text"
      },
      "source": [
        "To get elements at (1st row, 2nd col) and (3rd row, 1st col):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51FS9adl3EMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = a[[0,2],[1,0]]\n",
        "c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqZH2MQI5S_C",
        "colab_type": "text"
      },
      "source": [
        "To get max numbers in a along 1st axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B44B1RuW3e3c",
        "colab_type": "code",
        "outputId": "9358cd41-3700-4ee7-8e5b-2fac1ed9d373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "d = np.max(a, axis = 0)\n",
        "d"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxwcGufG5NzO",
        "colab_type": "text"
      },
      "source": [
        "To get the indices of max numbers in a along 1st axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOx8WAE03rQm",
        "colab_type": "code",
        "outputId": "1b0585ad-e11a-4385-8ef0-0f80394094b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e = np.argmax(a, axis=0);\n",
        "e"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW3W8i2Dpoxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action and update value of step\n",
        "        Input\n",
        "          state(np.array) \n",
        "            A single observation of the current state, dimension is (state_dim)\n",
        "        Output\n",
        "          action\n",
        "            An integer representing which action agent will perform\n",
        "        \"\"\"\n",
        "        # TODO choose action with epsilon-greedy policy\n",
        "        if np.random.rand() < self.eps:\n",
        "          # random action\n",
        "          pass\n",
        "        else:\n",
        "          # policy action\n",
        "          pass\n",
        "          \n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyw4YkzkqgpZ",
        "colab_type": "text"
      },
      "source": [
        "## Remember\n",
        "\n",
        "As described above, the agent needs to remember its past experiences. We declared a deque named *memory* that stores prior experiences. The agent uses experiences to learn how to update its value prediction in the future. Each time the agent performs an action, it collects an experience, please append the experience to *memory*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H488eKESqg59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the experience to memory (deque)\n",
        "        Input\n",
        "          experience =  (state, next_state, action, reward, done) tuple\n",
        "            Each time agent performs an action, it collects an experience which characterize the current state,\n",
        "            action it performs under current state, the next state after performing the action, the reward it\n",
        "            collects after performing the action, and whether the game is finished or not\n",
        "        Output\n",
        "            None\n",
        "        \"\"\"\n",
        "        # TODO Add the experience to memory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3j9G7Zq5OC",
        "colab_type": "text"
      },
      "source": [
        "## Learn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The learning process relies on the Q-learning algorithm in RL Cheatsheet. Specifically, we make the observation that the prediction based on reward and next action-state value is more accurate than predicting directly the current action-state pair. \n",
        "\n",
        "There are some key steps to perform:\n",
        "- experience sampling: We will sample experience from all past experiences as the “training set” of the current iteration of improving the q value.\n",
        "\n",
        "\n",
        "- predicting online q values: Using the sampled experience, we try to predict online q values (q values for current state)  for all state-action pairs in the sampled batch, by invoking the predict() method\n",
        "\n",
        "\n",
        "- predicting target q values:\n",
        "Using the sampled experience, we try to predict target q values (q values for next_state) for all state-action pairs in the sampled batch, using next_state and reward, also by invoking the predict() method\n",
        "\n",
        "\n",
        "- calculate loss:\n",
        "Similar to all machine learning models, we need to measure how our prediction(pred) fit the actual results(label), and improve our model by trying to minimize that difference between them. In this step, we calculate the loss(Huber loss between the predicted online q value(pred) and the predicted target q value(label)\n",
        "- update online q function: Using the loss we computed, we update our online q function. That is how we keep improving the quality of our q value estimations by Temporal Difference Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmnJ8HDiq5Ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:  \n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # set up and check learning criterion \n",
        "\n",
        "        # sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = sample_batch(self.memory, self.batch_size)\n",
        "\n",
        "        # calculate prediction Q values for the batch\n",
        "        pred_q = calculate_prediction_q(state, action)\n",
        "\n",
        "        # calculate target Q values for the batch\n",
        "        target_q = calculate_target_q(next_state, reward)\n",
        "\n",
        "        # calculate huber loss of target and prediction values\n",
        "        loss = calculate_huber_loss(pred_q, target_q)\n",
        "        \n",
        "        # update target network\n",
        "        update_prediction_q(loss, optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb3ip_Wgp0l-",
        "colab_type": "text"
      },
      "source": [
        "### Experience Sampling \n",
        "Mario learns by drawing past experiences from its memory. The memory is a queue data structure that stores each individual experience in the format of \n",
        "\n",
        "> state, next_state, action, reward, done\n",
        "\n",
        "Examples of some experiences in Mario's memory:\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1D34QpsmJSwHrdzszROt405ZwNY9LkTej)  next_state: ![pic](https://drive.google.com/uc?id=13j2TzRd1SGmFru9KJImZsY9DMCdqcr_J) action: jump reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v) next_state: ![pic](https://drive.google.com/uc?id=1hmGGVO1cS7N7kdcM99-K3Y2sxrAFd0Oh) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=10MHERSI6lap79VcZfHtIzCS9qT45ksk-) next_state: ![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1T6CAIMzNxeZlBTUdz3sB8t_GhDFbNdUO) next_state: ![pic](https://drive.google.com/uc?id=1aZlA0EnspQdcSQcVxuVmaqPW_7jT3lfW) action: jump_right reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1bPRnGRx2c1HJ_0y_EEOFL5GOG8sUBdIo) next_state: ![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv) action: right reward: 10.0 done: False\n",
        "\n",
        "State and next_state are observations at timestep *t* and *t+1* respectively. They are both of type `LazyFrame`, which allows us to optimize memory usage. To convert a `LazyFrame` to numpy array, do\n",
        "\n",
        "```\n",
        "state_np_array = np.array(state_lazy_frame)\n",
        "```\n",
        "\n",
        "Action represents what Mario takes when the state transition happens. \n",
        "\n",
        "Reward is the feedback from environment after transition happens. \n",
        "\n",
        "Done indicates if next_state is a terminal state, which means Mario is dead. Terminal state by definition has a return value of 0.\n",
        "\n",
        "\n",
        "One question one might ask why do we want to sample data points from all past experiences rather than the most recent ones(for example, from the previous episode), which are newly trained with higher accuracy. \n",
        "\n",
        "The intuition is behind the tradeoff between these two approaches:\n",
        "\n",
        "Do we want to train on data that are generated from a small-size dataset with relatively high quality or a huge-size dataset with relatively lower quality? \n",
        "\n",
        "The answer is the latter, because the more data we have, the more of a wholistic, comprehensive point of view we have on the overall behavior of the system we have, in our case, the Mario game. Limited size dataset has the danger of overfitting and overlooking bigger pictures of the entire action/state space. \n",
        "\n",
        "\n",
        "Remember, Reinforcement Learning is all about exploring different scenarios(state) and keeping improving based on trial and errors, generated from the interactions between the agent(action) and the environmental feedback(reward). \n",
        "\n",
        "## Instruction\n",
        "\n",
        "Return a batch of experiences grouped by (state, next_state, action, reward, done) individually. Standardize all formats to numpy array. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR3QdqXmhr7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def sample_batch(memory, batch_size):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      memory (FIFO queue)\n",
        "        a queue where each entry has five elements as below\n",
        "        state: LazyFrame of dimension (state_dim)\n",
        "        next_state: LazyFrame of dimension (state_dim)\n",
        "        action: integer, representing the action taken\n",
        "        reward: float, the reward from state to next_state with action\n",
        "        done: boolean, whether state is a terminal state\n",
        "      batch_size (int)\n",
        "        size of the batch to return \n",
        "\n",
        "    Output\n",
        "      state, next_state, action, reward, done (tuple)\n",
        "        a tuple of five elements: state, next_state, action, reward, done\n",
        "        state: numpy array of dimension (batch_size x state_dim)\n",
        "        next_state: numpy array of dimension (batch_size x state_dim)\n",
        "        action: numpy array of dimension (batch_size)\n",
        "        reward: numpy array of dimension (batch_size)\n",
        "        done: numpy array of dimension (batch_size)\n",
        "    \"\"\"\n",
        "    return (None, None, None, None, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOALqrSC5VIf",
        "colab_type": "text"
      },
      "source": [
        "### Predicted Q Value\n",
        "\n",
        "The learning process relies on the Q-learning algorithm (refer to Q-learning in cheatsheet):\n",
        "\n",
        "> Q_p(s, a) <- Q_p(s, a) + α(r + γ max Q_t(s', a') - Q_p(s,a))\n",
        "\n",
        "where Q_p is the prediction value function, Q_t is the target value function, s and a are the current state and action, s' is the next state, a' is the best next action decided by Q_p and s' collectively. We use two separate neural networks to represent Q_p and Q_t. The neural networks learn to estimate state-action value (Q value) better over the learning process. All s, a and s' are retrieved from memory. \n",
        "\n",
        "The reason to have 2 value functions is to prevent divergence during the optimization. Q_p is used to make actual prediction of the current state-action value, while Q_t is used in conjunction with r to determine the target state-action value (refer to Temporal Difference Learning in Cheatsheet). In this section we make value prediction using Q_p. \n",
        "\n",
        "Ideally we pass both s and a to the Q_p function, which outputs the predicted value for the state-action pair. Imagine having 5 possible actions, this means passing the state-action pair to the Q_p neural network 5 times, which is very costly. To improve efficiency, we pass only the state to Q_p, which outputs predicted Q values for all possible actions at once. For example:\n",
        "\n",
        "Input\n",
        "\n",
        "state (s): ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v)\n",
        "\n",
        "Output\n",
        "- moving right (a_1): -10\n",
        "- jumping up (a_2): 10\n",
        "- jumping right (a_3): 0\n",
        "\n",
        "This gives us \n",
        "\n",
        "```\n",
        "Q_p(s, a_1) = -10\n",
        "Q_p(s, a_2) = 10\n",
        "Q_p(s, a_3) = 0\n",
        "```\n",
        "\n",
        "In our scenario, since the action is given (e.g. a_2), we can directly return the associated Q value, i.e. Q_p(s, a_2). \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of states (s) and actions (a), calculate the estimated value for each state-action pair Q(s, a). Return the results in `torch.tensor` format. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJx-LMQmKMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  # def predict()\n",
        "  \n",
        "  def calculate_prediction_q(state, action):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the current state \n",
        "      action (np.array)\n",
        "        dimension is (batch_size), each item is an integer representing the \n",
        "        action taken for current state \n",
        "\n",
        "    Output\n",
        "      pred_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a predicted Q value of the \n",
        "        current state-action pair \n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6efQFg5WCD",
        "colab_type": "text"
      },
      "source": [
        "### Target Q Value\n",
        "\n",
        "In this section we calculate the target Q value, in the form of\n",
        "\n",
        "> r + γ max Q_t(s', a')\n",
        "\n",
        "where r is the reward at transition from s to s',  γ  is the discounting factor, and s' is the next state. Because a' is not part of the actual experience (it is the predicted best action to take at next state), we will estimate it using our prediction value function Q_p by taking the argmax of Q_p(s', a') with respect to a'. \n",
        "\n",
        "> a' = argmax_a Q_p(s', a)\n",
        "\n",
        "Target Q value, in comparison to prediction Q value Q_p(s, a), gives a better estimate of the current state-action value. We want to update the predicted Q value Q_p(s, a) towards target Q value, r + γ max Q_t(s', a'). \n",
        "\n",
        "In this section we calculate the target Q value of current state-action. \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of next_states (s') and rewards (r), calculate the target Q value using above mentioned equation. Note that a' is not explicitly given, so we will need to first obtain that using prediction value function Q_p. \n",
        "\n",
        "Return the results in `torch.tensor` format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_yrSjmhsCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  # def predict()\n",
        "  \n",
        "  def calculate_target_q(next_state, reward):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      next_state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the next state \n",
        "      reward (np.array)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      target_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a target Q value of the current\n",
        "        state-action pair, calculated based on reward collected and \n",
        "        estimated Q value for next state\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkq2h_jg5Wn_",
        "colab_type": "text"
      },
      "source": [
        "### Loss between Prediction and Target Q Value\n",
        "\n",
        "To improve our value estimation, we would like our predicted Q value to be as close to the target Q value as possible. In other words, we want to minimize the distance between Q_p(s, a) and r + γ max Q_t(s', a'). To do this, we calculate the *huber loss* between the two values, and use this loss to update Q_p, the prediction value function. \n",
        "\n",
        "\n",
        "![pic](https://drive.google.com/uc?id=1FZM7sBnMgY5GQNTx-o3LtLRLQQM0mwat)\n",
        "\n",
        "\n",
        "Huber loss is a smoothed version of L1 loss. Graph above gives some intuition behind L1 vs. L2 vs. Huber loss. L1 is intolerant around the origin and gives a high loss when there is only small difference between predicted and target value. On the other hand, L2 loss explodes quickly when there is a big difference between predicted and target value. Huber loss conveniently avoids both issues. \n",
        "\n",
        "Hint: the huber loss can be called in this way\n",
        "```\n",
        "loss = nn.functional.smooth_l1_loss(input, target)\n",
        "```\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Given predicted and target Q values for the batch of experiences, return the sum of huber loss. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnzaTGehsHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def calculate_huber_loss(pred_q, target_q):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      pred_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is an observation \n",
        "        for the next state \n",
        "      target_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-vyeniHBsdQ",
        "colab_type": "text"
      },
      "source": [
        "### Update Online_Q based on loss\n",
        "\n",
        "After we calculated the loss between pred_q and target_q, we need to update the policy to reduce the decrepancy between them. To update the policy (the parameters in nueral network), we need to use optimizer which backpropages the loss to parameters in neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Behu1bxODdrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def __init__(self, ...):\n",
        "    # optimizer updates parameters in online_q using backpropagation\n",
        "    self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "  def update_prediction_q(self, loss, optimizer):\n",
        "    '''\n",
        "    Input\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "      optimizer\n",
        "        optimizer updates parameter in our online_q neural network to reduce\n",
        "        the loss\n",
        "    '''\n",
        "    pass\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lex52LDkfmJp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "With all the helper methods implemented, let's revisit our learn function. \n",
        "\n",
        "We need to set up the learning process and check some criterion. Logic is added for you. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acxIouqZflqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.copy_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "\n",
        "        # sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = sample_batch(self.memory, self.batch_size)\n",
        "\n",
        "        # calculate prediction Q values for the batch\n",
        "        pred_q = calculate_prediction_q(state, action)\n",
        "\n",
        "        # calculate target Q values for the batch\n",
        "        target_q = calculate_target_q(next_state, reward)\n",
        "\n",
        "        # calculate huber loss of target and prediction values\n",
        "        loss = calculate_huber_loss(pred_q, target_q)\n",
        "        \n",
        "        # update target network\n",
        "        update_prediction_q(loss, optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49CcL-pvKdA",
        "colab_type": "text"
      },
      "source": [
        "## Save\n",
        "\n",
        "We know have all the key functionlities realized. There are some additional helper methods that would be useful. For example:\n",
        "- to be able to save the agent\n",
        "\n",
        "We implemented these methods for you, and here is your completed agent file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StRaEpPovKli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from neural import ConvNet\n",
        "import pdb\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_memory, double_q):\n",
        "        # state space dimension\n",
        "        self.state_dim = state_dim\n",
        "        # action space dimension\n",
        "        self.action_dim = action_dim\n",
        "        # replay buffer\n",
        "        self.memory = deque(maxlen=max_memory)\n",
        "        # if double_q, use best action from online_q for next state q value\n",
        "        self.double_q = double_q\n",
        "        # future reward discount rate\n",
        "        self.gamma = 0.9\n",
        "        # initial epsilon(random exploration rate)\n",
        "        self.eps = 1\n",
        "        # final epsilon\n",
        "        self.eps_min = 0.1\n",
        "        # epsilon decay rate\n",
        "        self.eps_decay = 0.99999975\n",
        "        # current step, updated everytime the agent acts\n",
        "        self.step = 0\n",
        "        # number of experiences between updating online q\n",
        "        self.learn_every = 3\n",
        "        # number of experiences to collect before training\n",
        "        # self.burnin = 1e5\n",
        "        self.burnin = 1e2\n",
        "        # number of experiences between updating target q with online q\n",
        "        self.copy_every = 1e4\n",
        "        # number of experiences between saving the current agent\n",
        "        self.save_every = 5e5\n",
        "\n",
        "        # batch size used to update online q\n",
        "        self.batch_size = 32\n",
        "        # online action value function, Q(s, a)\n",
        "        self.online_q = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "        # target action value function, Q'(s, a)\n",
        "        self.target_q = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "        # optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "    def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all actions\n",
        "        model is either 'online' or 'target'\n",
        "        \"\"\"\n",
        "        state_float = torch.tensor(np.array(state)).float() / 255.\n",
        "        if model == 'online':\n",
        "            return self.online_q(state_float)\n",
        "        if model == 'target':\n",
        "            return self.target_q(state_float)\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.eps:\n",
        "            # random action\n",
        "            action = np.random.randint(low=0, high=self.action_dim)\n",
        "        else:\n",
        "            # policy action\n",
        "            q = self.predict(np.expand_dims(state, 0), model='online')\n",
        "            action = torch.max(q, axis=1)[1].item()\n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.copy_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "        # sample batch\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(np.array, zip(*batch))\n",
        "        # get next q values from target_q\n",
        "        next_q = self.predict(next_state, 'target')\n",
        "        # calculate discounted future reward\n",
        "        if self.double_q:\n",
        "            q = self.predict(next_state, 'online')\n",
        "            q_idx = torch.max(q, axis=1)[1]\n",
        "            target_q = torch.tensor(reward) + torch.tensor(1. - done) * self.gamma * next_q[np.arange(0, self.batch_size), q_idx]\n",
        "        else:\n",
        "            target_q = torch.tensor(reward) + torch.tensor(1. - done) * self.gamma * torch.max(next_q, axis=1)[0]\n",
        "        # get predicted q values from online_q and actions taken\n",
        "        curr_q = self.predict(state, 'online')\n",
        "        pred_q = curr_q[np.arange(0, self.batch_size), action]\n",
        "        # huber loss\n",
        "        loss = nn.functional.mse_loss(pred_q, target_q)\n",
        "        # update online_q\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the current agent\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def sync_target_q(self):\n",
        "        \"\"\"Update target action value (Q) function with online action value (Q) function\n",
        "        \"\"\"\n",
        "        self.target_q.load_state_dict(self.online_q.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwxsUgYe9_8B",
        "colab_type": "text"
      },
      "source": [
        "# Start Learning! \n",
        "\n",
        "After finishing the implementing the Reinforcement Learning model and the environment wrappers, we are ready to build our main driver function that will get our Mario up and running. The purpose of the `main` function is to call the agent and environment wrappers we already built.\n",
        "\n",
        "We will a big `while` loop that will keep running until we quit game or stop training(by checking the `done` variable), and repeat again for `num_episodes` number of episodes.\n",
        "\n",
        "The meat of the algorithm is in the main loop, let's take a closer look: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbI2hCO1uChJ",
        "colab_type": "text"
      },
      "source": [
        "**Instruction:**\n",
        "\n",
        "1.At the beginning of a new episode, we need to reinitialize the `state` by calling `env.reset()`\n",
        "\n",
        "2.Then we need several variables to hold the logging information we collected in this episode:\n",
        "\n",
        "`ep_reward`: reward collected in this episode\n",
        "\n",
        "`ep_num_steps`: total number of actions perforemed in this episode\n",
        "    \n",
        "`ep_total_loss`: total loss collected in this episode\n",
        "\n",
        "`ep_total_q`: total loss collected in this episode\n",
        "\n",
        "`ep_learn_length`: used for mean loss/q_value\n",
        "\n",
        "\n",
        "3.Now we are inside the while loop that plays the game, and we can call `env.render()` to display the visual\n",
        "\n",
        "4.We want to apply the action policy on the current state by calling `agent.act`, remember action policy  $pi$ characterizes how the agent reacts to environment.\n",
        "\n",
        "5.Then after the agent performs the action, the environment will ouput its feedback, including information such as next state, reward, if Mario is dead(`done`), and some other info. Calling `env.step` with the agent's action should do the trick.\n",
        "\n",
        "6.Agent needs to remember the experience in this action he takes in this state, call `agent.remember` using all the environment output from the step above.\n",
        "\n",
        "7.Call `agent.learn`\n",
        "\n",
        "8.Update logging info\n",
        "\n",
        "9.Update state to get the evironment ready for the next interation of agent-environment/action-reward interactions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4boS5vr4-AJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### for Loop that train the model num_episodes times by playing the game\n",
        "\n",
        "for e in range(num_episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = None\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "    ep_total_loss = 0.0\n",
        "    ep_total_q = 0.0\n",
        "    ep_learn_length = 1 # used for mean loss/q_value\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = None\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = None\n",
        "\n",
        "        # 6. Remember\n",
        "        agent.remember(experience=(None,None,None,None))\n",
        "\n",
        "        # 7.Learn (conditional) (80% time cost)\n",
        "        q_value, loss = None\n",
        "\n",
        "        # 8.Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "        if q_value and loss:\n",
        "            ep_total_loss += loss\n",
        "            ep_total_q += q_value\n",
        "            ep_learn_length += 1\n",
        "\n",
        "        # Update state\n",
        "        state = None\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB4N7gPXpp0x",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Below is the fully functional `main` class, we added logging info that will help keep track of the status of the training. \n",
        "\n",
        "We have helped you initialize and applied the wrappers for the environment for you, and we also initialized the agent.\n",
        "\n",
        "In addition, we also added model saving functionality for you so that you can replay the model you trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFXMe9N4pxyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "import numpy as np\n",
        "import pdb\n",
        "import time\n",
        "# original environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "\n",
        "# define action space on the environment:\n",
        "# NOOP: no action\n",
        "# right: walk right\n",
        "# right, A: jump right\n",
        "# right, B: run right\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        "    )\n",
        "\n",
        "## apply environment wrappers\n",
        "env = apply_wrappers_to_env(env)\n",
        "\n",
        "# After applying environment wrappers, observation space (a.k.a state_dim) shrinks from\n",
        "# 240 (height) x 256 (width) x 3 (RGB color channels) \n",
        "# to \n",
        "# 4 (#frames) x 84 (height) x 84 (width)\n",
        "\n",
        "# dimensional parameters after reshaping\n",
        "state_dim = (4,84,84)\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# Intialize agent\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, max_memory=100000, double_q=True)\n",
        "\n",
        "# Logs\n",
        "log = {\n",
        "    \"rewards\": [],\n",
        "    \"lengths\": [],\n",
        "    \"losses\": [],\n",
        "    \"q_values\": []\n",
        "}\n",
        "log_file = os.path.join(agent.save_dir, \"log.txt\")\n",
        "\n",
        "# Timing\n",
        "start = time.time()\n",
        "step = 0\n",
        "\n",
        "# number of episodes\n",
        "num_episodes = 10000\n",
        "\n",
        "# Main training loop\n",
        "for e in range(num_episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = None\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "    ep_total_loss = 0.0\n",
        "    ep_total_q = 0.0\n",
        "    ep_learn_length = 1 # used for mean loss/q_value\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = None\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = None\n",
        "\n",
        "        # 6. Remember\n",
        "        agent.remember(experience=(None,None,None,None))\n",
        "\n",
        "        # 7.Learn (conditional) (80% time cost)\n",
        "        q_value, loss = None\n",
        "\n",
        "        # 8.Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "        if q_value and loss:\n",
        "            ep_total_loss += loss\n",
        "            ep_total_q += q_value\n",
        "            ep_learn_length += 1\n",
        "\n",
        "        # Update state\n",
        "        state = None\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            break\n",
        "\n",
        "    # Log info in this episode\n",
        "    log[\"rewards\"].append(ep_reward)\n",
        "    log[\"lengths\"].append(ep_length)\n",
        "    log[\"losses\"].append(np.round(ep_total_loss/ep_learn_length, 5))\n",
        "    log[\"q_values\"].append(np.round(ep_total_q/ep_learn_length, 5))\n",
        "\n",
        "    # Print & Log every 50th episode\n",
        "    if e % 50 == 0:\n",
        "        mean_reward = np.round(np.mean(log['rewards'][-100:]), 3)\n",
        "        mean_length = np.round(np.mean(log['lengths'][-100:]), 3)\n",
        "        mean_loss = np.round(np.mean(log['losses'][-100:]), 3)\n",
        "        mean_q_value = np.round(np.mean(log['q_values'][-100:]), 3)\n",
        "        eps = np.round(agent.eps, 3)\n",
        "        step_time = np.round((time.time() - start_time)/(agent.step - start_step), 3)\n",
        "        start_time = time.time()\n",
        "        start_step = agent.step\n",
        "        print(\n",
        "            f\"Episode {e} - \"\n",
        "            f\"Step {agent.step} - \"\n",
        "            f\"Step Time {step_time} - \"\n",
        "            f\"Epsilon {eps} - \"\n",
        "            f\"Mean Reward {mean_reward} - \"\n",
        "            f\"Mean Length {mean_length} - \"\n",
        "            f\"Mean Loss {mean_loss} - \"\n",
        "            f\"Mean Q Value {mean_q_value} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{e:8d}{agent.step:10d}{eps:10.3f}\"\n",
        "                f\"{mean_reward:15.3f}{mean_length:15.3f}{mean_loss:15.3f}{mean_q_value:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # Running on Colab, download checkpoints to local\n",
        "        if 'google.colab' in sys.modules:\n",
        "            from google.colab import files\n",
        "            files.download(os.path.join(agent.save_dir, \"online_q_1.chkpt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}