{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuansongFeng/MadMario/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdoL_As2yKf9",
        "colab_type": "text"
      },
      "source": [
        "Pre-MVP tutorial for walking users through building a learning Mario. Guidelines for creating this notebook (feel free to add/edit):\n",
        "1. Extensive explanation (link to AI cheatsheet where necessary) \n",
        "2. Only ask for core logics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4hh1rNclV9",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to Mad Mario! \n",
        "\n",
        "We put together this project to walk you through fundamentals of reinforcement learning. Along the project, you will implement a smart Mario that learns to complete levels on itself. For now, we don't expect you to know anything about reinforcement learning. In case you wanna peek ahead, here is a [cheatsheet on RL basics](https://drive.google.com/file/d/1qwyemCX7o37OLbSky7Kkufg_YViWwL-g/view?usp=sharing) that we will refer to throughout the project. At the end of the tutorial, you will gain a solid understanding of RL fundamentals and implement a classic RL algorithm, Q-learning, on yourself. \n",
        "\n",
        "\n",
        "It's recommended that you have familiarity with Python and high school level of math/stats background -- that said, don't worry if those memory are blurry. Just leave comments anywhere you feel confused, and we will explain the section in more details. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6IAcrfyegjL",
        "colab_type": "text"
      },
      "source": [
        "Let's get started! \n",
        "\n",
        "First thing first, let's look at how Mario will learn: Just like when we first try the game, Mario enters the game not knowing anything about the game. It makes random action just to understand the game better. Each failure experience adds to Mario's memory, and as failure accumulates, Mario starts to see what actions are better in a particular scenario. Eventually Mario learns a good strategy and completes the level. \n",
        "\n",
        "Let's put the story into pesudo code.\n",
        "\n",
        "```\n",
        "for a total of N episodes:\n",
        "  for a total of M steps in each episode:\n",
        "    Mario makes an action\n",
        "    Game gives a feedback \n",
        "    Mario remembers the action and feedback\n",
        "    after building up some experiences:\n",
        "      Mario learns from experiences   \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzxAwU4f1-H",
        "colab_type": "text"
      },
      "source": [
        "In RL terminology: agent (Mario) interacts with environment (Game) by choosing actions, and environment responds with reward and next state. Based on the collected (state, action, reward) information, agent learns to maximize its future return by optimizing its action policy. \n",
        "\n",
        "While these terms may sounds scary, in a short while they will all make sense. It'd be helpful to review the [cheatsheet]((https://drive.google.com/file/d/1qwyemCX7o37OLbSky7Kkufg_YViWwL-g/view?usp=sharing)), before we start coding. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgK3jHa65bW9",
        "colab_type": "text"
      },
      "source": [
        "# Environment\n",
        "Environemtn is a key concept in reinforcement learning. It's the world that Mario interacts with and learns from. Environment is characterized by state (refer to state in cheatsheet). In Mario, this is the game console consisting of tubes, mushrooms and all other goodies. When Mario makes an action, environment responds with a reward (refer to cheatsheet) and the next state.  \n",
        "\n",
        "\n",
        "To run Mario envirioment, do\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt3eAj6PC0nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym_super_mario_bros\n",
        "\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "  # render game output\n",
        "  env.render()\n",
        "\n",
        "  # choose random action\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  # perform action\n",
        "  env.step(action=action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNBusrylG2uN",
        "colab_type": "text"
      },
      "source": [
        "Examples of state and reward:\n",
        "\n",
        "state:\n",
        "![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90)\n",
        "reward: -10\n",
        "\n",
        "state:\n",
        "![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv)\n",
        "reward: 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bN0TFDnCpil",
        "colab_type": "text"
      },
      "source": [
        "## Wrappers\n",
        "\n",
        "Like in supervised machine learning, data preparation is crucial in reinforcement learning. For example, a standard pre-processing step is turning RGB image/videos to grayscale. This compresses input size without losing much information. \n",
        "\n",
        "**before pre-processing**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1c9-tUWFyk4u_vNNrkZo1Rg0e2FUcbF3N)\n",
        "<!-- ![picture](https://drive.google.com/uc?id=1s7UewXkmF4g_gZfD7vloH7n1Cr-D3YYX)\n",
        "![picture](https://drive.google.com/uc?id=1mXDt8rFLKT9a-YvhGOgGZT4bq0T2y7iw) -->\n",
        "\n",
        "**after pre-processing**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1ED9brgnbPmUZL43Bl_x2FDmXd-hsHBQt)\n",
        "<!-- ![picture](https://drive.google.com/uc?id=1PB1hHSPk6jIhSxVok2u2ntHjvE3zrk7W)\n",
        "![picture](https://drive.google.com/uc?id=1CYm5q71f_OlY_mqvZADuMMjPmcMgbjVW) -->\n",
        "\n",
        "We perform pre-processing on state through environment wrappers. By wrapping `env` like so \n",
        "```\n",
        "env = wrapper(env)\n",
        "```\n",
        "states from environment are transformed into our desired format.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXl7ISdnn5UF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*Instructions*\n",
        "\n",
        "We want to apply 3 built-in wrappers to the given `env`, `GrayScaleObservation`, `ResizeObservation`, and `FrameStack`.  \n",
        "\n",
        "https://github.com/openai/gym/tree/master/gym/wrappers\n",
        "\n",
        "\n",
        "`FrameStack` is a wrapper that will allow us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can differentiate between when Mario was landing or jumping based on his direction of movement in the previous several frames. \n",
        "\n",
        "We can start with the following arguments:\n",
        "`GrayScaleObservation`: keep_dim=False \n",
        "`ResizeObservation`: shape=84 \n",
        "`FrameStack`: num_stack=4 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ib7vjUD5cGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
        "import gym_super_mario_bros\n",
        "\n",
        "# the original environment object \n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "\n",
        "# TODO wrap the given env with GrayScaleObservation\n",
        "env = None\n",
        "# TODO wrap the given env with ResizeObservation\n",
        "env = None\n",
        "# TODO wrap the given env with FrameStack \n",
        "env = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5DA19ABgiY",
        "colab_type": "text"
      },
      "source": [
        "## Custom Wrapper\n",
        "\n",
        "We also would like you to get a taste of implementing an environment wrapper on your own, instead of calling off-the-shelf packages. \n",
        "\n",
        "Here is an idea:\n",
        "As an effort of downsizing our model to make training faster, we can choose to skip every n-th frame. In other words, our wrapped environment will only output every n-th frame. Below is a skeleton of the class `SkipFrame`, inherited from `gym.Wrapper`.  Notice in the `__init__` function, the `_skip` field is overriden by the input parameter, default set at 4.\n",
        "\n",
        "\n",
        "However, it is important to accumulate the reward during these skipped steps, because the reward is the most important factor in determining the success of the learning model, so while we can skip frame for dimension reduction purpose, it is crucial we keep adding those rewards to our total reward. \n",
        "\n",
        "\n",
        "*Instruction*\n",
        "\n",
        "Implement the reward accumulation function, using your favorite for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZKyjJVHQBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = None\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzNYGLqcJ0Gt",
        "colab_type": "text"
      },
      "source": [
        "*Instruction*\n",
        "\n",
        "Our environment is already wrapped with three built-in wrappers. Now further wrap your `env` with `SkipFrame`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlemM08BrR-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: wrap the env from previous section with SkipFrame \n",
        "env = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VUq3afemcj-",
        "colab_type": "text"
      },
      "source": [
        "# Agent\n",
        "\n",
        "Agent is the other core concept in reinforcement learning. It interacts with environemnt by making actions (link to cheatsheet). We call the rules by which Mario acts upon action policy (link to cheatsheet). For example in the below code, the agent is making random actions in every step. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLWekHcuHfe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "\n",
        "while True:\n",
        "\n",
        "  # choose random action\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  # perform action\n",
        "  next_state, reward, done, info = env.step(action=action)\n",
        "\n",
        "  # update state\n",
        "  state = next_state\n",
        "\n",
        "  # game ends\n",
        "  if done:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmr1axsfHf4Y",
        "colab_type": "text"
      },
      "source": [
        "The agent class, `Mario`, captures Mario's behavior in the game environment. The agent should be able to \n",
        "\n",
        "- Make its decision about next action to take. Mario takes the environment state and acts following its optimal action (link to cheatsheet) \n",
        "\n",
        "- Remember past experiences. Later mario uses these experiences to update its action policy (link to cheatsheet)\n",
        "\n",
        "- Improve action policy over time. Mario updates its action policy following the Q-learning algorithm (link to cheatsheet). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FMfCSqgmv23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, max_memory):\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTdoKmKCovs",
        "colab_type": "text"
      },
      "source": [
        "## Initialize\n",
        "Before implementing the core functions, let's define some key parameters. For example, we use *epsilon* in action policy (refer to cheatsheet) to encourage exploration, use *gamma* to calculate discounted future return (link to cheatsheet). \n",
        "\n",
        "eps, float\n",
        "> Random Exploration Prabability. Under some probability, agent does not follow the optimal action policy (cheatsheet), but instead chooses a random actino to explore the environment. A high exploration rate is important at the early stage of learning to ensure proper exploration and not falling to local optima. The exploration rate should decrease as agent improves its policy. \n",
        "\n",
        "> Initialize to 1.0. \n",
        "\n",
        "\n",
        "\n",
        "eps_decay, float\n",
        "\n",
        "> Decay rate of eps. Agent rigorously explores space at the early stage, but gradually reduces its exploration rate to maintain action quality. In the later stage, agent already learns a fairly good policy, so we want it to follow its policy more frequently. Decrease eps by the factor of eps_decay each time the agent acts.\n",
        "\n",
        "> Initialize to 0.99999975. \n",
        "\n",
        "\n",
        "gamma, float\n",
        "> Future reward discount rate. *gamma* serves to make agent give higher weight on the short-term rewards over future reward.\n",
        "\n",
        "> Initialize to 0.9\n",
        "\n",
        "batch_size, int\n",
        "> Number of experiences used to update each time.\n",
        "\n",
        "> Please initialize it to 32\n",
        "\n",
        "state_dim, tuple of int\n",
        "\n",
        "> State space dimension. In Mario, this is 4 consecutive snapshots of the enviroment stacked together, where each snapshot is a 84*84 gray-scale image, so state_dim = (4, 84, 84).\n",
        "\n",
        "action_dim, int\n",
        ">  Action space dimension. In Mario, this is the number of total possible actions. \n",
        "\n",
        "max_memory, int\n",
        "> Size of Mario's memory. The memory is filled with Mario's past experiences. Each experience consists of (state, next_state, action, reward, done). Note we use a *queue* (FIFO) for storing memory. As Mario collects more experiences, old experiences are popped to make room for most recent ones. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxRwyg8D6gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, max_memory):\n",
        "       # state space dimension\n",
        "      self.state_dim = state_dim\n",
        "      # action space dimension\n",
        "      self.action_dim = action_dim\n",
        "      # replay buffer\n",
        "      self.memory = deque(maxlen=max_memory)\n",
        "      # current step, updated everytime the agent acts\n",
        "      self.step = 0\n",
        "\n",
        "      # TODO: Please initialize other variables as described above\n",
        "      pass\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TPviLoSpAg5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Predict Q value\n",
        "\n",
        "*Q(s, a)*, optimal value function of a state-action pair, is the most important function in this project. It's used to both choose optimal action (cheatsheet) and to improve action policy (cheatsheet to q-learning). Let's implement the method `agent.predict()`. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwoZWE97LJXf",
        "colab_type": "text"
      },
      "source": [
        "### Define Q function\n",
        "\n",
        "Because input to *Q(s, a)* is a stack of images, let's use a *convolution neural network* as our Q function. Instead of passing action together with state into the Q function, we pass only state, and obtain a list of Q values for all possible actions. We then choose the specific Q value based on the given action. \n",
        "\n",
        "Let's now look at Q-learning more closely. We use *Q\\*(s, a)* to define both the *TD target* (r + \\gamma max_a Q(s', a')) and current state-action value (Q(s, a)). While mathematically Q(s', a') and Q(s, a) are all the same Q function, in practice we use a separate function to approximate each. This is to prevent the divergence problem during optimization. We use Q_online to represent the Q(s, a) function, and Q_target to represent the Q(s', a') function. Q_online is used to make actual action decision by agent in `agent.act()`. Q_target is used to determine the optimization target for Q_online. \n",
        "\n",
        "### Instructions\n",
        "\n",
        "We have implemented `ConvNet`, a simple convolution neural network, for you. Define `self.online_q` and `self.target_q` as two separate `ConvNet`s. `ConvNet` takes two parameters: input dimension `input_dim` and output dimension `output_dim`. Initialize input dimension with state dimension and output dimension with action dimension. \n",
        "\n",
        "Example:\n",
        "```\n",
        "self.neural_net = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BuvQ1-RpPF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, ...):\n",
        "      # TODO: define online action value function\n",
        "      self.online_q = None\n",
        "      # TODO: define target action value function\n",
        "      self.target_q = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY-WQ4_MCvjB",
        "colab_type": "text"
      },
      "source": [
        "### Call Q function\n",
        "\n",
        "### Instruction \n",
        "\n",
        "Q function takes a single input `state`. Depending on the requested model ('online'/'target'), use the corresponding Q functons to calculate Q values of the given `state`. Note the result is Q values for all possible actions. \n",
        "\n",
        "Example:\n",
        "```\n",
        "q_values = q_function(state)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc1gXVO6Cv4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "      def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all possible actions using specified model (either online or target)\n",
        "        Input:\n",
        "          state\n",
        "           dimension of (batch_size * state_dim)\n",
        "          model\n",
        "           either 'online' or 'target'\n",
        "        Output\n",
        "          pred_q_values (torch.tensor)\n",
        "            dimension of (batch_size * action_dim), predicted Q values for all possible actions given the state\n",
        "        \"\"\"\n",
        "        # LazyFrame -> np array -> torch tensor\n",
        "        state_float = torch.FloatTensor(np.array(state))\n",
        "        # normalize\n",
        "        state_float = state_float / 255.\n",
        "        \n",
        "        # TODO return the predicted Q values for online/target function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU6KQb-AohNS",
        "colab_type": "text"
      },
      "source": [
        "## Act\n",
        "\n",
        "Let's now look at how Mario should `act()` in the environment. \n",
        "\n",
        "Given a state, Mario mostly chooses the action with the highest Q value (cheatsheet to optimal action). There is an *epislon* chance that Mario acts randomly instead, which encourages environment exploration. \n",
        "\n",
        "### Instruction\n",
        "\n",
        "We will now implement `Mario.act()`. Recall that we have defined Q_online above, which we will use here to calculate Q values for all actions. We have set up the logic for epsilon-greedy policy, and leave it to you to determine the optimal and random action. \n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhhV_Sxk2FSQ",
        "colab_type": "text"
      },
      "source": [
        "###    Small examples on NumPy indexing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boIQ-Hi22Jf6",
        "colab_type": "code",
        "outputId": "f15f7775-7a8f-43a5-ab56-3d0a0f83f3d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [7, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoaaXCcL2jLz",
        "colab_type": "text"
      },
      "source": [
        "To get element at 1st row, 2nd col:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za-Gi-Bd2tPc",
        "colab_type": "code",
        "outputId": "9fe54cb0-8f85-4ca6-a1f9-8200aa7c29b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = a[0,1]\n",
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WFiMf2U2zS-",
        "colab_type": "text"
      },
      "source": [
        "To get elements at (1st row, 2nd col) and (3rd row, 1st col):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51FS9adl3EMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = a[[0,2],[1,0]]\n",
        "c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqZH2MQI5S_C",
        "colab_type": "text"
      },
      "source": [
        "To get max numbers in a along 1st axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B44B1RuW3e3c",
        "colab_type": "code",
        "outputId": "9358cd41-3700-4ee7-8e5b-2fac1ed9d373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "d = np.max(a, axis = 0)\n",
        "d"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxwcGufG5NzO",
        "colab_type": "text"
      },
      "source": [
        "To get the indices of max numbers in a along 1st axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOx8WAE03rQm",
        "colab_type": "code",
        "outputId": "1b0585ad-e11a-4385-8ef0-0f80394094b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e = np.argmax(a, axis=0);\n",
        "e"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW3W8i2Dpoxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action and update value of step\n",
        "        Input\n",
        "          state(np.array) \n",
        "            A single observation of the current state, dimension is (state_dim)\n",
        "        Output\n",
        "          action\n",
        "            An integer representing which action agent will perform\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.eps:\n",
        "          # TODO: choose a random action from all possible actions (self.action_dim)\n",
        "          pass\n",
        "        else:\n",
        "          # TODO: choose the best action based on self.online_q\n",
        "          pass\n",
        "          \n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyw4YkzkqgpZ",
        "colab_type": "text"
      },
      "source": [
        "## Remember\n",
        "\n",
        "In order to improve policy, Mario need to collect and save past experiences. We use a Queue structure to save historic experience, consisting of (state, next_state, action, reward, done). We will refer to this Queue as our memory. \n",
        "\n",
        "### Instruction\n",
        "\n",
        "Implement `Mario.remember()` to save the experience to Mario's memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H488eKESqg59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the experience to self.memory\n",
        "        Input\n",
        "          experience =  (state, next_state, action, reward, done) tuple\n",
        "            Each time agent performs an action, it collects an experience which characterize the current state,\n",
        "            action it performs under current state, the next state after performing the action, the reward it\n",
        "            collects after performing the action, and whether the game is finished or not\n",
        "        Output\n",
        "            None\n",
        "        \"\"\"\n",
        "        # TODO Add the experience to memory\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3j9G7Zq5OC",
        "colab_type": "text"
      },
      "source": [
        "## Learn\n",
        "\n",
        "\n",
        "The entire learning process is based on Q-learning algorithm (cheatsheet). By learning, we mean updating our Q function to better predict the Q value of given state-action pair. Recall that Q_online is the function that we are updating and using to choose actions. Q_target is used to determine the target Q value.  \n",
        "\n",
        "\n",
        "Some key steps to perform:\n",
        "- Experience Sampling: \n",
        "We will sample experiences from memory as the “training set” of the current learning step. \n",
        "\n",
        "\n",
        "- Predicting Online Q Values: \n",
        "We predict Q values for all sampled state-action pairs using Q_online.\n",
        "\n",
        "\n",
        "- Predicting Target Q Values:\n",
        "We predict Q values for all sampled state-action pairs using Q_target and reward. Unlike predicting online Q values, we don't directly calculate the Q value for the state-action pair. Instead we approximate it with the sum of reward and discounted Q value for the next state-action pair (cheatsheet). \n",
        "\n",
        "\n",
        "- Loss between Online and Target Q:\n",
        "In this step, we calculate the loss between predicted online Q values and target Q values. \n",
        "\n",
        "\n",
        "- Update Q_online: \n",
        "Update Q_online to minimize the loss using Adam optimizer. This improves the predicting accuracy of Q_online. \n",
        "\n",
        "\n",
        "Summarizing the above in pseudo code for `Mario.learn()`:\n",
        "\n",
        "```\n",
        "if enough experiences are collected:\n",
        "  sample a batch of experiences\n",
        "  calculate the predicted Q values using Q_online\n",
        "  calculate the target Q values using Q_target and reward\n",
        "  calculate loss between prediction and target Q values\n",
        "  update Q_online based on loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb3ip_Wgp0l-",
        "colab_type": "text"
      },
      "source": [
        "### Experience Sampling \n",
        "Mario learns by drawing past experiences from its memory. The memory is a queue data structure that stores each individual experience in the format of \n",
        "\n",
        "> state, next_state, action, reward, done\n",
        "\n",
        "Examples of some experiences in Mario's memory:\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1D34QpsmJSwHrdzszROt405ZwNY9LkTej)  next_state: ![pic](https://drive.google.com/uc?id=13j2TzRd1SGmFru9KJImZsY9DMCdqcr_J) action: jump reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v) next_state: ![pic](https://drive.google.com/uc?id=1hmGGVO1cS7N7kdcM99-K3Y2sxrAFd0Oh) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=10MHERSI6lap79VcZfHtIzCS9qT45ksk-) next_state: ![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1T6CAIMzNxeZlBTUdz3sB8t_GhDFbNdUO) next_state: ![pic](https://drive.google.com/uc?id=1aZlA0EnspQdcSQcVxuVmaqPW_7jT3lfW) action: jump_right reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1bPRnGRx2c1HJ_0y_EEOFL5GOG8sUBdIo) next_state: ![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv) action: right reward: 10.0 done: False\n",
        "\n",
        "State/next_state:\n",
        "Observation at timestep *t*/*t+1*. They are both of type `LazyFrame`. \n",
        "\n",
        "Action:\n",
        "Mario's action during state transition. \n",
        "\n",
        "Reward:\n",
        "Environment's reward during state transition. \n",
        "\n",
        "Done:\n",
        "Boolean indicating if next_state is a terminal state (end of game). Terminal state has a known Q value of 0. \n",
        "\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Sample a batch of experiences from `self.memory` of size `self.batch_size`. \n",
        "\n",
        "Return a tuple of numpy arrays, in the order of (state, next_state, action, reward, done). Each numpy array should have its first dimension equal to `self.batch_size`. \n",
        "\n",
        "To convert a `LazyFrame` to numpy array, do\n",
        "\n",
        "```\n",
        "state_np_array = np.array(state_lazy_frame)\n",
        "```\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR3QdqXmhr7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "  def sample_batch(self, batch_size):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      self.memory (FIFO queue)\n",
        "        a queue where each entry has five elements as below\n",
        "        state: LazyFrame of dimension (state_dim)\n",
        "        next_state: LazyFrame of dimension (state_dim)\n",
        "        action: integer, representing the action taken\n",
        "        reward: float, the reward from state to next_state with action\n",
        "        done: boolean, whether state is a terminal state\n",
        "      batch_size (int)\n",
        "        size of the batch to return \n",
        "\n",
        "    Output\n",
        "      state, next_state, action, reward, done (tuple)\n",
        "        a tuple of five elements: state, next_state, action, reward, done\n",
        "        state: numpy array of dimension (batch_size x state_dim)\n",
        "        next_state: numpy array of dimension (batch_size x state_dim)\n",
        "        action: numpy array of dimension (batch_size)\n",
        "        reward: numpy array of dimension (batch_size)\n",
        "        done: numpy array of dimension (batch_size)\n",
        "    \"\"\"\n",
        "    return (None, None, None, None, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOALqrSC5VIf",
        "colab_type": "text"
      },
      "source": [
        "### Prediction Q Value\n",
        "\n",
        "The learning process relies on the Q-learning algorithm (cheatsheet). We refer to the calculated Q values using current state-action pair as *prediction Q values*. These are calculated using Q_online. \n",
        "\n",
        "To call Q_online, use our defined `Mario.predict()` above:\n",
        "```\n",
        "q_values = self.predict(state, model='online')\n",
        "```\n",
        "\n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of states (s) and actions (a), calculate the estimated value for each state-action pair Q(s, a). Return the results in `torch.tensor` format. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJx-LMQmKMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:  \n",
        "  def calculate_prediction_q(state, action):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the current state \n",
        "      action (np.array)\n",
        "        dimension is (batch_size), each item is an integer representing the \n",
        "        action taken for current state \n",
        "\n",
        "    Output\n",
        "      pred_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a predicted Q value of the \n",
        "        current state-action pair \n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6efQFg5WCD",
        "colab_type": "text"
      },
      "source": [
        "### Target Q Value\n",
        "\n",
        "We refer to the calculated Q values using current reward and next state-action pair as *target Q values* (cheatsheet). *target Q values* are in the form of \n",
        "\n",
        "> r + γ max Q(s', a')\n",
        "\n",
        "Q is the Q_target function, r is the current reward, s' is the next state, a' is the next action. Because we don't know what next action will be, we estimate it using next state s' and Q_online. Specifically,\n",
        "\n",
        "> a' = argmax_a Q_p(s', a)\n",
        "\n",
        "Let's calculate *target Q values* now. \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of next_states (s') and rewards (r), calculate the target Q values. Note that a' is not explicitly given, so we will need to first obtain that using Q_online and next state s'.\n",
        "\n",
        "Return the results in `torch.tensor` format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_yrSjmhsCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:  \n",
        "  def calculate_target_q(next_state, reward):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      next_state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the next state \n",
        "      reward (np.array)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      target_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a target Q value of the current\n",
        "        state-action pair, calculated based on reward collected and \n",
        "        estimated Q value for next state\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkq2h_jg5Wn_",
        "colab_type": "text"
      },
      "source": [
        "### Loss\n",
        "\n",
        "Let's now calculate the loss between *prediction Q values* and *target Q values*. Loss is what drives the optimization and updates Q_online to better predict Q values in the future. We will calculate the mean squared loss as in:\n",
        "```\n",
        "loss = nn.functional.mse_loss(input, target)\n",
        "```\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Given predicted and target Q values for the batch of experiences, return the sum of huber loss. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnzaTGehsHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def calculate_loss(pred_q, target_q):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      pred_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is an observation \n",
        "        for the next state \n",
        "      target_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-vyeniHBsdQ",
        "colab_type": "text"
      },
      "source": [
        "### Update Q_online\n",
        "\n",
        "As the final step to complete `Mario.learn()`, we use Adam optimizer to optimize upon the above calculated `loss`. This updates the parameters inside Q_online function so that prediction Q values are closer to target Q values.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Behu1bxODdrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def __init__(self, ...):\n",
        "    # optimizer updates parameters in online_q using backpropagation\n",
        "    self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "  def update_online_q(self, loss):\n",
        "    '''\n",
        "    Input\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "      optimizer\n",
        "        optimizer updates parameter in our online_q neural network to reduce\n",
        "        the loss\n",
        "    '''\n",
        "    # update online_q\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lex52LDkfmJp",
        "colab_type": "text"
      },
      "source": [
        "### Put them Together\n",
        "With all the helper methods implemented, let's revisit our `Mario.learn()` function. \n",
        "\n",
        "### Instructions\n",
        "\n",
        "We've added some logic on checking learning criterion. For the rest, use the helper methods defined above to complete `Mario.learn()` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acxIouqZflqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.copy_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "\n",
        "        # TODO: sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = (None, None, None, None, None)\n",
        "\n",
        "        # TODO: calculate prediction Q values for the batch\n",
        "        pred_q = None\n",
        "\n",
        "        # TODO: calculate target Q values for the batch\n",
        "        target_q = None\n",
        "\n",
        "        # TODO: calculate huber loss of target and prediction values\n",
        "        loss = None\n",
        "        \n",
        "        # TODO: update target network\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49CcL-pvKdA",
        "colab_type": "text"
      },
      "source": [
        "## Completed Agent\n",
        "\n",
        "We now have all the key functionlities implemented! Glory is all yours. Here is the completed agent file, revisited. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StRaEpPovKli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from neural import ConvNet\n",
        "import pdb\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_memory, double_q):\n",
        "        # state space dimension\n",
        "        self.state_dim = state_dim\n",
        "        # action space dimension\n",
        "        self.action_dim = action_dim\n",
        "        # replay buffer\n",
        "        self.memory = deque(maxlen=max_memory)\n",
        "        # if double_q, use best action from online_q for next state q value\n",
        "        self.double_q = double_q\n",
        "        # future reward discount rate\n",
        "        self.gamma = 0.9\n",
        "        # initial epsilon(random exploration rate)\n",
        "        self.eps = 1\n",
        "        # final epsilon\n",
        "        self.eps_min = 0.1\n",
        "        # epsilon decay rate\n",
        "        self.eps_decay = 0.99999975\n",
        "        # current step, updated everytime the agent acts\n",
        "        self.step = 0\n",
        "        # number of experiences between updating online q\n",
        "        self.learn_every = 3\n",
        "        # number of experiences to collect before training\n",
        "        # self.burnin = 1e5\n",
        "        self.burnin = 1e2\n",
        "        # number of experiences between updating target q with online q\n",
        "        self.copy_every = 1e4\n",
        "        # number of experiences between saving the current agent\n",
        "        self.save_every = 5e5\n",
        "\n",
        "        # batch size used to update online q\n",
        "        self.batch_size = 32\n",
        "        # online action value function, Q(s, a)\n",
        "        self.online_q = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "        # target action value function, Q'(s, a)\n",
        "        self.target_q = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "        # optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "    def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all actions\n",
        "        model is either 'online' or 'target'\n",
        "        \"\"\"\n",
        "        state_float = torch.tensor(np.array(state)).float() / 255.\n",
        "        if model == 'online':\n",
        "            return self.online_q(state_float)\n",
        "        if model == 'target':\n",
        "            return self.target_q(state_float)\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.eps:\n",
        "            # random action\n",
        "            action = np.random.randint(low=0, high=self.action_dim)\n",
        "        else:\n",
        "            # policy action\n",
        "            q = self.predict(np.expand_dims(state, 0), model='online')\n",
        "            action = torch.max(q, axis=1)[1].item()\n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.copy_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "        # sample batch\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(np.array, zip(*batch))\n",
        "        # get next q values from target_q\n",
        "        next_q = self.predict(next_state, 'target')\n",
        "        # calculate discounted future reward\n",
        "        if self.double_q:\n",
        "            q = self.predict(next_state, 'online')\n",
        "            q_idx = torch.max(q, axis=1)[1]\n",
        "            target_q = torch.tensor(reward) + torch.tensor(1. - done) * self.gamma * next_q[np.arange(0, self.batch_size), q_idx]\n",
        "        else:\n",
        "            target_q = torch.tensor(reward) + torch.tensor(1. - done) * self.gamma * torch.max(next_q, axis=1)[0]\n",
        "        # get predicted q values from online_q and actions taken\n",
        "        curr_q = self.predict(state, 'online')\n",
        "        pred_q = curr_q[np.arange(0, self.batch_size), action]\n",
        "        # huber loss\n",
        "        loss = nn.functional.mse_loss(pred_q, target_q)\n",
        "        # update online_q\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the current agent\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def sync_target_q(self):\n",
        "        \"\"\"Update target action value (Q) function with online action value (Q) function\n",
        "        \"\"\"\n",
        "        self.target_q.load_state_dict(self.online_q.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwxsUgYe9_8B",
        "colab_type": "text"
      },
      "source": [
        "# Start Learning! \n",
        "\n",
        "With the agent and environment wrappers implemented, we are ready to put Mario in the game and start learning! We will wrap the learning process in a big `for` loop that repeats the process of acting, remembering and learning by Mario. \n",
        "\n",
        "The meat of the algorithm is in the loop, let's take a closer look: \n",
        "\n",
        "### Instruction\n",
        "\n",
        "1.At the beginning of a new episode, we need to reinitialize the `state` by calling `env.reset()`\n",
        "\n",
        "2.Then we need several variables to hold the logging information we collected in this episode:\n",
        "\n",
        "`ep_reward`: reward collected in this episode\n",
        "\n",
        "`ep_num_steps`: total number of actions perforemed in this episode\n",
        "    \n",
        "`ep_total_loss`: total loss collected in this episode\n",
        "\n",
        "`ep_total_q`: total loss collected in this episode\n",
        "\n",
        "`ep_learn_length`: used for mean loss/q_value\n",
        "\n",
        "\n",
        "3.Now we are inside the while loop that plays the game, and we can call `env.render()` to display the visual\n",
        "\n",
        "4.We want to apply the action policy on the current state by calling `agent.act`, remember action policy  $pi$ characterizes how the agent reacts to environment.\n",
        "\n",
        "5.Then after the agent performs the action, the environment will ouput its feedback, including information such as next state, reward, if Mario is dead(`done`), and some other info. Calling `env.step` with the agent's action should do the trick.\n",
        "\n",
        "6.Agent needs to remember the experience in this action he takes in this state, call `agent.remember` using all the environment output from the step above.\n",
        "\n",
        "7.Call `agent.learn`\n",
        "\n",
        "8.Update logging info\n",
        "\n",
        "9.Update state to get the evironment ready for the next interation of agent-environment/action-reward interactions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4boS5vr4-AJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### for Loop that train the model num_episodes times by playing the game\n",
        "\n",
        "for e in range(num_episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = None\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "    ep_total_loss = 0.0\n",
        "    ep_total_q = 0.0\n",
        "    ep_learn_length = 1 # used for mean loss/q_value\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = None\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = None\n",
        "\n",
        "        # 6. Remember\n",
        "        agent.remember(experience=(None,None,None,None))\n",
        "\n",
        "        # 7.Learn (conditional) (80% time cost)\n",
        "        q_value, loss = None\n",
        "\n",
        "        # 8.Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "        if q_value and loss:\n",
        "            ep_total_loss += loss\n",
        "            ep_total_q += q_value\n",
        "            ep_learn_length += 1\n",
        "\n",
        "        # Update state\n",
        "        state = None\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB4N7gPXpp0x",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Below is the fully functional `main` class, we added logging info that will help keep track of the status of the training. \n",
        "\n",
        "We have helped you initialize and applied the wrappers for the environment for you, and we also initialized the agent.\n",
        "\n",
        "In addition, we also added model saving functionality for you so that you can replay the model you trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFXMe9N4pxyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "import numpy as np\n",
        "import pdb\n",
        "import time\n",
        "# original environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "\n",
        "# define action space on the environment:\n",
        "# NOOP: no action\n",
        "# right: walk right\n",
        "# right, A: jump right\n",
        "# right, B: run right\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        "    )\n",
        "\n",
        "## apply environment wrappers\n",
        "env = apply_wrappers_to_env(env)\n",
        "\n",
        "# After applying environment wrappers, observation space (a.k.a state_dim) shrinks from\n",
        "# 240 (height) x 256 (width) x 3 (RGB color channels) \n",
        "# to \n",
        "# 4 (#frames) x 84 (height) x 84 (width)\n",
        "\n",
        "# dimensional parameters after reshaping\n",
        "state_dim = (4,84,84)\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# Intialize agent\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, max_memory=100000, double_q=True)\n",
        "\n",
        "# Logs\n",
        "log = {\n",
        "    \"rewards\": [],\n",
        "    \"lengths\": [],\n",
        "    \"losses\": [],\n",
        "    \"q_values\": []\n",
        "}\n",
        "log_file = os.path.join(agent.save_dir, \"log.txt\")\n",
        "\n",
        "# Timing\n",
        "start = time.time()\n",
        "step = 0\n",
        "\n",
        "# number of episodes\n",
        "num_episodes = 10000\n",
        "\n",
        "# Main training loop\n",
        "for e in range(num_episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = None\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "    ep_total_loss = 0.0\n",
        "    ep_total_q = 0.0\n",
        "    ep_learn_length = 1 # used for mean loss/q_value\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = None\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = None\n",
        "\n",
        "        # 6. Remember\n",
        "        agent.remember(experience=(None,None,None,None))\n",
        "\n",
        "        # 7.Learn (conditional) (80% time cost)\n",
        "        q_value, loss = None\n",
        "\n",
        "        # 8.Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "        if q_value and loss:\n",
        "            ep_total_loss += loss\n",
        "            ep_total_q += q_value\n",
        "            ep_learn_length += 1\n",
        "\n",
        "        # Update state\n",
        "        state = None\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            break\n",
        "\n",
        "    # Log info in this episode\n",
        "    log[\"rewards\"].append(ep_reward)\n",
        "    log[\"lengths\"].append(ep_length)\n",
        "    log[\"losses\"].append(np.round(ep_total_loss/ep_learn_length, 5))\n",
        "    log[\"q_values\"].append(np.round(ep_total_q/ep_learn_length, 5))\n",
        "\n",
        "    # Print & Log every 50th episode\n",
        "    if e % 50 == 0:\n",
        "        mean_reward = np.round(np.mean(log['rewards'][-100:]), 3)\n",
        "        mean_length = np.round(np.mean(log['lengths'][-100:]), 3)\n",
        "        mean_loss = np.round(np.mean(log['losses'][-100:]), 3)\n",
        "        mean_q_value = np.round(np.mean(log['q_values'][-100:]), 3)\n",
        "        eps = np.round(agent.eps, 3)\n",
        "        step_time = np.round((time.time() - start_time)/(agent.step - start_step), 3)\n",
        "        start_time = time.time()\n",
        "        start_step = agent.step\n",
        "        print(\n",
        "            f\"Episode {e} - \"\n",
        "            f\"Step {agent.step} - \"\n",
        "            f\"Step Time {step_time} - \"\n",
        "            f\"Epsilon {eps} - \"\n",
        "            f\"Mean Reward {mean_reward} - \"\n",
        "            f\"Mean Length {mean_length} - \"\n",
        "            f\"Mean Loss {mean_loss} - \"\n",
        "            f\"Mean Q Value {mean_q_value} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{e:8d}{agent.step:10d}{eps:10.3f}\"\n",
        "                f\"{mean_reward:15.3f}{mean_length:15.3f}{mean_loss:15.3f}{mean_q_value:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # Running on Colab, download checkpoints to local\n",
        "        if 'google.colab' in sys.modules:\n",
        "            from google.colab import files\n",
        "            files.download(os.path.join(agent.save_dir, \"online_q_1.chkpt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o7N4mNSQDK8",
        "colab_type": "text"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "One question one might ask why do we want to sample data points from all past experiences rather than the most recent ones(for example, from the previous episode), which are newly trained with higher accuracy. \n",
        "\n",
        "The intuition is behind the tradeoff between these two approaches:\n",
        "\n",
        "Do we want to train on data that are generated from a small-size dataset with relatively high quality or a huge-size dataset with relatively lower quality? \n",
        "\n",
        "The answer is the latter, because the more data we have, the more of a wholistic, comprehensive point of view we have on the overall behavior of the system we have, in our case, the Mario game. Limited size dataset has the danger of overfitting and overlooking bigger pictures of the entire action/state space. \n",
        "\n",
        "\n",
        "Remember, Reinforcement Learning is all about exploring different scenarios(state) and keeping improving based on trial and errors, generated from the interactions between the agent(action) and the environmental feedback(reward). "
      ]
    }
  ]
}