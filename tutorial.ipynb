{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuansongFeng/MadMario/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdoL_As2yKf9",
        "colab_type": "text"
      },
      "source": [
        "Pre-MVP tutorial for walking users through building a learning Mario. Guidelines for creating this notebook (feel free to add/edit):\n",
        "1. Extensive explanation (link to AI cheatsheet where necessary) \n",
        "2. Only ask for core logics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4hh1rNclV9",
        "colab_type": "text"
      },
      "source": [
        "reinforcement learning is taking over -- learn it\n",
        "\n",
        "we put together a tutorial project to get you up to speed -- as well as a intro rl handbook for reference! Along the tutorials, YOU will code up the project\n",
        "\n",
        "\n",
        "it'd be helpful if you have basic knowlegde of python to work through this notebook, and some knowledge about math/stats to fully understand the rl cheatsheet. if anywhere it's unclear, please leave comments in the notebook directly! we will gather all your helpful reviews "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6IAcrfyegjL",
        "colab_type": "text"
      },
      "source": [
        "lets get going! first thing first, let's see what you will be building. A mario that playes the game numerous time, improve itself and eventually pass a level. In codewise, this corresponds to . \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL7asFfFfwIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pseudo code\n",
        "for i in epriosdes:\n",
        "  act\n",
        "  rem\n",
        "  learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzxAwU4f1-H",
        "colab_type": "text"
      },
      "source": [
        "in rl terminology, this is saying: . environment and agent are two key concepts in rl at play. We refer the concepts in rl cheathsheet in multiple places. it'd be helpful to review it, before we start coding. \n",
        "\n",
        "\n",
        "data pre-processing is a key step in machine learing, whether it'd be trianing a image classifier or a text generator. in rl we also pre-process data -- data here are observarions of the environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgK3jHa65bW9",
        "colab_type": "text"
      },
      "source": [
        "# Environment\n",
        "In the below section, you will pre-process the environment by turning the perceived RGB images into gray-scale images. The advantage of doing this is that now the model can be significantly smaller because the input channels turn from 3 to 1. Due to a reduced number of model parameters to learn, the training will be faster. \n",
        "\n",
        "To visualize what your pre-processing logic will do, here are the environment feedback to Mario before and after the pre-processing:\n",
        "\n",
        "**before pre-processing**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1c9-tUWFyk4u_vNNrkZo1Rg0e2FUcbF3N)\n",
        "![picture](https://drive.google.com/uc?id=1s7UewXkmF4g_gZfD7vloH7n1Cr-D3YYX)\n",
        "![picture](https://drive.google.com/uc?id=1mXDt8rFLKT9a-YvhGOgGZT4bq0T2y7iw)\n",
        "\n",
        "**after pre-processing**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1ED9brgnbPmUZL43Bl_x2FDmXd-hsHBQt)\n",
        "![picture](https://drive.google.com/uc?id=1PB1hHSPk6jIhSxVok2u2ntHjvE3zrk7W)\n",
        "![picture](https://drive.google.com/uc?id=1CYm5q71f_OlY_mqvZADuMMjPmcMgbjVW)\n",
        "\n",
        "To pre-process the environment, we use the idea of a *wrapper*. By wrapping the environment, we can specify a desired pre-processing step to the environment output, specifically, the observation.  \n",
        "\n",
        "Example of applying an environment wrapper:\n",
        "```\n",
        "env = ResizeObservation(env, shape=84)\n",
        "```\n",
        "In this case, the environment observation output is resized to a dimension of 84 x 84. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXl7ISdnn5UF",
        "colab_type": "text"
      },
      "source": [
        "## Wrappers\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "We want to apply 3 built-in wrappers to the given `env`, `GrayScaleObservation`, `ResizeObservation`, and `FrameStack`.  \n",
        "\n",
        "https://github.com/openai/gym/tree/master/gym/wrappers\n",
        "\n",
        "\n",
        "`FrameStack` is a wrapper that will allow us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can differentiate between when Mario was landing or jumping based on his direction of movement in the previous several frames. \n",
        "\n",
        "We can start with the following arguments:\n",
        "`GrayScaleObservation`: keep_dim=False \n",
        "`ResizeObservation`: shape=84 \n",
        "`FrameStack`: num_stack=4 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ib7vjUD5cGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
        "import gym_super_mario_bros\n",
        "\n",
        "# the original environment object \n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "\n",
        "def apply_wrappers_to_env(env):\n",
        "    # TODO wrap the given env with GrayScaleObservation, ResizeObservation and FrameStack and return result\n",
        "\n",
        "    # TODO wrap the given env with GrayScaleObservation and return result\n",
        "    env = None\n",
        "    # TODO wrap the given env with ResizeObservation and return result\n",
        "    env = None\n",
        "    # TODO wrap the given env with FrameStack and return result\n",
        "    env = None\n",
        "\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5DA19ABgiY",
        "colab_type": "text"
      },
      "source": [
        "## Custom Wrapper\n",
        "\n",
        "We also would like you to get a taste of implementing an environment wrapper on your own, instead of calling off-the-shelf packages. \n",
        "\n",
        "Here is an idea:\n",
        "As an effort of downsizing our model to make training faster, we can choose to skip every n-th frame. In other words, our wrapped environment will only output every n-th frame. Below is a skeleton of the class `SkipFrame`, inherited from `gym.Wrapper`.  Notice in the `__init__` function, the `_skip` field is overriden by the input parameter, default set at 4.\n",
        "\n",
        "\n",
        "However, it is important to accumulate the reward during these skipped steps, because the reward is the most important factor in determining the success of the learning model, so while we can skip frame for dimension reduction purpose, it is crucial we keep adding those rewards to our total reward. \n",
        "\n",
        "\n",
        "**Instruction**\n",
        "\n",
        "Implement the reward accumulation function, using your favorite for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZKyjJVHQBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = None\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzNYGLqcJ0Gt",
        "colab_type": "text"
      },
      "source": [
        "**Instruction**\n",
        "\n",
        "After you finished the `SkipFrame` class, you can call it on your preprocessed `env`, along with other three build-in wrappers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlemM08BrR-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wrapper.py\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = None\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "def apply_wrappers_to_env(env):\n",
        "    # TODO wrap the given env with GrayScaleObservation and return result\n",
        "    env = None\n",
        "    # TODO wrap the given env with ResizeObservation and return result\n",
        "    env = None\n",
        "    # TODO wrap the given env with FrameStack and return result\n",
        "    env = None\n",
        "    # TODO wrap the given env with SkipEnv\n",
        "    env = SkipFrame(env, skip = 4)\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VUq3afemcj-",
        "colab_type": "text"
      },
      "source": [
        "# Agent\n",
        "\n",
        "Agent and environment are two core concepts in reinforcement learning. The agent continuously interacts with the environment, collects reward and learn to maximize its overall return in the long term. In our scenario, Mario is the agent and other game components (blocks, tubes, mushrooms, etc.) are the environment. \n",
        "\n",
        "The agent class, `DQNAgent`, captures Mario's behavior in the game environment. The agent should be able to \n",
        "\n",
        "- Make its decision about next action to take. This requires Mario to process the environment state and find the optimal action that yields the highest return value. Refer to Optimal Action in Cheatsheet. \n",
        "\n",
        "- Remember past experiences. Mario should be able to add the current experience to its memory. Later, it uses all the previous experiences to learn to act smarter. \n",
        "\n",
        "- Learn to improve action over time. The decision made by Mario should yield higher and higher return as the training proceeds. This requires Mario to update its decision process based on previous experiences. Refer to Q-learning in the RL Cheatsheet. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FMfCSqgmv23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_memory, double_q):\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTdoKmKCovs",
        "colab_type": "text"
      },
      "source": [
        "## Initialize\n",
        "Before implementing the core functions, we need to first declare some attributes(variables) the agent will need to regulate its behaviors, some examples include epsilon(random exploration rate), epsilon decay rate, future reward discount rate, etc. Please refer to cheat sheet for more details.\n",
        "\n",
        "eps (real number):\n",
        "> Random Exploration Prabability. Under some probability, agent will not follow the policy(perform the best action),instead, it will randomly choose an action to explore the state space. This is very important at the early stage of learning, because agent does not have a good policy in the begining, it needs to try different actions to see which actions leads to better rewards. Random exploration also helps agent to fall into the local optima. eps will gradually decrease as agent's policy becomes better and better. \n",
        "\n",
        "> Please initialize it to 1.0.\n",
        "\n",
        "\n",
        "\n",
        "eps_decay (real number):\n",
        "\n",
        "> Decay rate of eps. Agent rigorously explores space at the early stage, but gradually reduces its exploration rate to maintain action quality. In the later stage, agent already learns a fairly good policy, so we want it to follow its policy more frequently. Decrease eps by the factor of eps_decay each time the agent acts.\n",
        "\n",
        "> Please initialize it to 0.99999975\n",
        "\n",
        "\n",
        "gamma (real number):\n",
        "> Future reward discount rate. gamma serves to make agent give higher weight on the short-term rewards over future reward.\n",
        "\n",
        ">Please initialize it to 0.9\n",
        "\n",
        "batch_size (integer):\n",
        "> number of experiences used to update neu each time.\n",
        "\n",
        "> Please initialize it to 32\n",
        "\n",
        "state_dim (tuple):\n",
        "\n",
        "> state is the observation of the current environment which includes locations of obstacles, opponents, etc. The agent chooses the best action based entirely on the state. state_dim is the dimension of the state, in mario example, it is 4 consecutive snapshots of the enviroment stacked together, and each snapshot is a 84*84 gray-scale picture, so state_dim = (4, 84,84).\n",
        "\n",
        "action_dim (integer/tuple):\n",
        ">  The number of all possible actions the agent can perform.\n",
        "\n",
        "max_memory (integer):\n",
        "> The agent needs to remember its past experience(what the current state looks like, what action it performs, what state the action leads to, and the reward collected after performing the action). The experiences are stored in a double-ended queue(deque). During the learning phase, agent samples batch_size past experiences and update its policy. max_memory defines the capacity of the deque. If max_memory is 100, agent would remember its last 100 experiences. deque is FIFO and would throw away oldest experiences when capacity is reached and new experience is collected.\n",
        "\n",
        "Note: You can always try other combinations of parameters and test how agent would behave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxRwyg8D6gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  \n",
        "    def __init__(self, state_dim, action_dim, max_memory):\n",
        "       # state space dimension\n",
        "      self.state_dim = state_dim\n",
        "      # action space dimension\n",
        "      self.action_dim = action_dim\n",
        "      # replay buffer\n",
        "      self.memory = deque(maxlen=max_memory)\n",
        "      # current step, updated everytime the agent acts\n",
        "      self.step = 0\n",
        "\n",
        "      #TODO: Please initialize other variables as described above\n",
        "\n",
        "\n",
        "      pass\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TPviLoSpAg5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Predict Q value\n",
        "\n",
        "### `agent.predict()`\n",
        "Before we work on `agent.act()` and `agent.learn()`, we will implement an important helper method that gets used all the time: `agent.predict(state)`. Given an environment state, this function calculates the estimated Q values for all possible actions. In other words, this is the function *Q\\*(s, a)* in our Cheatsheet. Note the Q value function here is the *optimal value action function* (Refer to Optimal Value Action Function in Cheatsheet). \n",
        "\n",
        "In `agent.act()`, we use `predict()` to give us a list of Q values for all possible actions. Then we can choose the action with the highest Q value, as we are following a greedy policy. In `agent.learn()`, we rely on the Q-learning algorithm outlined in the Cheatsheet, which heavily depends on the function *Q\\*(s, a)*. \n",
        "\n",
        "\n",
        "### Online and Target Q functions\n",
        "\n",
        "Looking closer at the Q-learning algorithm in cheatsheet, we use *Q\\*(s, a)* to define both the *TD target* (r + \\gamma max_a Q(s', a')) and current state-action value (Q(s, a)). While mathematically Q(s', a') and Q(s, a) are all the same Q function, in practice we use a separate function to approximate each. This is to prevent the divergence problem during optimization. We use Q_online to represent the Q(s, a) function, and Q_target to represent the Q(s', a') function. Q_online is used to make actual action decision by the agent in `agent.act()`, and is being optimized upon in `agent.learn()`. Q_target is used to determine the optimization target of Q_online. \n",
        "\n",
        "\n",
        "### Q function:  convolution neural network\n",
        "We use a neural network to approximates each Q function. The input to the Q function are game state, which is represented as a stack of frames. Outputs from Q function are the Q values corresponding to each state-action pair. Because the inputs are in image (stack of images) format, we use a *convolution neural network*. \n",
        "\n",
        "Ideally, the Q function outputs a single Q value for a given state-action pair. For a given state, if we have 5 possible actions to take, this would require running through Q function/neural network 5 times. This is costly. Instead, we let the Q function directly output 5 values corresponding to all possible actions, in a single run. \n",
        "\n",
        "### Calling neural network\n",
        "We have implemented an off-the-shelf *convolution neural network* for you. To use the network, first initialize the network with state dimension and output dimension. State dimension is the dimension of environment observation, and output dimension is number of possible actions. Example:\n",
        "```\n",
        "self.neural_net = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "```\n",
        "\n",
        "After initialization, call the neural network with \n",
        "```\n",
        "q_values = self.neural_net(state)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BuvQ1-RpPF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, ...):\n",
        "      # online action value function, Q(s, a)\n",
        "      self.online_q = ConvNet(input_dim=state_dim, output_dim=action_dim).to(self.device)\n",
        "      # target action value function, Q'(s, a)\n",
        "      self.target_q = ConvNet(input_dim=state_dim, output_dim=action_dim).to(self.device)\n",
        "\n",
        "    def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all possible actions using specified model (either online or target)\n",
        "        Input:\n",
        "          state\n",
        "           dimension of (batch_size * state_dim)\n",
        "          model\n",
        "           either 'online' or 'target'\n",
        "        Output\n",
        "          pred_q_values (torch.tensor)\n",
        "            dimension of (batch_size * action_dim), predicted Q values for all possible actions given the state\n",
        "        \"\"\"\n",
        "        # LazyFrame -> np array -> torch tensor\n",
        "        state_float = torch.FloatTensor(np.array(state)).to(self.device)\n",
        "        # normalize\n",
        "        state_float = state_float / 255.\n",
        "        \n",
        "        # TODO return the predicted Q values for online/target function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU6KQb-AohNS",
        "colab_type": "text"
      },
      "source": [
        "## Act\n",
        "\n",
        "The *act* function defines how agent reacts to current state(observation of environment). Given a state, the agent chooses the optimal action to perform based on the policy(Q function), or sometimes it would act randomly regardless of the policy to explore the state space. Please refer to the optimal action section in the cheatsheet. To choose an action, we need to predict the Q values for all possible actions in action dimension, and choose the one that gives the highest Q-value.\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhhV_Sxk2FSQ",
        "colab_type": "text"
      },
      "source": [
        "###    Small examples on NumPy indexing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boIQ-Hi22Jf6",
        "colab_type": "code",
        "outputId": "f15f7775-7a8f-43a5-ab56-3d0a0f83f3d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [7, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoaaXCcL2jLz",
        "colab_type": "text"
      },
      "source": [
        "To get element at 1st row, 2nd col:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za-Gi-Bd2tPc",
        "colab_type": "code",
        "outputId": "9fe54cb0-8f85-4ca6-a1f9-8200aa7c29b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = a[0,1]\n",
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WFiMf2U2zS-",
        "colab_type": "text"
      },
      "source": [
        "To get elements at (1st row, 2nd col) and (3rd row, 1st col):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51FS9adl3EMP",
        "colab_type": "code",
        "outputId": "2c62f5cd-1e2c-450f-d38f-726d8bc7c593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c = a[[0,2],[1,0]]\n",
        "c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqZH2MQI5S_C",
        "colab_type": "text"
      },
      "source": [
        "To get max numbers in a along 1st axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B44B1RuW3e3c",
        "colab_type": "code",
        "outputId": "9358cd41-3700-4ee7-8e5b-2fac1ed9d373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "d = np.max(a, axis = 0)\n",
        "d"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxwcGufG5NzO",
        "colab_type": "text"
      },
      "source": [
        "To get the indices of max numbers in a along 1st axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOx8WAE03rQm",
        "colab_type": "code",
        "outputId": "1b0585ad-e11a-4385-8ef0-0f80394094b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e = np.argmax(a, axis=0);\n",
        "e"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW3W8i2Dpoxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    # def predict()\n",
        "    \n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action and update value of step\n",
        "        Input\n",
        "          state(np.array) \n",
        "            A single observation of the current state, dimension is (state_dim)\n",
        "        Output\n",
        "          action\n",
        "            An integer representing which action agent will perform\n",
        "        \"\"\"\n",
        "        # TODO choose action with epsilon-greedy policy\n",
        "        if np.random.rand() < self.eps:\n",
        "          # random action\n",
        "          pass\n",
        "        else:\n",
        "          # policy action\n",
        "          pass\n",
        "          \n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyw4YkzkqgpZ",
        "colab_type": "text"
      },
      "source": [
        "## Remember\n",
        "\n",
        "As described above, the agent needs to remember its past experiences. We declared a deque named *memory* that stores prior experiences. The agent uses experiences to learn how to update its value prediction in the future. Each time the agent performs an action, it collects an experience, please append the experience to *memory*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H488eKESqg59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the experience to memory (deque)\n",
        "        Input\n",
        "          experience =  (state, next_state, action, reward, done) tuple\n",
        "            Each time agent performs an action, it collects an experience which characterize the current state,\n",
        "            action it performs under current state, the next state after performing the action, the reward it\n",
        "            collects after performing the action, and whether the game is finished or not\n",
        "        Output\n",
        "            None\n",
        "        \"\"\"\n",
        "        # TODO Add the experience to memory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3j9G7Zq5OC",
        "colab_type": "text"
      },
      "source": [
        "## Learn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The learning process relies on the Q-learning algorithm in RL Cheatsheet. Specifically, we make the observation that the prediction based on reward and next action-state value is more accurate than predicting directly the current action-state pair. \n",
        "\n",
        "There are some key steps to perform:\n",
        "- experience sampling: We will sample experience from all past experiences as the “training set” of the current iteration of improving the q value.\n",
        "\n",
        "\n",
        "- predicting online q values: Using the sampled experience, we try to predict online q values (q values for current state)  for all state-action pairs in the sampled batch, by invoking the predict() method\n",
        "\n",
        "\n",
        "- predicting target q values:\n",
        "Using the sampled experience, we try to predict target q values (q values for next_state) for all state-action pairs in the sampled batch, using next_state and reward, also by invoking the predict() method\n",
        "\n",
        "\n",
        "- calculate loss:\n",
        "Similar to all machine learning models, we need to measure how our prediction(pred) fit the actual results(label), and improve our model by trying to minimize that difference between them. In this step, we calculate the loss(Huber loss between the predicted online q value(pred) and the predicted target q value(label)\n",
        "- update online q function: Using the loss we computed, we update our online q function. That is how we keep improving the quality of our q value estimations by Temporal Difference Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmnJ8HDiq5Ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:  \n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # set up and check learning criterion \n",
        "\n",
        "        # sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = sample_batch(self.memory, self.batch_size)\n",
        "\n",
        "        # calculate prediction Q values for the batch\n",
        "        pred_q = calculate_prediction_q(state, action)\n",
        "\n",
        "        # calculate target Q values for the batch\n",
        "        target_q = calculate_target_q(next_state, reward)\n",
        "\n",
        "        # calculate huber loss of target and prediction values\n",
        "        loss = calculate_huber_loss(pred_q, target_q)\n",
        "        \n",
        "        # update target network\n",
        "        update_prediction_q(loss, optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb3ip_Wgp0l-",
        "colab_type": "text"
      },
      "source": [
        "### Experience Sampling \n",
        "Mario learns by drawing past experiences from its memory. The memory is a queue data structure that stores each individual experience in the format of \n",
        "\n",
        "> state, next_state, action, reward, done\n",
        "\n",
        "Examples of some experiences in Mario's memory:\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1D34QpsmJSwHrdzszROt405ZwNY9LkTej)  next_state: ![pic](https://drive.google.com/uc?id=13j2TzRd1SGmFru9KJImZsY9DMCdqcr_J) action: jump reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v) next_state: ![pic](https://drive.google.com/uc?id=1hmGGVO1cS7N7kdcM99-K3Y2sxrAFd0Oh) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=10MHERSI6lap79VcZfHtIzCS9qT45ksk-) next_state: ![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1T6CAIMzNxeZlBTUdz3sB8t_GhDFbNdUO) next_state: ![pic](https://drive.google.com/uc?id=1aZlA0EnspQdcSQcVxuVmaqPW_7jT3lfW) action: jump_right reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1bPRnGRx2c1HJ_0y_EEOFL5GOG8sUBdIo) next_state: ![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv) action: right reward: 10.0 done: False\n",
        "\n",
        "State and next_state are observations at timestep *t* and *t+1* respectively. They are both of type `LazyFrame`, which allows us to optimize memory usage. To convert a `LazyFrame` to numpy array, do\n",
        "\n",
        "```\n",
        "state_np_array = np.array(state_lazy_frame)\n",
        "```\n",
        "\n",
        "Action represents what Mario takes when the state transition happens. \n",
        "\n",
        "Reward is the feedback from environment after transition happens. \n",
        "\n",
        "Done indicates if next_state is a terminal state, which means Mario is dead. Terminal state by definition has a return value of 0.\n",
        "\n",
        "\n",
        "One question one might ask why do we want to sample data points from all past experiences rather than the most recent ones(for example, from the previous episode), which are newly trained with higher accuracy. \n",
        "\n",
        "The intuition is behind the tradeoff between these two approaches:\n",
        "\n",
        "Do we want to train on data that are generated from a small-size dataset with relatively high quality or a huge-size dataset with relatively lower quality? \n",
        "\n",
        "The answer is the latter, because the more data we have, the more of a wholistic, comprehensive point of view we have on the overall behavior of the system we have, in our case, the Mario game. Limited size dataset has the danger of overfitting and overlooking bigger pictures of the entire action/state space. \n",
        "\n",
        "\n",
        "Remember, Reinforcement Learning is all about exploring different scenarios(state) and keeping improving based on trial and errors, generated from the interactions between the agent(action) and the environmental feedback(reward). \n",
        "\n",
        "## Instruction\n",
        "\n",
        "Return a batch of experiences grouped by (state, next_state, action, reward, done) individually. Standardize all formats to numpy array. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR3QdqXmhr7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def sample_batch(memory, batch_size):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      memory (FIFO queue)\n",
        "        a queue where each entry has five elements as below\n",
        "        state: LazyFrame of dimension (state_dim)\n",
        "        next_state: LazyFrame of dimension (state_dim)\n",
        "        action: integer, representing the action taken\n",
        "        reward: float, the reward from state to next_state with action\n",
        "        done: boolean, whether state is a terminal state\n",
        "      batch_size (int)\n",
        "        size of the batch to return \n",
        "\n",
        "    Output\n",
        "      state, next_state, action, reward, done (tuple)\n",
        "        a tuple of five elements: state, next_state, action, reward, done\n",
        "        state: numpy array of dimension (batch_size x state_dim)\n",
        "        next_state: numpy array of dimension (batch_size x state_dim)\n",
        "        action: numpy array of dimension (batch_size)\n",
        "        reward: numpy array of dimension (batch_size)\n",
        "        done: numpy array of dimension (batch_size)\n",
        "    \"\"\"\n",
        "    return (None, None, None, None, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOALqrSC5VIf",
        "colab_type": "text"
      },
      "source": [
        "### Predicted Q Value\n",
        "\n",
        "The learning process relies on the Q-learning algorithm (refer to Q-learning in cheatsheet):\n",
        "\n",
        "> Q_p(s, a) <- Q_p(s, a) + α(r + γ max Q_t(s', a') - Q_p(s,a))\n",
        "\n",
        "where Q_p is the prediction value function, Q_t is the target value function, s and a are the current state and action, s' is the next state, a' is the best next action decided by Q_p and s' collectively. We use two separate neural networks to represent Q_p and Q_t. The neural networks learn to estimate state-action value (Q value) better over the learning process. All s, a and s' are retrieved from memory. \n",
        "\n",
        "The reason to have 2 value functions is to prevent divergence during the optimization. Q_p is used to make actual prediction of the current state-action value, while Q_t is used in conjunction with r to determine the target state-action value (refer to Temporal Difference Learning in Cheatsheet). In this section we make value prediction using Q_p. \n",
        "\n",
        "Ideally we pass both s and a to the Q_p function, which outputs the predicted value for the state-action pair. Imagine having 5 possible actions, this means passing the state-action pair to the Q_p neural network 5 times, which is very costly. To improve efficiency, we pass only the state to Q_p, which outputs predicted Q values for all possible actions at once. For example:\n",
        "\n",
        "Input\n",
        "\n",
        "state (s): ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v)\n",
        "\n",
        "Output\n",
        "- moving right (a_1): -10\n",
        "- jumping up (a_2): 10\n",
        "- jumping right (a_3): 0\n",
        "\n",
        "This gives us \n",
        "\n",
        "```\n",
        "Q_p(s, a_1) = -10\n",
        "Q_p(s, a_2) = 10\n",
        "Q_p(s, a_3) = 0\n",
        "```\n",
        "\n",
        "In our scenario, since the action is given (e.g. a_2), we can directly return the associated Q value, i.e. Q_p(s, a_2). \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of states (s) and actions (a), calculate the estimated value for each state-action pair Q(s, a). Return the results in `torch.tensor` format. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJx-LMQmKMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  # def predict()\n",
        "  \n",
        "  def calculate_prediction_q(state, action):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the current state \n",
        "      action (np.array)\n",
        "        dimension is (batch_size), each item is an integer representing the \n",
        "        action taken for current state \n",
        "\n",
        "    Output\n",
        "      pred_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a predicted Q value of the \n",
        "        current state-action pair \n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6efQFg5WCD",
        "colab_type": "text"
      },
      "source": [
        "### Target Q Value\n",
        "\n",
        "In this section we calculate the target Q value, in the form of\n",
        "\n",
        "> r + γ max Q_t(s', a')\n",
        "\n",
        "where r is the reward at transition from s to s',  γ  is the discounting factor, and s' is the next state. Because a' is not part of the actual experience (it is the predicted best action to take at next state), we will estimate it using our prediction value function Q_p by taking the argmax of Q_p(s', a') with respect to a'. \n",
        "\n",
        "> a' = argmax_a Q_p(s', a)\n",
        "\n",
        "Target Q value, in comparison to prediction Q value Q_p(s, a), gives a better estimate of the current state-action value. We want to update the predicted Q value Q_p(s, a) towards target Q value, r + γ max Q_t(s', a'). \n",
        "\n",
        "In this section we calculate the target Q value of current state-action. \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of next_states (s') and rewards (r), calculate the target Q value using above mentioned equation. Note that a' is not explicitly given, so we will need to first obtain that using prediction value function Q_p. \n",
        "\n",
        "Return the results in `torch.tensor` format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_yrSjmhsCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  # def predict()\n",
        "  \n",
        "  def calculate_target_q(next_state, reward):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      next_state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the next state \n",
        "      reward (np.array)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      target_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a target Q value of the current\n",
        "        state-action pair, calculated based on reward collected and \n",
        "        estimated Q value for next state\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkq2h_jg5Wn_",
        "colab_type": "text"
      },
      "source": [
        "### Loss between Prediction and Target Q Value\n",
        "\n",
        "To improve our value estimation, we would like our predicted Q value to be as close to the target Q value as possible. In other words, we want to minimize the distance between Q_p(s, a) and r + γ max Q_t(s', a'). To do this, we calculate the *huber loss* between the two values, and use this loss to update Q_p, the prediction value function. \n",
        "\n",
        "\n",
        "![pic](https://drive.google.com/uc?id=1FZM7sBnMgY5GQNTx-o3LtLRLQQM0mwat)\n",
        "\n",
        "\n",
        "Huber loss is a smoothed version of L1 loss. Graph above gives some intuition behind L1 vs. L2 vs. Huber loss. L1 is intolerant around the origin and gives a high loss when there is only small difference between predicted and target value. On the other hand, L2 loss explodes quickly when there is a big difference between predicted and target value. Huber loss conveniently avoids both issues. \n",
        "\n",
        "Hint: the huber loss can be called in this way\n",
        "```\n",
        "loss = nn.functional.smooth_l1_loss(input, target)\n",
        "```\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Given predicted and target Q values for the batch of experiences, return the sum of huber loss. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnzaTGehsHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def calculate_huber_loss(pred_q, target_q):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      pred_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is an observation \n",
        "        for the next state \n",
        "      target_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-vyeniHBsdQ",
        "colab_type": "text"
      },
      "source": [
        "### Update Online_Q based on loss\n",
        "\n",
        "After we calculated the loss between pred_q and target_q, we need to update the policy to reduce the decrepancy between them. To update the policy (the parameters in nueral network), we need to use optimizer which backpropages the loss to parameters in neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Behu1bxODdrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def __init__(self, ...):\n",
        "    # optimizer updates parameters in online_q using backpropagation\n",
        "    self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "  def update_prediction_q(self, loss, optimizer):\n",
        "    '''\n",
        "    Input\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "      optimizer\n",
        "        optimizer updates parameter in our online_q neural network to reduce\n",
        "        the loss\n",
        "    '''\n",
        "    pass\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lex52LDkfmJp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "With all the helper methods implemented, let's revisit our learn function. \n",
        "\n",
        "We need to set up the learning process and check some criterion. Logic is added for you. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acxIouqZflqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.copy_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "\n",
        "        # sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = sample_batch(self.memory, self.batch_size)\n",
        "\n",
        "        # calculate prediction Q values for the batch\n",
        "        pred_q = calculate_prediction_q(state, action)\n",
        "\n",
        "        # calculate target Q values for the batch\n",
        "        target_q = calculate_target_q(next_state, reward)\n",
        "\n",
        "        # calculate huber loss of target and prediction values\n",
        "        loss = calculate_huber_loss(pred_q, target_q)\n",
        "        \n",
        "        # update target network\n",
        "        update_prediction_q(loss, optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49CcL-pvKdA",
        "colab_type": "text"
      },
      "source": [
        "## Save\n",
        "\n",
        "We know have all the key functionlities realized. There are some additional helper methods that would be useful. For example:\n",
        "- to be able to save the agent\n",
        "\n",
        "We implemented these methods for you, and here is your completed agent file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StRaEpPovKli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from neural import ConvNet\n",
        "import pdb\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_memory, double_q):\n",
        "        # state space dimension\n",
        "        self.state_dim = state_dim\n",
        "        # action space dimension\n",
        "        self.action_dim = action_dim\n",
        "        # replay buffer\n",
        "        self.memory = deque(maxlen=max_memory)\n",
        "        # if double_q, use best action from online_q for next state q value\n",
        "        self.double_q = double_q\n",
        "        # future reward discount rate\n",
        "        self.gamma = 0.9\n",
        "        # initial epsilon(random exploration rate)\n",
        "        self.eps = 1\n",
        "        # final epsilon\n",
        "        self.eps_min = 0.1\n",
        "        # epsilon decay rate\n",
        "        self.eps_decay = 0.99999975\n",
        "        # current step, updated everytime the agent acts\n",
        "        self.step = 0\n",
        "        # number of experiences between updating online q\n",
        "        self.learn_every = 3\n",
        "        # number of experiences to collect before training\n",
        "        # self.burnin = 1e5\n",
        "        self.burnin = 1e2\n",
        "        # number of experiences between updating target q with online q\n",
        "        self.copy_every = 1e4\n",
        "        # number of experiences between saving the current agent\n",
        "        self.save_every = 5e5\n",
        "\n",
        "        # batch size used to update online q\n",
        "        self.batch_size = 32\n",
        "        # online action value function, Q(s, a)\n",
        "        self.online_q = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "        # target action value function, Q'(s, a)\n",
        "        self.target_q = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "        # optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "    def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all actions\n",
        "        model is either 'online' or 'target'\n",
        "        \"\"\"\n",
        "        state_float = torch.tensor(np.array(state)).float() / 255.\n",
        "        if model == 'online':\n",
        "            return self.online_q(state_float)\n",
        "        if model == 'target':\n",
        "            return self.target_q(state_float)\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.eps:\n",
        "            # random action\n",
        "            action = np.random.randint(low=0, high=self.action_dim)\n",
        "        else:\n",
        "            # policy action\n",
        "            q = self.predict(np.expand_dims(state, 0), model='online')\n",
        "            action = torch.max(q, axis=1)[1].item()\n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.copy_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "        # sample batch\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(np.array, zip(*batch))\n",
        "        # get next q values from target_q\n",
        "        next_q = self.predict(next_state, 'target')\n",
        "        # calculate discounted future reward\n",
        "        if self.double_q:\n",
        "            q = self.predict(next_state, 'online')\n",
        "            q_idx = torch.max(q, axis=1)[1]\n",
        "            target_q = torch.tensor(reward) + torch.tensor(1. - done) * self.gamma * next_q[np.arange(0, self.batch_size), q_idx]\n",
        "        else:\n",
        "            target_q = torch.tensor(reward) + torch.tensor(1. - done) * self.gamma * torch.max(next_q, axis=1)[0]\n",
        "        # get predicted q values from online_q and actions taken\n",
        "        curr_q = self.predict(state, 'online')\n",
        "        pred_q = curr_q[np.arange(0, self.batch_size), action]\n",
        "        # huber loss\n",
        "        loss = nn.functional.mse_loss(pred_q, target_q)\n",
        "        # update online_q\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the current agent\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def sync_target_q(self):\n",
        "        \"\"\"Update target action value (Q) function with online action value (Q) function\n",
        "        \"\"\"\n",
        "        self.target_q.load_state_dict(self.online_q.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwxsUgYe9_8B",
        "colab_type": "text"
      },
      "source": [
        "# Start Learning! \n",
        "\n",
        "After finishing the implementing the Reinforcement Learning model and the environment wrappers, we are ready to build our main driver function that will get our Mario up and running. The purpose of the `main` function is to call the agent and environment wrappers we already built.\n",
        "\n",
        "We will a big `while` loop that will keep running until we quit game or stop training(by checking the `done` variable), and repeat again for `num_episodes` number of episodes.\n",
        "\n",
        "The meat of the algorithm is in the main loop, let's take a closer look: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbI2hCO1uChJ",
        "colab_type": "text"
      },
      "source": [
        "**Instruction:**\n",
        "\n",
        "1.At the beginning of a new episode, we need to reinitialize the `state` by calling `env.reset()`\n",
        "\n",
        "2.Then we need several variables to hold the logging information we collected in this episode:\n",
        "\n",
        "`ep_reward`: reward collected in this episode\n",
        "\n",
        "`ep_num_steps`: total number of actions perforemed in this episode\n",
        "    \n",
        "`ep_total_loss`: total loss collected in this episode\n",
        "\n",
        "`ep_total_q`: total loss collected in this episode\n",
        "\n",
        "`ep_learn_length`: used for mean loss/q_value\n",
        "\n",
        "\n",
        "3.Now we are inside the while loop that plays the game, and we can call `env.render()` to display the visual\n",
        "\n",
        "4.We want to apply the action policy on the current state by calling `agent.act`, remember action policy  $pi$ characterizes how the agent reacts to environment.\n",
        "\n",
        "5.Then after the agent performs the action, the environment will ouput its feedback, including information such as next state, reward, if Mario is dead(`done`), and some other info. Calling `env.step` with the agent's action should do the trick.\n",
        "\n",
        "6.Agent needs to remember the experience in this action he takes in this state, call `agent.remember` using all the environment output from the step above.\n",
        "\n",
        "7.Call `agent.learn`\n",
        "\n",
        "8.Update logging info\n",
        "\n",
        "9.Update state to get the evironment ready for the next interation of agent-environment/action-reward interactions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4boS5vr4-AJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### for Loop that train the model num_episodes times by playing the game\n",
        "\n",
        "for e in range(num_episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = None\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "    ep_total_loss = 0.0\n",
        "    ep_total_q = 0.0\n",
        "    ep_learn_length = 1 # used for mean loss/q_value\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = None\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = None\n",
        "\n",
        "        # 6. Remember\n",
        "        agent.remember(experience=(None,None,None,None))\n",
        "\n",
        "        # 7.Learn (conditional) (80% time cost)\n",
        "        q_value, loss = None\n",
        "\n",
        "        # 8.Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "        if q_value and loss:\n",
        "            ep_total_loss += loss\n",
        "            ep_total_q += q_value\n",
        "            ep_learn_length += 1\n",
        "\n",
        "        # Update state\n",
        "        state = None\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB4N7gPXpp0x",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Below is the fully functional `main` class, we added logging info that will help keep track of the status of the training. \n",
        "\n",
        "We have helped you initialize and applied the wrappers for the environment for you, and we also initialized the agent.\n",
        "\n",
        "In addition, we also added model saving functionality for you so that you can replay the model you trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFXMe9N4pxyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "import numpy as np\n",
        "import pdb\n",
        "import time\n",
        "# original environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "\n",
        "# define action space on the environment:\n",
        "# NOOP: no action\n",
        "# right: walk right\n",
        "# right, A: jump right\n",
        "# right, B: run right\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        "    )\n",
        "\n",
        "## apply environment wrappers\n",
        "env = apply_wrappers_to_env(env)\n",
        "\n",
        "# After applying environment wrappers, observation space (a.k.a state_dim) shrinks from\n",
        "# 240 (height) x 256 (width) x 3 (RGB color channels) \n",
        "# to \n",
        "# 4 (#frames) x 84 (height) x 84 (width)\n",
        "\n",
        "# dimensional parameters after reshaping\n",
        "state_dim = (4,84,84)\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# Intialize agent\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, max_memory=100000, double_q=True)\n",
        "\n",
        "# Logs\n",
        "log = {\n",
        "    \"rewards\": [],\n",
        "    \"lengths\": [],\n",
        "    \"losses\": [],\n",
        "    \"q_values\": []\n",
        "}\n",
        "log_file = os.path.join(agent.save_dir, \"log.txt\")\n",
        "\n",
        "# Timing\n",
        "start = time.time()\n",
        "step = 0\n",
        "\n",
        "# number of episodes\n",
        "num_episodes = 10000\n",
        "\n",
        "# Main training loop\n",
        "for e in range(num_episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = None\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "    ep_total_loss = 0.0\n",
        "    ep_total_q = 0.0\n",
        "    ep_learn_length = 1 # used for mean loss/q_value\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = None\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = None\n",
        "\n",
        "        # 6. Remember\n",
        "        agent.remember(experience=(None,None,None,None))\n",
        "\n",
        "        # 7.Learn (conditional) (80% time cost)\n",
        "        q_value, loss = None\n",
        "\n",
        "        # 8.Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "        if q_value and loss:\n",
        "            ep_total_loss += loss\n",
        "            ep_total_q += q_value\n",
        "            ep_learn_length += 1\n",
        "\n",
        "        # Update state\n",
        "        state = None\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            break\n",
        "\n",
        "    # Log info in this episode\n",
        "    log[\"rewards\"].append(ep_reward)\n",
        "    log[\"lengths\"].append(ep_length)\n",
        "    log[\"losses\"].append(np.round(ep_total_loss/ep_learn_length, 5))\n",
        "    log[\"q_values\"].append(np.round(ep_total_q/ep_learn_length, 5))\n",
        "\n",
        "    # Print & Log every 50th episode\n",
        "    if e % 50 == 0:\n",
        "        mean_reward = np.round(np.mean(log['rewards'][-100:]), 3)\n",
        "        mean_length = np.round(np.mean(log['lengths'][-100:]), 3)\n",
        "        mean_loss = np.round(np.mean(log['losses'][-100:]), 3)\n",
        "        mean_q_value = np.round(np.mean(log['q_values'][-100:]), 3)\n",
        "        eps = np.round(agent.eps, 3)\n",
        "        step_time = np.round((time.time() - start_time)/(agent.step - start_step), 3)\n",
        "        start_time = time.time()\n",
        "        start_step = agent.step\n",
        "        print(\n",
        "            f\"Episode {e} - \"\n",
        "            f\"Step {agent.step} - \"\n",
        "            f\"Step Time {step_time} - \"\n",
        "            f\"Epsilon {eps} - \"\n",
        "            f\"Mean Reward {mean_reward} - \"\n",
        "            f\"Mean Length {mean_length} - \"\n",
        "            f\"Mean Loss {mean_loss} - \"\n",
        "            f\"Mean Q Value {mean_q_value} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{e:8d}{agent.step:10d}{eps:10.3f}\"\n",
        "                f\"{mean_reward:15.3f}{mean_length:15.3f}{mean_loss:15.3f}{mean_q_value:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # Running on Colab, download checkpoints to local\n",
        "        if 'google.colab' in sys.modules:\n",
        "            from google.colab import files\n",
        "            files.download(os.path.join(agent.save_dir, \"online_q_1.chkpt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}