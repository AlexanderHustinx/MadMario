{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuansongFeng/MadMario/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdoL_As2yKf9",
        "colab_type": "text"
      },
      "source": [
        "Pre-MVP tutorial for walking users through building a learning Mario. Guidelines for creating this notebook (feel free to add/edit):\n",
        "1. Extensive explanation (link to AI cheatsheet where necessary) \n",
        "2. Only ask for core logics\n",
        "3. Extensive error parsing \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9_1UxghBk5m",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eArv8on_0kh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://user:pwd@github.com/YuansongFeng/MadMario\n",
        "# %cd MadMario/\n",
        "# !pip install gym-super-mario-bros"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgK3jHa65bW9",
        "colab_type": "text"
      },
      "source": [
        "# Section 0.0\n",
        "In the below section, you will pre-process the environment by turning the perceived RGB images into gray-scale images. The advantage of doing this is that now the model can be significantly smaller because the input channels turn from 3 to 1. Due to a reduced number of model parameters to learn, the training will be faster. \n",
        "\n",
        "To visualize what your pre-processing logic will do, here are the environment feedback to Mario before and after the pre-processing:\n",
        "\n",
        "**before pre-processing**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1c9-tUWFyk4u_vNNrkZo1Rg0e2FUcbF3N)\n",
        "![picture](https://drive.google.com/uc?id=1s7UewXkmF4g_gZfD7vloH7n1Cr-D3YYX)\n",
        "![picture](https://drive.google.com/uc?id=1mXDt8rFLKT9a-YvhGOgGZT4bq0T2y7iw)\n",
        "\n",
        "**after pre-processing**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1ED9brgnbPmUZL43Bl_x2FDmXd-hsHBQt)\n",
        "![picture](https://drive.google.com/uc?id=1PB1hHSPk6jIhSxVok2u2ntHjvE3zrk7W)\n",
        "![picture](https://drive.google.com/uc?id=1CYm5q71f_OlY_mqvZADuMMjPmcMgbjVW)\n",
        "\n",
        "To pre-process the environment, we use the idea of a *wrapper*. By wrapping the environment, we can specify a desired pre-processing step to the environment output, specifically, the observation.  \n",
        "\n",
        "Example of applying an environment wrapper:\n",
        "```\n",
        "env = ResizeObservation(env, shape=84)\n",
        "```\n",
        "In this case, the environment observation output is resized to a dimension of 84 x 84. \n",
        "\n",
        "# Instruction\n",
        "\n",
        "We want to apply 3 built-in wrappers to the given `env`, `GrayScaleObservation`, `ResizeObservation`, and `FrameStack`.  \n",
        "\n",
        "`FrameStack` is a wrapper that will allow us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can differentiate between when Mario was landing or jumping based on his direction of movement in the previous several frames. \n",
        "\n",
        "We can start with the following arguments:\n",
        "`GrayScaleObservation`: keep_dim=False \n",
        "`ResizeObservation`: shape=84 \n",
        "`FrameStack`: num_stack=4 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ib7vjUD5cGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
        "\n",
        "def wrapper(env):\n",
        "    # TODO wrap the given env with GrayScaleObservation, ResizeObservation and FrameStack and return result\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5DA19ABgiY",
        "colab_type": "text"
      },
      "source": [
        "We also would like you to get a taste of implementing an environment wrapper on your own, instead of calling off-the-shelf packages. Here is an idea:\n",
        "As an effort of downsizing our model to make training faster, we can choose to skip every n-th frame. In other words, our wrapped environment will only output every n-th frame. Below is a skeleton of the class `SkipFrame`, inherited from `gym.Wrapper`.  Notice in the `__init__` function, the `_skip` field is overriden by the input parameter, default set at 4.\n",
        "However, it is important to accumulate the reward during these skipped steps, because the reward is the most important factor in determining the success of the learning model, so while we can skip frame for dimension reduction purpose, it is crucial we keep adding those rewards to our total reward. Implement the reward accumulation function, using your favorite for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZKyjJVHQBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = None\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzNYGLqcJ0Gt",
        "colab_type": "text"
      },
      "source": [
        "After you finished the `SkipFrame` class, you can call it on your preprocessed `env`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1eilFmc9yoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This should be imported from a standalone python file specifically for error \n",
        "# checking and feedback. For now, define it here for example purpose. \n",
        "import gym_super_mario_bros\n",
        "\n",
        "def feedback_section_0_0(wrapper):\n",
        "  if not wrapper:\n",
        "    return \"Do you forget to define the wrapper() function?\"\n",
        "  env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "  env = wrapper(env)\n",
        "  if not env:\n",
        "    return \"Do you remember to return the wrapped env?\"\n",
        "  if not env.observation_space.shape == (240, 256):\n",
        "    return \"Do you remember to call GrayScaleObservation on env?\"\n",
        "  # More detailed tests here... \n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sceY5IceHdzs",
        "colab_type": "code",
        "outputId": "7e1cd38d-dcfe-48c6-f2ed-c38ec8a46768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "error = feedback_section_0_0(wrapper)\n",
        "if error:\n",
        "  print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do you remember to return the wrapped env?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb3ip_Wgp0l-",
        "colab_type": "text"
      },
      "source": [
        "# Experience Sampling \n",
        "Mario learns by drawing past experiences from its memory. The memory is a queue data structure that stores each individual experience in the format of \n",
        "\n",
        "> state, next_state, action, reward, done\n",
        "\n",
        "Examples of some experiences in Mario's memory:\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1D34QpsmJSwHrdzszROt405ZwNY9LkTej)  next_state: ![pic](https://drive.google.com/uc?id=13j2TzRd1SGmFru9KJImZsY9DMCdqcr_J) action: jump reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v) next_state: ![pic](https://drive.google.com/uc?id=1hmGGVO1cS7N7kdcM99-K3Y2sxrAFd0Oh) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=10MHERSI6lap79VcZfHtIzCS9qT45ksk-) next_state: ![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1T6CAIMzNxeZlBTUdz3sB8t_GhDFbNdUO) next_state: ![pic](https://drive.google.com/uc?id=1aZlA0EnspQdcSQcVxuVmaqPW_7jT3lfW) action: jump_right reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1bPRnGRx2c1HJ_0y_EEOFL5GOG8sUBdIo) next_state: ![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv) action: right reward: 10.0 done: False\n",
        "\n",
        "State and next_state are observations at timestep *t* and *t+1* respectively. They are both of type `LazyFrame`, which allows us to optimize memory usage. To convert a `LazyFrame` to numpy array, do\n",
        "\n",
        "```\n",
        "state_np_array = np.array(state_lazy_frame)\n",
        "```\n",
        "\n",
        "Action represents what Mario takes when the state transition happens. \n",
        "\n",
        "Reward is the feedback from environment after transition happens. \n",
        "\n",
        "Done indicates if next_state is a terminal state, which means Mario is dead. Terminal state by definition has a return value of 0.\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Return a batch of experiences grouped by (state, next_state, action, reward, done) individually. Standardize all formats to numpy array. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR3QdqXmhr7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_batch(memory, batch_size):\n",
        "  \"\"\"\n",
        "  Input\n",
        "    memory (FIFO queue)\n",
        "      a queue where each entry has five elements as below\n",
        "      state: LazyFrame of dimension (state_dim)\n",
        "      next_state: LazyFrame of dimension (state_dim)\n",
        "      action: integer, representing the action taken\n",
        "      reward: float, the reward from state to next_state with action\n",
        "      done: boolean, whether state is a terminal state\n",
        "    batch_size (int)\n",
        "      size of the batch to return \n",
        "\n",
        "  Output\n",
        "    state, next_state, action, reward, done (tuple)\n",
        "      a tuple of five elements: state, next_state, action, reward, done\n",
        "      state: numpy array of dimension (batch_size x state_dim)\n",
        "      next_state: numpy array of dimension (batch_size x state_dim)\n",
        "      action: numpy array of dimension (batch_size)\n",
        "      reward: numpy array of dimension (batch_size)\n",
        "      done: numpy array of dimension (batch_size)\n",
        "  \"\"\"\n",
        "  return (None, None, None, None, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOALqrSC5VIf",
        "colab_type": "text"
      },
      "source": [
        "## Predicted Q Value\n",
        "\n",
        "The learning process relies on the Q-learning algorithm (refer to Q-learning in cheatsheet):\n",
        "\n",
        "> Q_p(s, a) <- Q_p(s, a) + α(r + γ max Q_t(s', a') - Q_p(s,a))\n",
        "\n",
        "where Q_p is the prediction value function, Q_t is the target value function, s and a are the current state and action, s' is the next state, a' is the best next action decided by Q_p and s' collectively. We use two separate neural networks to represent Q_p and Q_t. The neural networks learn to estimate state-action value (Q value) better over the learning process. All s, a and s' are retrieved from memory. \n",
        "\n",
        "The reason to have 2 value functions is to prevent divergence during the optimization. Q_p is used to make actual prediction of the current state-action value, while Q_t is used in conjunction with r to determine the target state-action value (refer to Temporal Difference Learning in Cheatsheet). In this section we make value prediction using Q_p. \n",
        "\n",
        "Ideally we pass both s and a to the Q_p function, which outputs the predicted value for the state-action pair. Imagine having 5 possible actions, this means passing the state-action pair to the Q_p neural network 5 times, which is very costly. To improve efficiency, we pass only the state to Q_p, which outputs predicted Q values for all possible actions at once. For example:\n",
        "\n",
        "Input\n",
        "\n",
        "state (s): ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v)\n",
        "\n",
        "Output\n",
        "- moving right (a_1): -10\n",
        "- jumping up (a_2): 10\n",
        "- jumping right (a_3): 0\n",
        "\n",
        "This gives us \n",
        "\n",
        "```\n",
        "Q_p(s, a_1) = -10\n",
        "Q_p(s, a_2) = 10\n",
        "Q_p(s, a_3) = 0\n",
        "```\n",
        "\n",
        "In our scenario, since the action is given (e.g. a_2), we can directly return the associated Q value, i.e. Q_p(s, a_2). \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of states (s) and actions (a), calculate the estimated value for each state-action pair Q(s, a). Return the results in `torch.tensor` format. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJx-LMQmKMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_prediction_q(state, action):\n",
        "  \"\"\"\n",
        "  Input\n",
        "    state (np.array)\n",
        "      dimension is (batch_size x state_dim), each item is an observation \n",
        "      for the current state \n",
        "    action (np.array)\n",
        "      dimension is (batch_size), each item is an integer representing the \n",
        "      action taken for current state \n",
        "\n",
        "  Output\n",
        "    pred_q (torch.tensor)\n",
        "      dimension of (batch_size), each item is a predicted Q value of the \n",
        "      current state-action pair \n",
        "  \"\"\"\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6efQFg5WCD",
        "colab_type": "text"
      },
      "source": [
        "## Target Q Value\n",
        "\n",
        "In this section we calculate the target Q value, in the form of\n",
        "\n",
        "> r + γ max Q_t(s', a')\n",
        "\n",
        "where r is the reward at transition from s to s',  γ  is the discounting factor, and s' is the next state. Because a' is not part of the actual experience (it is the predicted best action to take at next state), we will estimate it using our prediction value function Q_p by taking the argmax of Q_p(s', a') with respect to a'. \n",
        "\n",
        "> a' = argmax_a Q_p(s', a)\n",
        "\n",
        "Target Q value, in comparison to prediction Q value Q_p(s, a), gives a better estimate of the current state-action value. We want to update the predicted Q value Q_p(s, a) towards target Q value, r + γ max Q_t(s', a'). \n",
        "\n",
        "In this section we calculate the target Q value of current state-action. \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of next_states (s') and rewards (r), calculate the target Q value using above mentioned equation. Note that a' is not explicitly given, so we will need to first obtain that using prediction value function Q_p. \n",
        "\n",
        "Return the results in `torch.tensor` format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_yrSjmhsCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_target_q(next_state, reward):\n",
        "  \"\"\"\n",
        "  Input\n",
        "    next_state (np.array)\n",
        "      dimension is (batch_size x state_dim), each item is an observation \n",
        "      for the next state \n",
        "    reward (np.array)\n",
        "      dimension is (batch_size), each item is a float representing the \n",
        "      reward collected from (state -> next state) transition \n",
        "\n",
        "  Output\n",
        "    target_q (torch.tensor)\n",
        "      dimension of (batch_size), each item is a target Q value of the current\n",
        "      state-action pair, calculated based on reward collected and \n",
        "      estimated Q value for next state\n",
        "  \"\"\"\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkq2h_jg5Wn_",
        "colab_type": "text"
      },
      "source": [
        "## Loss between Prediction and Target Q Value\n",
        "\n",
        "To improve our value estimation, we would like our predicted Q value to be as close to the target Q value as possible. In other words, we want to minimize the distance between Q_p(s, a) and r + γ max Q_t(s', a'). To do this, we calculate the *huber loss* between the two values, and use this loss to update Q_p, the prediction value function. \n",
        "\n",
        "\n",
        "![pic](https://drive.google.com/uc?id=1FZM7sBnMgY5GQNTx-o3LtLRLQQM0mwat)\n",
        "\n",
        "\n",
        "Huber loss is a smoothed version of L1 loss. Graph above gives some intuition behind L1 vs. L2 vs. Huber loss. L1 is intolerant around the origin and gives a high loss when there is only small difference between predicted and target value. On the other hand, L2 loss explodes quickly when there is a big difference between predicted and target value. Huber loss conveniently avoids both issues. \n",
        "\n",
        "Hint: the huber loss can be called in this way\n",
        "```\n",
        "loss = nn.functional.smooth_l1_loss(input, target)\n",
        "```\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Given predicted and target Q values for the batch of experiences, return the sum of huber loss. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnzaTGehsHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_huber_loss(pred_q, target_q):\n",
        "  \"\"\"\n",
        "  Input\n",
        "    pred_q (torch.tensor)\n",
        "      dimension is (batch_size), each item is an observation \n",
        "      for the next state \n",
        "    target_q (torch.tensor)\n",
        "      dimension is (batch_size), each item is a float representing the \n",
        "      reward collected from (state -> next state) transition \n",
        "\n",
        "  Output\n",
        "    loss (torch.tensor)\n",
        "      a single value representing the Huber loss of pred_q and target_q\n",
        "  \"\"\"\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lex52LDkfmJp",
        "colab_type": "text"
      },
      "source": [
        "## Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acxIouqZflqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn(self):\n",
        "    \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "    \"\"\"\n",
        "    # sync target network\n",
        "    if self.step % self.copy_every == 0:\n",
        "        self.sync_target_q()\n",
        "    # checkpoint model\n",
        "    if self.step % self.save_every == 0:\n",
        "        self.save_model()\n",
        "    # break if burn-in\n",
        "    if self.step < self.burnin:\n",
        "        return\n",
        "    # break if no training\n",
        "    if self.step % self.learn_every != 0:\n",
        "        return\n",
        "    # sample a batch of experiences from self.memory\n",
        "    state, next_state, action, reward, done = sample_batch(self.memory, self.batch_size)\n",
        "\n",
        "    # calculate prediction Q values for the batch\n",
        "    pred_q = calculate_prediction_q(state, action)\n",
        "\n",
        "    # calculate target Q values for the batch\n",
        "    target_q = calculate_target_q(next_state, reward)\n",
        "\n",
        "    # calculate huber loss of target and prediction values\n",
        "    loss = calculate_huber_loss(pred_q, target_q)\n",
        "    \n",
        "    # update target network\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}