{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuansongFeng/MadMario/blob/master/tutorial_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VUq3afemcj-",
        "colab_type": "text"
      },
      "source": [
        "## Agent [Yuansong]\n",
        "\n",
        "Agent and environment are two core concepts in reinforcement learning. The agent continuously interacts with the environment, collects reward and learn to maximize its overall return in the long term. In our scenario, Mario is the agent and other game components (blocks, tubes, mushrooms, etc.) are the environment. \n",
        "\n",
        "The agent class, `DQNAgent`, captures Mario's behavior in the game environment. The agent should be able to \n",
        "\n",
        "- Make its decision about next action to take. This requires Mario to process the environment state and find the optimal action that yields the highest return value. Refer to Optimal Action in Cheatsheet. \n",
        "\n",
        "- Remember past experiences. Mario should be able to add the current experience to its memory. Later, it uses all the previous experiences to learn to act smarter. \n",
        "\n",
        "- Learn to improve action over time. The decision made by Mario should yield higher and higher return as the training proceeds. This requires Mario to update its decision process based on previous experiences. Refer to Q-learnin in the RL Cheatsheet. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FMfCSqgmv23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_memory, double_q):\n",
        "        pass\n",
        "\n",
        "    def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all actions\n",
        "        model is either 'online' or 'target'\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfH3hulpoQTD",
        "colab_type": "text"
      },
      "source": [
        "Along the way we will create some helper methods that make the code more modular. Lets look at these function individually.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTdoKmKCovs",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Instance Variables [Steven]\n",
        "\n",
        "Before implementing the core functions, we need to first declare some attributes(variables) the agent will need to regulate its behaviors, some examples include epsilon(random exploration rate), epsilon decay rate, future reward discount rate, etc. Please refer to cheat sheet for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxRwyg8D6gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  \"\"\"\n",
        "  eps (real number)\n",
        "    Random Exploration Prabability. Under some probability, agent will not follow the policy(perform the best action), \n",
        "    instead, it will randomly choose an action to explore the state space. This is very important at the early stage of \n",
        "    learning, because agent does not have a good policy in the begining, it needs to try different actions to see \n",
        "    which actions leads to better rewards. Random exploration also helps agent to fall into the local optima. eps will\n",
        "    gradually decrease as agent's policy becomes better and better.\n",
        "    Please initialize it to 1.0.\n",
        "  eps_decay (real number)\n",
        "    Decay rate of eps. Agent rigorously explores space at the early stage, but gradually reduces its exploration rate \n",
        "    to maintain action quality. In the later stage, agent already learns a fairly good policy, so we want it to follow\n",
        "    its policy more frequently. Decrease eps by the factor of eps_decay each time the agent acts.\n",
        "    Please initialize it to 0.99999975\n",
        "  gamma (real number)\n",
        "    Future reward discount rate. gamma serves to make agent give higher weight on the short-term rewards over future reward.\n",
        "    Please initialize it to 0.9\n",
        "  batch_size (integer)\n",
        "    # of experiences used to update neu each time. \n",
        "    Please initialize it to 32\n",
        "  state_dim (tuple)\n",
        "    state is the observation of the current environment which includes locations of obstacles, opponents, etc. The agent\n",
        "    chooses the best action based entirely on the state. state_dim is the dimension of the state, in mario example, it is\n",
        "    4 consecutive snapshots of the enviroment stacked together, and each snapshot is a 84*84 gray-scale picture, so\n",
        "    state_dim = (4, 84,84)\n",
        "  Note: You can always try other combinations of parameters and test how agent would behave.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, max_memory):\n",
        "       # state space dimension\n",
        "      self.state_dim = state_dim\n",
        "      # action space dimension\n",
        "      self.action_dim = action_dim\n",
        "      # replay buffer\n",
        "      self.memory = deque(maxlen=max_memory)\n",
        "      # current step, updated everytime the agent acts\n",
        "      self.step = 0\n",
        "\n",
        "      #TODO: Please declare other variables as described above\n",
        "\n",
        "\n",
        "      pass\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TPviLoSpAg5",
        "colab_type": "text"
      },
      "source": [
        "## Predict Q value [Yuansong]\n",
        "\n",
        "The key function we are trying to learn here is the Q function, which is parameterized by a neural network. \n",
        "\n",
        "Instruction:\n",
        "implement prediction function for both online and target Q function. \n",
        "\n",
        "Syntax: To call the forward function of an pytorch model, we use this syntax: model(input), eg. pred_q_values = self.online_q(state_float)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BuvQ1-RpPF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, ...):\n",
        "      self.online_q = ConvNet(input_dim=state_dim, output_dim=action_dim).to(self.device)\n",
        "\n",
        "    def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all possible actions using specified model (either online or target)\n",
        "        Input:\n",
        "          state\n",
        "           dimension of (batch_size * state_dim)\n",
        "          model\n",
        "           either 'online' or 'target'\n",
        "        Output\n",
        "          pred_q_values (torch.tensor)\n",
        "            dimension of (batch_size * action_dim), predicted Q values for all possible actions given the \n",
        "            state\n",
        "        \"\"\"\n",
        "        # LazyFrame -> np array -> torch tensor\n",
        "        state_float = torch.FloatTensor(np.array(state)).to(self.device)\n",
        "        # normalize\n",
        "        state_float = state_float / 255.\n",
        "        \n",
        "        # TODO return the predicted Q values for online/target function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU6KQb-AohNS",
        "colab_type": "text"
      },
      "source": [
        "## Act [Steven]\n",
        "\n",
        "The *act* function defines how Mario reacts to current state(observation of environment). Given a state, the agent chooses the optimal action to perform based on the policy(Q function), or sometimes it would act randomly regardless of the policy to explore the state space. Please refer to the optimal action section in the cheatsheet. To choose an action, we need to predict the Q values for all possible actions in action dimension, and choose the one that gives the highest Q-value.\n",
        "\n",
        "Small examples on NumPy indexing:\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW3W8i2Dpoxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    # def predict()\n",
        "    \n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action and update value of step\n",
        "        Input\n",
        "          state(np.array) \n",
        "            A single observation of the current state, dimension is (state_dim)\n",
        "        Output\n",
        "          action\n",
        "            An integer representing which action agent will perform\n",
        "        \"\"\"\n",
        "        # TODO choose action with epsilon-greedy policy\n",
        "        if np.random.rand() < self.eps:\n",
        "          # random action\n",
        "          pass\n",
        "        else:\n",
        "          # policy action\n",
        "          pass\n",
        "          \n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyw4YkzkqgpZ",
        "colab_type": "text"
      },
      "source": [
        "## Remember [Steven]\n",
        "\n",
        "Mario has a memory that stores lots of prior experiences. Mario uses them to learn how to update its value prediction in the future. Store up experience. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H488eKESqg59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory (deque)\n",
        "        Input\n",
        "          experience =  (state, next_state, action, reward, done) tuple\n",
        "        Output\n",
        "            None\n",
        "        \"\"\"\n",
        "        # TODO Add the observation to memory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3j9G7Zq5OC",
        "colab_type": "text"
      },
      "source": [
        "## Learn [Yuansong]\n",
        "\n",
        "The learning process relies on the Q-learning algorithm in RL Cheatsheet. Specifically, we make the observation that the prediction based on reward and next action-state value is more accurate than predicting directly the current action-state pair. \n",
        "\n",
        "There are some key steps to perform:\n",
        "- experience sampling\n",
        "- predicting online q values\n",
        "- predicting target q values\n",
        "- calculate loss\n",
        "- update online q function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmnJ8HDiq5Ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:  \n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # set up and check learning criterion \n",
        "\n",
        "        # sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = sample_batch(self.memory, self.batch_size)\n",
        "\n",
        "        # calculate prediction Q values for the batch\n",
        "        pred_q = calculate_prediction_q(state, action)\n",
        "\n",
        "        # calculate target Q values for the batch\n",
        "        target_q = calculate_target_q(next_state, reward)\n",
        "\n",
        "        # calculate huber loss of target and prediction values\n",
        "        loss = calculate_huber_loss(pred_q, target_q)\n",
        "        \n",
        "        # update target network\n",
        "        update_prediction_q(loss, optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb3ip_Wgp0l-",
        "colab_type": "text"
      },
      "source": [
        "# Experience Sampling [Howard]\n",
        "Mario learns by drawing past experiences from its memory. The memory is a queue data structure that stores each individual experience in the format of \n",
        "\n",
        "> state, next_state, action, reward, done\n",
        "\n",
        "Examples of some experiences in Mario's memory:\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1D34QpsmJSwHrdzszROt405ZwNY9LkTej)  next_state: ![pic](https://drive.google.com/uc?id=13j2TzRd1SGmFru9KJImZsY9DMCdqcr_J) action: jump reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v) next_state: ![pic](https://drive.google.com/uc?id=1hmGGVO1cS7N7kdcM99-K3Y2sxrAFd0Oh) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=10MHERSI6lap79VcZfHtIzCS9qT45ksk-) next_state: ![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1T6CAIMzNxeZlBTUdz3sB8t_GhDFbNdUO) next_state: ![pic](https://drive.google.com/uc?id=1aZlA0EnspQdcSQcVxuVmaqPW_7jT3lfW) action: jump_right reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1bPRnGRx2c1HJ_0y_EEOFL5GOG8sUBdIo) next_state: ![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv) action: right reward: 10.0 done: False\n",
        "\n",
        "State and next_state are observations at timestep *t* and *t+1* respectively. They are both of type `LazyFrame`, which allows us to optimize memory usage. To convert a `LazyFrame` to numpy array, do\n",
        "\n",
        "```\n",
        "state_np_array = np.array(state_lazy_frame)\n",
        "```\n",
        "\n",
        "Action represents what Mario takes when the state transition happens. \n",
        "\n",
        "Reward is the feedback from environment after transition happens. \n",
        "\n",
        "Done indicates if next_state is a terminal state, which means Mario is dead. Terminal state by definition has a return value of 0.\n",
        "\n",
        "\n",
        "One question one might ask why do we want to sample data points from all past experiences rather than the most recent ones(for example, from the previous episode), which are newly trained with higher accuracy. \n",
        "\n",
        "The intuition is behind the tradeoff between these two approaches:\n",
        "\n",
        "Do we want to train on data that are generated from a small-size dataset with relatively high quality or a huge-size dataset with relatively lower quality? \n",
        "\n",
        "The answer is the latter, because the more data we have, the more of a wholistic, comprehensive point of view we have on the overall behavior of the system we have, in our case, the Mario game. Limited size dataset has the danger of overfitting and overlooking bigger pictures of the entire action/state space. \n",
        "\n",
        "\n",
        "Remember, Reinforcement Learning is all about exploring different scenarios(state) and keeping improving based on trial and errors, generated from the interactions between the agent(action) and the environmental feedback(reward). \n",
        "\n",
        "## Instruction\n",
        "\n",
        "Return a batch of experiences grouped by (state, next_state, action, reward, done) individually. Standardize all formats to numpy array. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR3QdqXmhr7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def sample_batch(memory, batch_size):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      memory (FIFO queue)\n",
        "        a queue where each entry has five elements as below\n",
        "        state: LazyFrame of dimension (state_dim)\n",
        "        next_state: LazyFrame of dimension (state_dim)\n",
        "        action: integer, representing the action taken\n",
        "        reward: float, the reward from state to next_state with action\n",
        "        done: boolean, whether state is a terminal state\n",
        "      batch_size (int)\n",
        "        size of the batch to return \n",
        "\n",
        "    Output\n",
        "      state, next_state, action, reward, done (tuple)\n",
        "        a tuple of five elements: state, next_state, action, reward, done\n",
        "        state: numpy array of dimension (batch_size x state_dim)\n",
        "        next_state: numpy array of dimension (batch_size x state_dim)\n",
        "        action: numpy array of dimension (batch_size)\n",
        "        reward: numpy array of dimension (batch_size)\n",
        "        done: numpy array of dimension (batch_size)\n",
        "    \"\"\"\n",
        "    return (None, None, None, None, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOALqrSC5VIf",
        "colab_type": "text"
      },
      "source": [
        "## Predicted Q Value\n",
        "\n",
        "The learning process relies on the Q-learning algorithm (refer to Q-learning in cheatsheet):\n",
        "\n",
        "> Q_p(s, a) <- Q_p(s, a) + α(r + γ max Q_t(s', a') - Q_p(s,a))\n",
        "\n",
        "where Q_p is the prediction value function, Q_t is the target value function, s and a are the current state and action, s' is the next state, a' is the best next action decided by Q_p and s' collectively. We use two separate neural networks to represent Q_p and Q_t. The neural networks learn to estimate state-action value (Q value) better over the learning process. All s, a and s' are retrieved from memory. \n",
        "\n",
        "The reason to have 2 value functions is to prevent divergence during the optimization. Q_p is used to make actual prediction of the current state-action value, while Q_t is used in conjunction with r to determine the target state-action value (refer to Temporal Difference Learning in Cheatsheet). In this section we make value prediction using Q_p. \n",
        "\n",
        "Ideally we pass both s and a to the Q_p function, which outputs the predicted value for the state-action pair. Imagine having 5 possible actions, this means passing the state-action pair to the Q_p neural network 5 times, which is very costly. To improve efficiency, we pass only the state to Q_p, which outputs predicted Q values for all possible actions at once. For example:\n",
        "\n",
        "Input\n",
        "\n",
        "state (s): ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v)\n",
        "\n",
        "Output\n",
        "- moving right (a_1): -10\n",
        "- jumping up (a_2): 10\n",
        "- jumping right (a_3): 0\n",
        "\n",
        "This gives us \n",
        "\n",
        "```\n",
        "Q_p(s, a_1) = -10\n",
        "Q_p(s, a_2) = 10\n",
        "Q_p(s, a_3) = 0\n",
        "```\n",
        "\n",
        "In our scenario, since the action is given (e.g. a_2), we can directly return the associated Q value, i.e. Q_p(s, a_2). \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of states (s) and actions (a), calculate the estimated value for each state-action pair Q(s, a). Return the results in `torch.tensor` format. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJx-LMQmKMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  # def predict()\n",
        "  \n",
        "  def calculate_prediction_q(state, action):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the current state \n",
        "      action (np.array)\n",
        "        dimension is (batch_size), each item is an integer representing the \n",
        "        action taken for current state \n",
        "\n",
        "    Output\n",
        "      pred_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a predicted Q value of the \n",
        "        current state-action pair \n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6efQFg5WCD",
        "colab_type": "text"
      },
      "source": [
        "## Target Q Value\n",
        "\n",
        "In this section we calculate the target Q value, in the form of\n",
        "\n",
        "> r + γ max Q_t(s', a')\n",
        "\n",
        "where r is the reward at transition from s to s',  γ  is the discounting factor, and s' is the next state. Because a' is not part of the actual experience (it is the predicted best action to take at next state), we will estimate it using our prediction value function Q_p by taking the argmax of Q_p(s', a') with respect to a'. \n",
        "\n",
        "> a' = argmax_a Q_p(s', a)\n",
        "\n",
        "Target Q value, in comparison to prediction Q value Q_p(s, a), gives a better estimate of the current state-action value. We want to update the predicted Q value Q_p(s, a) towards target Q value, r + γ max Q_t(s', a'). \n",
        "\n",
        "In this section we calculate the target Q value of current state-action. \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of next_states (s') and rewards (r), calculate the target Q value using above mentioned equation. Note that a' is not explicitly given, so we will need to first obtain that using prediction value function Q_p. \n",
        "\n",
        "Return the results in `torch.tensor` format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_yrSjmhsCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  # def predict()\n",
        "  \n",
        "  def calculate_target_q(next_state, reward):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      next_state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the next state \n",
        "      reward (np.array)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      target_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a target Q value of the current\n",
        "        state-action pair, calculated based on reward collected and \n",
        "        estimated Q value for next state\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkq2h_jg5Wn_",
        "colab_type": "text"
      },
      "source": [
        "## Loss between Prediction and Target Q Value\n",
        "\n",
        "To improve our value estimation, we would like our predicted Q value to be as close to the target Q value as possible. In other words, we want to minimize the distance between Q_p(s, a) and r + γ max Q_t(s', a'). To do this, we calculate the *huber loss* between the two values, and use this loss to update Q_p, the prediction value function. \n",
        "\n",
        "\n",
        "![pic](https://drive.google.com/uc?id=1FZM7sBnMgY5GQNTx-o3LtLRLQQM0mwat)\n",
        "\n",
        "\n",
        "Huber loss is a smoothed version of L1 loss. Graph above gives some intuition behind L1 vs. L2 vs. Huber loss. L1 is intolerant around the origin and gives a high loss when there is only small difference between predicted and target value. On the other hand, L2 loss explodes quickly when there is a big difference between predicted and target value. Huber loss conveniently avoids both issues. \n",
        "\n",
        "Hint: the huber loss can be called in this way\n",
        "```\n",
        "loss = nn.functional.smooth_l1_loss(input, target)\n",
        "```\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Given predicted and target Q values for the batch of experiences, return the sum of huber loss. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnzaTGehsHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def calculate_huber_loss(pred_q, target_q):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      pred_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is an observation \n",
        "        for the next state \n",
        "      target_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lex52LDkfmJp",
        "colab_type": "text"
      },
      "source": [
        "## Learn\n",
        "\n",
        "With all the helper methods implemented, let's revisit our learn function. \n",
        "\n",
        "We need to set up the learning process and check some criterion. Logic is added for you. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acxIouqZflqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.copy_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "\n",
        "        # sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = sample_batch(self.memory, self.batch_size)\n",
        "\n",
        "        # calculate prediction Q values for the batch\n",
        "        pred_q = calculate_prediction_q(state, action)\n",
        "\n",
        "        # calculate target Q values for the batch\n",
        "        target_q = calculate_target_q(next_state, reward)\n",
        "\n",
        "        # calculate huber loss of target and prediction values\n",
        "        loss = calculate_huber_loss(pred_q, target_q)\n",
        "        \n",
        "        # update target network\n",
        "        update_prediction_q(loss, optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49CcL-pvKdA",
        "colab_type": "text"
      },
      "source": [
        "## Completed agent\n",
        "\n",
        "We know have all the key functionlities realized. There are some additional helper methods that would be useful. For example:\n",
        "- to be able to save the agent\n",
        "\n",
        "We implemented these methods for you, and here is your completed agent file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StRaEpPovKli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from neural import ConvNet\n",
        "import pdb\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_memory, double_q):\n",
        "        # state space dimension\n",
        "        self.state_dim = state_dim\n",
        "        # action space dimension\n",
        "        self.action_dim = action_dim\n",
        "        # replay buffer\n",
        "        self.memory = deque(maxlen=max_memory)\n",
        "        # if double_q, use best action from online_q for next state q value\n",
        "        self.double_q = double_q\n",
        "        # future reward discount rate\n",
        "        self.gamma = 0.9\n",
        "        # initial epsilon(random exploration rate)\n",
        "        self.eps = 1\n",
        "        # final epsilon\n",
        "        self.eps_min = 0.1\n",
        "        # epsilon decay rate\n",
        "        self.eps_decay = 0.99999975\n",
        "        # current step, updated everytime the agent acts\n",
        "        self.step = 0\n",
        "        # number of experiences between updating online q\n",
        "        self.learn_every = 3\n",
        "        # number of experiences to collect before training\n",
        "        # self.burnin = 1e5\n",
        "        self.burnin = 1e2\n",
        "        # number of experiences between updating target q with online q\n",
        "        self.copy_every = 1e4\n",
        "        # number of experiences between saving the current agent\n",
        "        self.save_every = 5e5\n",
        "\n",
        "        # batch size used to update online q\n",
        "        self.batch_size = 32\n",
        "        # online action value function, Q(s, a)\n",
        "        self.online_q = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "        # target action value function, Q'(s, a)\n",
        "        self.target_q = ConvNet(input_dim=state_dim, output_dim=action_dim)\n",
        "        # optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "    def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all actions\n",
        "        model is either 'online' or 'target'\n",
        "        \"\"\"\n",
        "        state_float = torch.tensor(np.array(state)).float() / 255.\n",
        "        if model == 'online':\n",
        "            return self.online_q(state_float)\n",
        "        if model == 'target':\n",
        "            return self.target_q(state_float)\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.eps:\n",
        "            # random action\n",
        "            action = np.random.randint(low=0, high=self.action_dim)\n",
        "        else:\n",
        "            # policy action\n",
        "            q = self.predict(np.expand_dims(state, 0), model='online')\n",
        "            action = torch.max(q, axis=1)[1].item()\n",
        "        # decrease eps\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps_min, self.eps)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.copy_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "        # sample batch\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(np.array, zip(*batch))\n",
        "        # get next q values from target_q\n",
        "        next_q = self.predict(next_state, 'target')\n",
        "        # calculate discounted future reward\n",
        "        if self.double_q:\n",
        "            q = self.predict(next_state, 'online')\n",
        "            q_idx = torch.max(q, axis=1)[1]\n",
        "            target_q = torch.tensor(reward) + torch.tensor(1. - done) * self.gamma * next_q[np.arange(0, self.batch_size), q_idx]\n",
        "        else:\n",
        "            target_q = torch.tensor(reward) + torch.tensor(1. - done) * self.gamma * torch.max(next_q, axis=1)[0]\n",
        "        # get predicted q values from online_q and actions taken\n",
        "        curr_q = self.predict(state, 'online')\n",
        "        pred_q = curr_q[np.arange(0, self.batch_size), action]\n",
        "        # huber loss\n",
        "        loss = nn.functional.mse_loss(pred_q, target_q)\n",
        "        # update online_q\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # TODO Log shit\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the current agent\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def sync_target_q(self):\n",
        "        \"\"\"Update target action value (Q) function with online action value (Q) function\n",
        "        \"\"\"\n",
        "        self.target_q.load_state_dict(self.online_q.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}